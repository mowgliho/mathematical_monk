---
title: "Machine Learning"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1: Overview, example

## 1.1: Machine learning: overview and applications

- Machine learning is defined here as algorithms for inferring unknowns from knowns
  - subfield of statistics focusing on algorithms
- Many different applications

## 1.2: What is supervised learning?

- Supervised learning: Given $(x^1,y^1),\dots,(x^n,y^n)$, where $x^i$ are data points, $y^i$ are class/value, we want to choose a function $f(x) = y$
  - and be able to generalize to new $x$ points.
  - Classification: $y^i\in \mathcal{Y}$, where $\mathcal{Y}$ is a finite set
  - Regression $y^i \in \mathbb{R}^d$
    - note that the word "regression" is historical

## 1.3: What is unsupervised learning

- Unsupervised learning is well defined
  - Given $(x^1,\dots,x^n)$, typically $x^i\in \mathbb{R}^k$, find "patterns" in the data
    - Clustering: separate data into clusters
    - Density estimation: find a function for the density of the data
    - Dimensionality reduction: find a lower dimensional space to represent data

## 1.4: Variations on supervised and unsupervised

- Semi-supervised Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - the task is to predict the labels $y^{k+1},\dots,y^n$
  - i.e. we know the location of the unlabeled points
- Active Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - can ask for $y^k$'s for particular points $x^k$
  - and after ask for a few points, make predictions on the rest
- Decision theory: How to decide to measure errors/loss functions
  - e.g. weight false positives more than false negatives (e.g. a doctor deciding to do a test)
- Reinforcement Learning: Make actions to maximize long term rewards and minimize overall losses, improve model as go along.

## 1.5: Generative vs discriminative models

- Discrimative: Model $\mathbb{P}(y|x)$, i.e. the class of a given point $x$
- Generative: Model the joint distribution $\mathbb{P}(x,y)$
- can use generative to discriminate by comparing probabilities
- creating generative models takes a lot of data, have to estimate things, etc, so often discriminative models are better - and more appropriate to the problem at hand.

## 1.6: $k$-Nearest Neighbor classification algorithm

- Given data $(x_i, y_i)$, $x_i\in \mathbb{R}^d$, $y_i\in \{0,1\}$, classify a new $x$
- $kNN$: by a majority vote of the $k$ nearest points.
- Distance given by, for example, the euclidean distance
- often works pretty well, has some nice theoretical guarantees as $n$ grows large
- probabilistic interpretation: $p(y)$ is the fraction of points among the $k$ nearest neighbors with label $y$, and we select the $y$ that maximizes this probability.
  - this is a discriminative model
- $k$ is a parameter, so we can select via cross-validation, bias variance tradeoff, etc.

# 2: Trees

## 2.1: Classification trees (CART)

- Decision trees are very simple, but can have pretty good performance (especially paired with randomization)
- Classification trees
  - main idea is to form a binary tree and minimize the error in each leaf of the tree
    - nodes are either 
      - a question/decision 
      - a leaf: classify by majority vote at the leaf

## 2.2: Regression trees (CART)

- Regression tree
  - again, make a binary tree and minimize error in each leaf
    - for example
      - split on $x \leq r$, i.e. separate $x$-axis piecewise
      - put a constant model in each leaf (e.g. the average)
    - e.g. error as square error over all $y$'s
    - get a piecewise constant function

## 2.3: Growing a regression tree (CART)

- note that the $x$ can be real valued, discrete, or even unordered
- Growing a tree
  - Greedy: At each step, choose the optimal decision
    - may not get globally optimal tree
    - for example, if $x_i\in \mathbb{R}^d$, we find $s$ and $j$ to minimize the prediction as the minimum $y$ for each part:
      - $\min_y\sum_{i:x_{ij}>s}(y-y_i)^2 + \min_y\sum_{i:x_{ij}\leq s}(y-y_i)^2$
        - $y$ is the prediction for each part
      - as we have a finite number of data points, we have in essence a finite number of split points
      - and repeat
    - how do we stop:
      - otherwise we end up with one point $x$ per leaf
      - only consider splits with regions with at least $n$ points
      - or something else
- we can prune afterwards or do something like random forest afterwards

## 2.4: Growing a classification tree (CART)

- suppose that the points are in $\mathbb{R}^d$, classified into classes
- given a subregion $R$, we classify by a majority/plurality vote
  - define $E_R = \min_y(1/N_R)\sum_{i: x_i\in R} I(y_i\neq y)$, 
    - $N_R$ is the number of train points in $R$
    - $I$ is the indicator function
  - define $R(j,s)$ to be $\{x_i: x_{ij} > s\}$, $R'(j,s)=\{x_i:x_{ij}\leq s\}$, i.e. the point sets defined by a split
- To grow a classification tree, choose a dimension $j$ and a split $s$ to minimize
  - $E_{R(j,s)} + E_{R'(j,s)}$
  - and repeat
- stop when 
  - number of points reach a minimum
  - $R_k$ contains points of only one class
  - or something else
- could also try to minimize entropy/gini index instead

## 2.5: Generalizations for trees (CART)

- impurity measures for classification:
  - $E_R$: the proportion of stuff that would be misclassified (which we used before)
  - $H_R$: entropy, i.e. if we define $p_R(y)$ to be the fraction of points in $R$ with class $y$
    - $H_R = -\sum_{y\in\mathcal{Y}} p_R(y)\log p_R(y)$
    - entropy for $R$ with all in one class is $0$
    - We want to find a split to make it as homogeneous as possible
  - $G_R$: Gini index, i.e.
    - $G_R = \sum_{y\in\mathcal{Y}} p_R(y) (1-p_R(y))$
    - has some nice analytic properties
- $H_R$ and $G_R$ can have some better performance
- more generalizations/topics:
  - the notion of a split if if the $x_i$ are categorical, so that splitting doesn't strictly make sense
  - use a loss matrix for loss instead
  - missing values: recommended solution is "surrogate variables"
  - splits on linear combinations instead of a coordinate
    - but this is less efficient and surprisingly doesn't really give better performance
      - especially with the upcoming aggregation techniques
  - instability: estimator can have high variance, so has low performance in general

## 2.6: Bootstrap Aggregation (Bagging)

- take a simple algorithm and make it better (not always, but quite often)
- reduces the variation of a classification or regression procedure
- here, we do regression
- Given
  - Data $D = (X_i,Y_i)$, distributed by $P$ iid, the $X$ and $Y$ are random variables
  - Want to predict the $y$ for an $x$
- If we had a bunch of data sets $D_i$, each gives a model to predict $x$
  - if each dataset is drawn from $P$, then the average of the $y$s will be pretty good as an estimate as the true $y$
  - $D_i = (X^i_j,Y^i_j)$, as $j$ ranges
  - suppose that we have an "unbiased" estimator, i.e. the mean of $Y_j$ is the true $Y$, i.e. $\mathbb{E}Y=y=f(x)$
  - $\mathbb{E}((Y-y)^2) = E((Y-\mathbb{E}(Y))^2) := \sigma^2(Y)$
  - define $z = (1/m)\sum_1^m Y^i$, then $\mathbb{E}(z)=y$, so $z$ is also an unbiased estimator of $y$
    - i.e. if we use the empirical distribution
  - Then $\mathbb{E}((Z-y)^2) = \mathbb{E}((z-\mathbb{E}(z))^2) = \sigma^2(z)=\sigma^2((1/m)\sum Y^i) = (1/m)^2\sigma^2(\sum Y^i) = (1/m^2)\sum(\sigma^2(Y^i)) = (1/m) \sigma^2(y)$
    - in other words, our expected loss from using $z$ to estimate is $(1/m) \sigma^2(y)$, 
      - i.e. the original estimator divided by $m$
      - so as we take $m$ to infinity, the expected loss goes to 0
- Unfortunately, we don't have a bunch of different datasets
  - but we can simulate this by sampling from our dataset
  - e.g. draw uniformly from the original dataset $D$, iid, with replacement

## 2.7: Bagging for classification

- natural extensions of previous one
  - majority vote of models (each dataset gives us a classifier $C_i$)
  - average the estimated probabilities from each classifier: each $C_i$ gives us an estimated pmf $p^i$, and then we do $p(y) = (1/m)\sum_i p^i(y)$
    - similar to the regression problem from before where we try to estimate the true probability of $y$
      - same analysis holds if the $p^i$ are unbiased - so $p$ is a unbiased estimator
    - and then classify $x$ as the $y$ with the highest $p(y)$ for $x$
  - suppose that the output follows a random distribution $W$, which is independent from $Y$ (the realizations in the form of data)
    - then we want to minimize $\mathbb{E}((Y-W)^2) = \mathbb{E}((Y-\mathbb{E}(Y) + \mathbb{E}(Y) - W)^2)$
    - then this is $\mathbb{E}((Y-\mathbb{E}(Y))^2) + \mathbb{E}((\mathbb{E}(Y) - W)^2)$
      - note that we used independentness to get rid of a few terms
    - $\geq \mathbb{E}((\mathbb{E}(Y)-W)^2)$
    - in other words, using $\mathbb{E}(Y)$ instead of $Y$ reduces expected error
    - as before, we can approximate $\mathbb{E}(Y)$ with $z$, i.e. the data that we have - and reduce randomness again
  - i.e. more rationalization of the bagging algorithm

## 2.8: Random forests

- Combination of CART trees and bagging - and "random subspace"
- Simple idea, but somehow state of the art performance
- Random Forests:
  - We have some observed data points $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - for $i = 1,\dots,B$, create a bootstrap data $D_i$ from $D$ via sampling with replacement
  - construct a tree $T_i$ using $D_i$ s.t.
    - at each node, choose a random subset of the features (e.g. dimensions of $x_i$)
    - only consider splits on those features (this is the "random subspace")
  - Then do the aggregation as before on these $B$ trees $T_i$: majority vote (classification), average of probabilities (classification), average of predictions (regression)
- works for the same reason that bootstrapping works
  - each tree has high variance, but boostrapping reduces the variance

# 3: The Big Picture

## 3.1: Decision Theory (Basic Framework)

- Decision Theory: minimize expected loss
- Loss Matrix:
  - e.g., look at FP, TP, FN, TN table, each has a loss
  - Define a loss function $L(y,\hat{y})$
    - "0-1 loss": $L(y,\hat{y}) = I(y\neq \hat{y})$, $I$ being the indicator function
    - "Square loss": $L(y,\hat{y}) = (y-\hat{y})^2$
- General Framework
  - State $s$ (true/unknown)
  - Have an observation (known)
  - take an action $a$
  - get a loss $L(s,a)$
- note that we can do the opposite and do "reward/utility" instead

## 3.2: Minimizing conditional expected loss

- Decision theory for supervised learning
  - Setup
    - Data $\{(x_i,y_i)\}$
  - Possible Situation 1:
    - get a new $x$, predict $\hat{y}$ for the true $y$
    - given $x$, we want to pick $y$ to minimize loss $L(y,\hat{y})$
    - but we don't know the true $y$
  - Possible Situation 2:
    - choose $f$ to minimize $L(y,f(x))$, but we don't know $x$ or $y$
    - i.e. choose a general function to minimize the $x$ that may come
- We move to Probabilistic framework: Want small loss on average
  - Data comes as $(X,Y)$, where $X$ and $Y$ are random variables according to $p$
    - for now, we assume that they are discrete
  - Possible Situation 1 now turns into minimizing 
    - $\mathbb{E}(L(Y, \hat{y})|X = x) = \sum_{y\in\mathcal{Y}}L(y,\hat{y})p(y|x)$
    - e.g. for 0-1 loss, we get $\sum_{y\neq \hat{y}}p(y|x) = 1 - p(\hat{y}|x)$
      - so we should pick $\hat{y}$ to maximize $p(\hat{y}|x)$
      - i.e. minimizing the expected 0-1 loss tells us to predict the most likely $y$

## 3.3: Choosing $f$ to minimize expected loss

- Now let's look at Possible Situation 2 from above
  - $\hat{y} = f(x)$
  - $\mathbb{E}(L(Y,\hat{Y})) = \mathbb{E}(L(Y,f(x))) = \sum_{x,y} L(y,f(x))p(x,y) = \sum_{x,y}L(y,f(x))p(y|x)p(x) = \sum_xp(x)\sum_yL(y,f(x))p(y|x)$
  - setting $g(x,f(x)):= \sum_y L(y,f(x))p(y|x)$, this turns into the expectation $\mathbb{E}^Xg(X,f(X))$ over the distribution $p(x)$.
  - Suppose for some $x',t$, $g(x',f(x')) > g(x',t)$.
    - define $f_0(x)$ to be $f(x)$ if $x\neq x'$, and $t$ if $x = x'$
    - then $g(x,f(x))\geq g(x,f_0(x))$
    - in particular, $\mathbb{E}^X(g,X,f(X)) \geq \mathbb{E}^X(g(X,f_0(X)))$
      - in other words to minimize the loss, we should choose $f$ to minimize $g(x,f(x))$, i.e. $\arg\min_t g(x,t)$
  - The minimization problem for situation 2 reduces to many cases of the minimization problem for situation 1
    - the fact that we don't know $x$ yet doesn't really matter
    - more important is $p(y|x)$

## 3.4: Square Loss

- Consider the square loss $L(y,\hat{y}) = (y-\hat{y})^2$, with $X,Y$ given by $p$
  - no longer discrete, suppose that $p$ is a smooth and has a derivative
  - $\mathbb{E}(L(Y,\hat{y})|X=x) = \int L(y,\hat{y})p(y|x)dy = \int(y-\hat{y})^2p(y|x)dy$
  - take the derivative wrt $\hat{y}$ and set to 0
    - $0 = \int 2(\hat{y}-y)p(y|x)dy = 2\hat{y} \int p(y|x)dy - 2\int yp(y|x)dy = 2\hat{y} - 2\mathbb{E}(Y|X=x)$
    - $\hat{y} = \mathbb{E}(Y|X=x)$
    - if we take another derivative, we note that it is positive $2$, so that we've found a minimum
  - in other words, the prediction at $x$ to minimize the expected square loss is the expectation of $Y$ given $x$
    - this is a nice property of the square loss
- the solution to the minimum problem for the square loss is to set $f(x):=\mathbb{E}(Y|X=x)$, i.e. the expectation of $Y$ given $x$ at each $x$

## 3.4: The Big Picture (part 1)

- Specifically focusing on supervised learning, but the concepts generalize
- many of the core concepts of machine learning come from trying to minimizing $\mathbb{E}(L(Y,f(X)))$
- $p(y|x)$, not $p(x)$ was key to minimizing expected loss
  - Discriminative methods: Estimate $p(y|x)$ directly using data $D$
    - e.g. kNN, Trees, SVM
    - easier to compute
  - Generative methods: Estimate the joint distribution $p(y,x)$ and then recover $p(y|x) = p(x,y)/p(x)$
    - harder to compute, but are a richer family
- parameters/latent variables: we denote as $\theta$. We can think of it as a random variable and consider $p(x,y|\theta)$
- we can also think of the data as being random: $p(y|x,D) = \int_\theta p(y|\theta,x,D)p(\theta|x,D)d\theta$
  - i.e. integrate over possible parameters $\theta$
  - $p(y|x,D,\theta)$ is typically nice, can get a defined/analytic model
  - $p(\theta|x,D)$ is nasty: usually can't get a closed form expression
  - the integral sucks too.
  - this problem often intractable

## 3.5: The Big Picture (part 2)

- notation:
  - $p(y|x,D)$ is the "predictive distribution" on $y$
  - $p(\theta|x,D)$ is the "posterior distribution on $\theta$
- approaches to solve the intractable problem from before
  - Exact inference: assume a nice enough model that has nice distributions
    - Multivariate Gaussian, Conjugate prior, Graphical models
  - Point estimates of $\theta$: 
    - e.g. 
      - MLE: $\theta_{MLE} = \arg\max_\theta \mathcal{L}(\theta|y)$, where $\mathcal{L}$ is the likelihood function
      - MAP: $\theta_{MAP} = \arg\max_\theta p(\theta | x,D)$
    - then estimate $p(y|x,D)\approx p(y|x,D,\theta)$
    - to solve for $\theta$, may need to use optimization techniques or expectation maximization
    - Empirical Bayes kind of fits in here too
  - Deterministic Approximations of the integral
    - Laplace Approximation, Variational methods, Expectation Propagation
  - Stochastic Approximations
    - Markov Chain Monte Carlo (Gibbs, Metropolis-Hastings), Importance Sampling
- many methods combine these techniques

## 3.6: The Big Picture (part 3)

- Extensions of methods above to unsupervised learning
- Density Estimation (Unsupervised)
  - $D = (x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$, iid
  - Goal is to estimate the density of the $x$
  - given parameters $\theta$, we can have a probability distribution $p_\theta$ that generates the $x_i$
  - we want to calculate $p(x|D) = \int p(x|D,\theta)p(\theta|D)d\theta$
    - it turns out that we can reduce to $p(x|\theta,D) = p(x|\theta)$
    - $p(\theta|D)$ is the posterior of $\theta$ given $D$
  - same problem as before, and we can apply the same types of methods as before:
    - $p(x|\theta)$ is nice: just the model
    - $p(\theta|D)$ and the integral are not nice.

# 4: MLE

## 4.1: Maximum Likelihood Estimation (MLE) (part 1)

- Setup:
  - Given data $D = (x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$
  - assume a set of distributions $\{p_\theta:\theta \in\Theta\}$ on $\mathbb{R}^d$
  - Assume that $D$ is an iid sample according to $p_\theta$ for some $\theta\in\Theta$
- Goal: Estimate the true value of $\theta$ that generated the data
- Definition: $\theta_{MLE}$ is a MLE for $\theta$ if $\theta_{MLE}=\arg\max_\theta p(D|\theta)$
  - more precisely, it maximizes $p(D|\theta_{MLE}) = \max_\theta p(D|\theta)$, as there can be multiple $\theta$ that maximize it in theory.
  - The likelihood $p(D|\theta) = \prod_1^n p(x_i|\theta)$, as they are iid.
- Remark:
  - MLE may not be unique
  - MLE may fail to exist (e.g. maximum of $p(D|\theta)$ may not be achieved for $\theta \in \Theta$)
- Pros:
  - Easy to compute
  - interpretable
  - has some nice asymptotic properties:
    - consistent: as $n$ goes to infinity, it converges to the true $\theta$
    - normal: as $n$ goes to infinity, the distrubution of $\theta$ is normal
    - efficient: it's the best possible estimate for the true $\theta$ because it has the lowest asymptotic variance/error
  - invariant under reparameterization of $\theta$: $g(\theta_{MLE})$ is the MLE for $g(\theta)$ for some function $g$
- Cons:
  - Point estimate, so has no representation of uncertainty (i.e. we don't consider $p(\theta|D)$)
    - for example, perhaps the likelihood spikes to a high maximum, but has a larger/more probable distribution elsewhere

## 4.2: Maximum Likelihood Estimation (MLE) (part 2)

- more cons of MLE
  - Overfitting
    - regression: e.g. curvy line when straight line is better
    - black swan paradox: give 0 probability to stuff not in data
  - Wrong objective? e.g. MLE disregards the loss function
  - Existence and Uniqueness is not guaranteed

## 4.3: MLE for univariate Gaussian mean

- Remember: $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
- We assume a univariate gaussian $X \sim N(\theta,\sigma^2)$
- $p(x|\theta) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp (-\frac{1}{2\sigma^2}(x-\theta)^2)$
- Data is $D=(x_1,\dots,x_n)$, $X_i \sim N(\theta, \sigma^2)$ iid
- Then $p(D|\theta) = p(x_1,\dots,x_n|\theta) = \prod_1^np(x_i|\theta)$
- If we plug in the equation for $p(x_i|\theta)$ and take the derivative
  - note that maximizing the probability is the same as maximizing the log,
  - so we can maximize $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_1^n(x_i-\theta)^2$
  - differentiate and set to 0 w.r.t $\theta$: $0 = \frac{1}{2\sigma^2}\sum_i 2(x_i - \theta)$
    - $n\theta = \sum_i x_i$, so $\theta = \frac{1}{n}\sum_i x_i$
  - take the second derivative wrt $\theta$ we get $-n/\sigma^2 < 0$, so that this is a maximum
  - thus $\theta$ is given by the average of the $x_i$'s, ie the sample mean
- if you take the derivative wrt to $\sigma$, you obtain $\sigma^2 = \frac{\sum(x_i-\theta)^2}{n}$, i.e. the sample variance

## 4.4: MLE for a PMF on a finite set (part 1)

- $X \sim p$, $X\in \{1,\dots,m\}$
  - for example, look at a die: $m = 6$
  - Given data, the MLE finds the distribution of $p$
- Data $D = (x_1,\dots, x_n)$, $X_1,\dots,X_n\sim p$ iid
- We parameterize $\theta = (\theta_1,\dots,\theta_n)$, where $\theta = p_i = p(i|\theta) = p_\theta(i)$
- We want to find the MLE $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
- $p(x|\theta) = \theta_x$, so $p(D|\theta) = \prod_i \theta_{x_i} = \prod_{i=1}^n \prod_{j=1}^m \theta_j^{I(x_i = j)} = \prod_{j=1}^m\theta_j^{\sum_{i=1}^mI(x_i=j)} = \prod_{j=1}^m \theta_j^{n_j}$
  - $n_j$ is the number of $j$s in the data
- to maximize $p(D|\theta)$, we can take the log, so we want to maximize $\sum_{j=1}^m n_j\log \theta_j$
- we can also max if we divided by $n$, i.e. $\sum_{j=1}^m \frac{n_j}{n}\log \theta_j$

## 4.5: MLE for a PMF on a finite set (part 2)

- continuing from last time
- set $q_j = n_j/n$
  - note that $\sum_j q_j = 1$
- Now consider relative entropy or Kullback-Leibler divergence, $D(\theta ||q)$
  - $D(\theta ||q) = \sum_{j=1}^m q_j \log (q_j/\theta_j)$
  - $D(\theta ||q) \geq 0$ and is $0$ iff $\theta = q$
- we are trying to maximize $\sum_{j=1}^m q_j \log \theta_j = \sum_{j=1}^m q_j \log(\theta_j/q_j) + \sum_{j=1}^m q_j\log q_j = -D(\theta || q) - H(q)$
  - As $H(q)$ is unaffected by $\theta$, this is maximized when $\theta = q$, 
- $\theta_{MLE} = (\theta_1,\dots,\theta_n)$, where $\theta_j = n_j/n$, or the proportion of the data that is $j$
  - i.e. the empirical distribution
 
# 5: Exponential Families

## 5.1-5.2: Exponential Families

- many common distributions are cases of exponential families
- have some nice properties
- Definition: An exponential family is a set $\{p_\theta: \theta \in \Theta\}$ of pmfs or pdfs on $\mathbb{R}^d$ such that $p_\theta(x) = \exp (\sum_{i=1}^m\eta_i(\theta)s_i(x))h(x)/z(\theta)$ where:
  - $\theta\in\mathbb{R}^k$: parameter
  - $x\in\mathbb{R}^d$: input
  - $\eta_i:\Theta \rightarrow\mathbb{R}$: a function of the parameters
  - $s_i:\mathbb{R}^d\rightarrow\mathbb{R}$: "sufficient statistics"
  - $h:\mathbb{R}^d\rightarrow[0,\infty)$: support/scaling of the distribution
  - $z:\Theta\rightarrow[0,\infty)$: "partition function", normalizing constant
- in vector form: $p_\theta(x) = \exp(\eta(\theta)^Ts(x)) h(x)/z(\theta)$
- Examples
  - Exponential distribution: $\theta > 0$, $p_\theta(x)=\theta e^{-\theta x}I(x\geq 0)$
    - $\eta(\theta) = \theta$, $s(x) = -x$, $h(x)=I(x\geq 0)$, $z(\theta)=1/\theta$
    - $\Theta = (0,\infty)\subset \mathbb{R}$, $k=1$, $d=1$
  - Bernoulli distribution: $p_\theta(x) = \theta^{I(x = 1)}(1-\theta)^{I(x = 0)}$, $x\in \{0,1\}$ (0 otherwise), $\theta\in (0,1)$
    - this is equal to $\exp\log(\theta^{I(x = 1)}(1-\theta)^{I(x = 0)}) = \exp(I(x=1)\log\theta + I(x=0)\log(1-\theta))$
      - $\eta_1(\theta) = \log(\theta)$, $\eta_2(\theta)=\log(1-\theta)$, $s_1(x) = I(x=1)$, $s_2(x) = I(x = 0)$, $h(x) = I(x\in\{0,1\})$ $z(\theta) = 1$
    - we can also write as $p_\theta(x) = h(x)\theta^{I(x=1)}(1-\theta)^{1-I(x=1)}$
      - or $(1-\theta)(\theta/(1-\theta))^{I(x=1)}h(x)$
      - do $\exp\log$, get $h(x)(1-\theta)\exp(I(x=1))\log(\theta/(1-\theta))$
      - $\eta(\theta)=\log(\theta/(1-\theta))$, $s(x)=I(x=1)$, $z(\theta)=1/(1-\theta)$
    - in other words, we can also write it in a different way, with a different number of functions
- We say that it is in "natural" or "canonical" form if $\eta(\theta) = \theta$, i.e. $\eta_i(\theta)=\theta_i$ for $i = 1,\dots,m$ (the projection of $\theta \in \mathbb{R}^k$ to one dimension)
  - Turns out that all exponential families can be put into canonical form
- Other examples of Exponential Families
  - PDF: Exponential, Normal, Beta, Gamma, $\chi^2$
  - PMF: Bernoulli, Binomial, Poisson, Geometric, Multinomial
- Exponential families
  - always have conjugate priors
  - arise as solutions to maximum entropy problems
- Exponential of a non-exponential family: The uniform distribution on $(0,\theta)$
  - support depends on $\theta$ not $x$
  - note that if we consider $\theta$ to be a constant (i.e. not a parameter), this can trivially be put into exponential family form.

## 5.3-5.4: MLE for an exponential family

- Assume that our family is in natural form, i.e.
  - $x\in\mathbb{R}^d$, $\theta \in \mathbb{R}^k$
  - $p_\theta(x)=e^{\theta^Ts(x)}h(x)/z(\theta)$
- have some data $D=(x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$ generated by $p_\theta$ iid 
- $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
  - $p(x|\theta) = e^{\theta^Ts(x)}h(x)/z(\theta)$
  - $p(D|\theta) = \prod_{i=1}^n p(x_i|\theta) = z(\theta)^{-n}e^{\theta^T\sum_{i=1}^ns(x_i)}\prod_{i=1}^nh(x_i)$
  - let $s(D):= \sum_{i=1}^ns(x_i)$
  - we want to maximize wrt to $\theta$
    - same as maximizing $\log p(D|\theta) = -n\log(z(\theta)) + \theta^Ts(D) + \sum_{i=1}^n\log(h(x_i))$
    - We set the gradient wrt each $\theta_j$ to zero
      - $\frac{\partial}{\partial \theta_j} \log p(D|\theta) = -n \frac{\partial}{\partial\theta_j} \log z(\theta) + s_j(D) = 0$
      - $\frac{\partial}{\partial\theta_j}\log z(\theta)$
        - Note that $\int_{\mathbb{R}^d}p(x|\theta) dx = 1$
        - $z(\theta) = \int_{\mathbb{R}^d}e^{\theta^Ts(x)}h(x)dx$
        - assuming smoothness
        - $\frac{\partial}{\partial \theta_j}\log z(\theta) = \frac{1}{z(\theta)}\frac{\partial}{\partial \theta_j}z(\theta) = \frac{1}{z(\theta)}\int_{\mathbb{R}^d}s_j(x)e^{\theta^Ts(x)}h(x)dx = \int_{\mathbb{R}^d} s_j(x)p_\theta(x)dx = \mathbb{E}_\theta(s_j(X))$
          - i.e. $\frac{\partial}{\partial\theta_j}\log z(\theta)$ is the expected value of $s_j$ as $x$ varies (for a given $\theta$.)
        - in other words $\nabla\log z(\theta) = \mathbb{E}_\theta s(X)$
      - $0 = -n\mathbb{E}_\theta s(X) + s(D)$
      - $n\mathbb{E}_\theta s(X) = s(D) = \sum_{i=1}^ns(x_i)$
      - $\mathbb{E}_\theta s(X) = \frac{1}{n}\sum_{i=1}^n s(x_i)$
      - in other words, the critical point of $p(D|\theta)$ satisfies the property that the mean of the sufficient statistics $s$ is the same as the sample mean of the suffiient statistics
        - if the MLE exists and $\theta_{MLE}\in Int(\Theta)$, i.e. the interior of $\Theta$, it also satisfies this
        - it turns out that if $\theta$ satisfies this property and is in the interior of $\Theta$, then $\theta$ is the MLE
          - by taking another derivative and showing that it's positive definite
- Example: Take an exponential distribution $p_\theta(x) = \theta e^{-\theta x} I(x\geq 0)$
  - $z(\theta) = 1/\theta$, $s(x) = -x$, $\eta(\theta) = \theta$
  - So we can apply the results from before
    - mean
      - from the definition of $z$, $\frac{\partial}{\partial \theta}\log z(\theta) = -1/\theta$
      - from the definition of $s$ and the result above, $\mathbb{E}s(x) = -\mathbb{E}(x) = -1/\theta$
      - $\mathbb{E}(x) = 1/\theta$
    - MLE
      - $-1/\theta = \mathbb{E}_\theta s(x) = (1/n)\sum_{i=1}^n(-x_i)$
      - $1/\theta = (1/n)\sum_{i=1}^n x_i$
      - $\theta_{MLE} = \frac{1}{\frac{1}{n}\sum_{i=1}^n x_i}$

# 6: MAP

## 6.1: Maximum a posteriori (MAP) estimation

- Setup
  - data $D=(x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$
  - assume a joint distribution $p(D,\theta)$, $\theta$ as a random variable
    - can factorize as $p(D|\theta)p(\theta)$, where $p(\theta)$ is the prior
  - Goal: choose a value of $\theta_{MAP}$ for $D$ that satisfies $\arg\max_\theta p(\theta|D)$
  - contrast with $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
  - in practice, MAP is done by maximizing $p(D|\theta)p(\theta)/p(D)$ by Bayes rule
    - and we drop the $p(D)$ because it doesn't change with $\theta$
- MLE maximimizes the likelihood, MAP maximizes the posterior distribution
- MAP Pros
  - Easy to compute
  - interpretable
  - Avoids overfitting: "Regularization"/"Shrinkage"
  - Tends to look like the MLE asymptotically
    - eventually, your priors get overwhelmed by your data
- MAP Cons
  - Point estimate: no representation of uncertainty in $\theta$
  - not invariant under reparameterization
    - if $\tau = g(\theta)$, then $\tau_{MLE} = g(\theta_{MLE})$: this doesn't hold for MAP
  - Must assume a prior distribution on $\theta$

## 6.2: MAP for univariate Gaussian mean

- Data $D = (x_1,\dots,x_n)$, $x_i\in\mathbb{R}$
- Suppose $\theta$ is a random variable distributed normally with unit variance: $N(\mu,1)$
- $X_1,\dots,X_n$ are conditionally independent given $\theta$ and distributed as $N(\theta,\sigma^2)$
  - we assume $\sigma^2$ is known.
  - $p(D|\theta) = \prod_{i=1}^n p(x_i|\theta)$
- $\theta_{MAP} = \arg\max_\theta p(\theta|D) = \arg\max_\theta p(\theta|D)p(\theta)/p(D) = \arg\max p(\theta|D)p(\theta) = \arg\max_\theta (\log p(D|\theta) + \log p(\theta))$
- take the derivative, set to 0
  - $0 = \frac{1}{\sigma^2}(\sum_{i=1}^n x_i - n\theta) + (\mu - \theta)$
  - $\theta = (\frac{1}{\sigma^2}\sum x_i + \mu)/(n/\sigma^2 + 1) = \frac{\sum x_i \mu\sigma^2}{n + \sigma^2} = \frac{n}{n+\sigma^2}\bar{x} + \frac{\sigma^2}{n+\sigma^2}\mu$
- verify that the second deriative is negative
- so the new estimate is a convex combination of the sample mean $\bar{x}$ and the prior mean $\mu$

## 6.3: Interpretation of MAP as convex combination

- from before, the MAP is a convex combination of the sample mean $\bar{x}$ and the prior mean $\mu$ for the univariate Gaussian mean
  - as we get more data, we lean towards the sample mean $\bar{x}$ (the MLE estimate)
  - with less data, we default to the prior mean $\mu$
- this happens often with the MAP: that it is a convex combination of the prior and the MLE

# 7: Bayesian Inference

## 7.1: Bayesian inference - A simple example

- Bayesian inference: put distributions on all the things and then use rules of probability
- Suppose Tom is going to XY University and joins the football team
  - Coach suspects that the football field isn't actually 100 yards long
  - Tom only has a yardstick
    - suppose he measures $(x_1,x_2,x_3) = (101, 100.5, 101.5)$
    - Assume that the $x_i$ are normally iid distributed $N(\theta,1)$, where $\theta$ is the true length
    - $\theta_{MLE} = \bar{x} = 101$
    - haven't accounted for the prior knowledge of how long football fields should be
    - make a prior $\theta ~ N(100,1)$
    - then $\theta_{MAP} = 100.75$
  - apparently coach is convinced that it is shorter than 100 yards
  - Tom considers $P(\theta < 100 | D)$
    - so he considers the posterior $P(\theta|D) = P(D|\theta)P(\theta)/P(D)$
      - $P(D|\theta) = \prod_i P(x_i|\theta)$
      - turns out that $P(\theta|D)$ is also a gaussian
    - and then can solve this problem
  - what about $P(x|D)$, i.e. the predictive distribution
    - $P(x|D) = \int_\theta P(x,\theta|D) d\theta = \int_\theta p(x|\theta) p(\theta|D)$ due to conditional independence
      - this is also a gaussian, nicely

## 7.2: Aspects of Bayesian inference

- Bayesian Inference: Assume a prior distribution $p(\theta)$, use probability to infer the answers that we're looking for
- Bayesian procedures: Minimize expected loss, averaging over $\theta$
- Objective vs Subjective Bayes approach
  - Subjective: choose a prior based on beliefs
  - Objective: choose "non-informative" priors
- Pros
  - Directly answer questions
  - Avoid pathological behaviors with frequentist approaches
  - by using a prior, can avoid some overfitting
  - Model selection ("Occam's razor")
  - Admissible (not always the best, but there is no other procedure that is "always better")
- Cons
  - must assume a prior
  - Exact computation can be intractable (have to take an integral over the $\theta$, etc.)
- Priors:
  - Non-informative
  - Improper: techinically not a probability distribution, but we use it proportionally
    - e.g. $\theta \in \mathbb{R}$ universally distributed, do $p(\theta) = 1$
  - Conjugate priors: very nice to work with

## 7.3: Proportionality

- Notation $f\propto g$ if $g(x) = cf(x)$ for all $x$ for a $c\neq 0$
  - also implies $g\propto f$
- If $f$ is a pdf and $f\propto g$, then $g$ uniquely determines $f$, and $f(x) = g(x)\int_x g(x) dx$
  - i.e. $g$ is "unnormalized", $f$ is normalized

## 7.4: Conjugate priors

- allow us to get posterior distributions in closed form
- Definition: A family $\mathcal{F}$ of prior distributions $p(\theta)$ is conjugate to a likelihood $p(D|\theta)$ if the resulting posterior $p(\theta|D)$ is in $\mathcal{F}$
- Examples:
  - Beta is conjugate to Bernoulli: If data comes from a Bernoulli process for a parameter $\theta$, then if we parameterize $\theta$ with a Beta distribution, the posterior is also Beta.
  - Gaussian is conjugate to Gaussian (if the $\theta$ we're considering is the mean of the distriubution)
  - Any exponential family has a conjugate prior: so if the data comes from an exponential family source, we have a conjugate prior that we can use to get analytic posterior.

## 7.5: Beta-Bernoulli model (part 1)

- Sequence of binary outcomes (e.g. coin flip) modeled with Bernoulli random variables with with beta prior distribution on the probability of heads
  - Beta is the conjugate prior for a Bernoulli
- Setup:
  - $X_1,\dots,X_N \sim Bern(\theta)$
  - $\theta \sim Beta(\theta|a,b)$
  - $P(X=1|\theta) = \theta$
  - $p(\theta) = \theta^{a-1}(1-\theta)^{b-1}/B(a,b) \propto \theta^{a-1}(1-\theta)^{b-1}$
  - $D = (x_1,\dots,x_n)$
- $p(\theta|D) \propto p(D|\theta)p(\theta) = p(\theta)\prod p(x_i|\theta)$
  - substituting, we get $\theta^{a-1}(1-\theta)^{b-1}\theta^{n_1}(1-\theta)^{n_0} = \theta^{a+n_1-1}(1-\theta)^{b+n_0-1}\propto Beta(\theta|a+n_1-1,b+n_0-1)$
    - where $n_i$ is the number of datapoints where $x_j = i$
  - thus $p(\theta|D)$ is also a beta distribution
  - we are operating under the support $[0,1]$
    - some examples of how the $a$ and $b$ affect the shape of the prior
- If $X\sim Beta(a,b)$, then $\mathbb{E} X = a/(a+b)$, $\sigma^2(X) = ab/((a+b)^2(a+b+1))$, mode (maximum density) is $(a-1)/(a+b-2)$
  - thus if $a:= a+n_1$, $b:= b+n_0$ i.e. the posterior calculated above, then we instantly get the summary statistics
    - $\mathbb{E}X = (a+n_1)/(a+b+n)$ as $n_0 + n_1 = n$
    - mode is $(a+n_1-1)/(a+b+n-2)$

## 7.6: Beta-Bernoulli model (part 2)

- With the same setup as before
  - $\theta_{MLE}$ is the empirical distribution, i.e. $\theta_{MLE} = n_1/n$
  - $\theta_{MAP}$ is the mode from the posterior that we calculated above: $\theta_{MAP} = (a+n_1-1)/(a+b+n-2)$
  - $\mathbb{E}(\theta|D) = (a+n_1)/(a+b+n) = \frac{a+b}{a+b+n}\frac{a}{a+b} + \frac{n}{a+b+n}\frac{n_1}{n}$
    - thus it's a convex combination of the prior mean and the MLE
    - when $n$ goes to $\infty$, get the MLE; when $n$ is 0 we get the prior mean
  - $p(x=1|D) = \int_\theta p(x=1|D,\theta)p(\theta|D)$
    - $x$ is conditionally independent of data given $\theta$, so this becomes $\int_\theta p(X=1|\theta)p(\theta|D)$
    - plugging in, $\int_\theta \theta Beta(\theta|a+n_1,b + n_0) = \mathbb{E}\theta = \frac{a+n_1}{a+b+n}$
    - so we get the predictive distribution in closed form


## 7.7.A1: Dirichlet distribution

- Distribution on probability distributions
- for $\theta = (\theta_1,\dots,\theta_m) = \theta \sim Dir(\alpha)$ if $p(\theta) = \frac{1}{B(\alpha)}\prod_{i=1}^m \theta_i^{\alpha_i -1} I(\theta\in S)$
  - $\alpha = (\alpha_1,\dots,\alpha_m)$, $\alpha_i > 0$
  - $S$ is the probability simplex given by $\{x\in \mathbb{R}^m|x_i\geq 0, \sum_i x_i = 1\}$
  - $B(\alpha)$ is the multivariate beta function $\frac{\prod_i\Gamma(\alpha_i)}{\Gamma(\alpha_0)}$, and normalizes the distribution
    - $\alpha_0 = \sum_i \alpha_i$
- Is a function on the probability distribution that can take a variety of shapes on the simlex depending on the $\alpha$
- often used as a prior on pmfs on a finite set
  - useful for Bayesian inference
- Useful statistics
  - $\mathbb{E} \theta_i = \frac{\alpha_i}{\alpha_0}$, i.e. proportional to the $i$-th parameter $\alpha_i$
  - the mode of $\theta = \big(\frac{\alpha_i-1}{a_0 -n}\big)$
  - $\sigma^2(\theta_i) = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}$
- If all the $\alpha_i=1$, then $p(\theta)$ is the uniform distribution on the simplex
- If all $\alpha_j=1$ for $j\neq i$, then 
  - as we raise $\alpha_i$, $\mathbb{E}\theta_i$ goes to $1$, all others go to $0$: get some kind of hump at the $i$ corner
  - as we lower $\alpha_i$ to $0$, then the density $Dir$ goes to infinity as $\theta_i$ goes to 0.

## 7.7A2: Expectation of a Dirichlet random variable

- as before, $p(\theta) = \frac{\Gamma(\alpha_0)}{\prod \Gamma(\alpha_i)}\prod_i \theta_i^{\alpha_i-1}I(\theta\in S)$
- Property of the $\Gamma$ function: $\Gamma(x+1) = x\Gamma(x)$
- $\mathbb{E}\theta_i = \int_\theta \theta_i p(\theta) d\theta = \int \theta_i\frac{\Gamma(\alpha_0)}{\prod \Gamma(\alpha_i)}\prod \theta_i^{\alpha_i-1} d\theta$
  - Looking at $\theta_1$, this becomes $\int \theta_1^{\alpha-1}\frac{\Gamma(\alpha_0)}{\prod \Gamma(\alpha_i)}\prod_{i>1} \theta_i^{\alpha_i-1} d\theta$
    - Define $\beta = (\alpha_1+1, \alpha_2,\alpha_3,\dots,\alpha_n)$
      - thus $\beta_0 = \sum \beta_i = 1 + \alpha_0$
      - so that $\Gamma(\alpha_0) = \Gamma(\beta_0)/\alpha_0$
    - Note $\Gamma(\alpha_1+1) = \alpha_1\Gamma(\alpha_1)$
    - plugging in, we get $\int \frac{\alpha_1}{\alpha_0}\frac{\Gamma(\beta_0)}{\prod \Gamma(\beta_i)}\prod_i \theta^{\beta_i-1} = \frac{\alpha_1}{\alpha_0}$
      - integral of a pdf is 1
  - similar arguments for the other $i$
  - thus, $\mathbb{E}\theta_i = \frac{\alpha_i}{\alpha_0}$

## 7.8-7.9: Dirichlet-Categorical model

- Setup
  - $\theta = (\theta_1,\dots,\theta_m)$, $\theta_i \geq 0$, $\sum \theta_i = 1$, $X_i\in \{1,\dots,m\}$
  - $X_1,\dots,X_n \sim Cat(\theta)$
    - $Cat(\theta)$ is defined by $P(X_i = j|\theta) = \theta_j$
  - $\theta \sim Dir(\alpha)$, i.e. $p(\theta) \propto \prod_j \theta_j^{\alpha_j-1} I(\theta \in S)$
  - $D = (x_1,\dots,x_n)$
- Then
  - $p(D|\theta) = \prod_1^n \theta_{x_i} = \prod_{i=1}^n\prod_{j=1}^m \theta^{I(x_i =j)} = \prod_{j=1}^m \theta_j^{c_j}$
    - $c_j$ is the number of times $j$s in $D$, $c = (c_1,\dots,c_m)$
  - $p(\theta|D) = p(D|\theta)p(\theta) = \prod_{j=1}^m \theta_j^{c_j + \alpha_j - 1} I(\theta\in S)\propto Dir(\alpha + c)$
  - thus the Dirichlet is the conjugate prior for Categorical, and we get the posterior in closed form
    - Briefly, $(\prod_i Cat(x_i|\theta))Dir(\theta|\alpha) = Dir(\theta|c+\alpha)$
  - $p(x|D) = \int p(x|\theta)p(\theta|D) d\theta = \int \theta_x Dir(\alpha + c)d\theta$
    - this is $\mathbb{E} \theta_x$ if $\theta$ is distributed by the posterior $Dir(\alpha + c)$
      - $p(x|D) = \frac{c_x + \alpha_x}{n+a_0}$
        - If we consider the $\alpha_i$ as "pseudo counts" (kind of like occurrences of $i$), then this is the empirical distribution if we added the new counts ($(c_i)$ with $\sum c_i = n$) to our pseudo counts

## 7.9-7.10: Posterior distribution for univariate Gaussian

- we are looking at the posterior distribution for the mean of a univariate Gaussian, i.e. $p(\theta|D)$
- Setup
  - $D = (x_1,\dots,x_n)$, $x_i\in\mathbb{R}$, $X_1,\dots X_n \sim N(\mu,\sigma^2)$ iid given $\mu$
  - $\mu \sim N(\mu_0, \sigma_0^2)$
  - Assume $\sigma^2$, $\mu_0$, $\sigma_0^2$ are known
  - We are looking for $\theta := \mu$
- $p(\theta|D) = p(\mu|D) \propto p(D|\mu)p(\mu) = p(\mu)\prod_i N(\mu,\sigma^2)\propto \exp \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 + \frac{-1}{2\sigma^2}\sum(x_i-\mu)^2\big)$
  - the exponent is quadratic in $\mu$, so the pdf is a gaussian
    - if we put it in form $-\frac{1}{2\sigma_n^2}(\mu-m)^2 + c$, we get
      - the new variance $\sigma_n^2 = \frac{\sigma_0^2\sigma^2}{\sigma^2 + n\sigma_0^2}$
        - is related to the harmonic mean of $(\sigma_0^2,\sigma^2,\dots\sigma^2)$: $\sigma_0^2$ once and $\sigma^2$ $n$ times
      - the new mean $m =  \sigma_n^2\big( \frac{\mu_0}{\sigma_0^2} + \frac{\sum x_i}{\sigma^2}\big)$
        - is a convex combination of the prior and the MLE (prior is $\mu_0$, MLE is $\bar{x} = \sum x_i/n$)
        - $m = \frac{\sigma_n^2}{\sigma_0^2}\mu_0 + \frac{\sigma_n^2}{\sigma^2}n\mu_{MLE}$
  - posterior distribution $p(\mu|D)\sim N(m,\sigma_n^2)$
    - $m$ is the MAP
    - as $n$ increases, the MAP goes to the MLE, $\sigma^2_n$ goes to 0
      - we get a point mass at the MLE

# 8: Naïve Bayes

## 8.1: Naïve Bayes Classification

- Naïve Bayes Model 
  - a simple family of classifiation models
  - not necessarily "Bayesian"
  - performance not great, but it's a good baseline/intro to generative models
- Setup
  - Given $D = ((x^1,y^1),\dots (x^n,y^n))$
  - $x^i = (x^i_1,\dots,x^i_d) \in \mathbb{R}^d$
  - $y_i\in \mathcal{Y}$, where $\mathcal{Y}$ is a finite set, e.g. $\{1,\dots,m\}$
- Assume a family of distributions $p_\theta$ such that for $x\in \mathbb{R}^d$ and $y\in\mathcal{Y}$, $p_\theta(x,y) = p_\theta(x|y)p_\theta(y) = p_\theta(x_1|y)\dots p_\theta(x_d|y)p_\theta(y)$
  - in other words, we assume that coordinates $x_i$ of $x$ are conditionally independent given $y$
  - we assume that our data came from $p_\theta$ for some $\theta$
- Classification: Given $x\in\mathbb{R}^d$, we predict $y$
  - Estimate $\theta$ from $D$
  - Compute $\hat{y} = \arg\max_{y\in\mathcal{Y}} p_\theta(y|x) = \arg\max_y p_\theta (x|y)p_\theta(y)/p_\theta(x)$
    - denominator is irrelevant when varying $y$
    - $p_\theta(x|y) = \prod p_\theta(x_i|y)$ from our assumption/model

## 8.2: More about Naïve Bayes

- How do we choose $p_\theta$: depending on the problem, can do
  - $p_\theta(y) = \pi_y$ where $\pi = (\pi_1,\dots,\pi_m)$
    - i.e. some arbitrary distribution with $\pi$ as part of the parameters $\theta$
  - $p_\theta(x_i|y)$
    - if $X_i\in \{1,\dots,N\}$, i.e. a finite set, then $p_\theta(x_i|y) = q(x_i,y)$, i.e. a set of $nm$ numbers (one for each $y$ class and $x$ dimension)
    - if $X_i\in \{1,2,\dots\}$, can choose a Poisson, Geometric etc.
    - if $X_i\in \mathbb{R}$, can choose Gaussian, Gamma, etc
- How to estimate $\theta$
  - MLE - MAP (assuming a prior on $\theta$)
  - Fully Bayesian Naïve Bayes: integrate over $\theta$ and get the predictive distribution $p(y|x)$
- Why do we assume conditional independence?
  - of course, we lose out on dependencies etc
  - the benefit is that we can estimate $\theta$ more accurately with less data
    - i.e. overfit less because data is denser if we restrict to each dimension than if we consider the points as elements of $\mathbb{R}^d$
  - a wrong but simple model can perform better than a correct but complicated model in practice.

## 8.3-8.6: Bayesian Naïve Bayes

- Bayesian approach to Naïve Bayes model
- "Fully Bayesian approach" to estimate $p(y|x,D)$
- we'll consider the Categorical
- Setup
  - $D = ((x^1,y^1),\dots,(x^n,y^n))$
  - $x^i = (x^i_1,\dots,x^i_d)$ with $x^i_j\in A_j$
  - $A_j$ are finite sets
  - $y^i \in \mathcal{Y} = \{1,\dots,m\}$
  - $(X^i,Y^i)$ are iid from a probability distribution $p(x,y|\theta)$
- Naïve Bayes assumption: $p(x,y|\theta) = p(y|\theta)\prod_{j=1}^d p(x_j,y)$
  - features are conditionally independent given the class $y$ and $\theta$
- Let's define the distributions
  - $p(y|\theta) = \pi(y)$ with $\pi = (\pi(1),\dots,\pi(m))$ i.e. a parameterized vector, $\sum \pi(y) = 1$
  - $p(x_j|y,\theta) = r_{jy}(x_j)$, i.e. $r_{jy}$ as a vector with $dm$ components, $\sum_{k\in A_j} r_{jy}(k) = 1$ for all $j,y$
  - $\theta = (\pi,\{r_{jy}\})$
  - can be seen as choose $y$ according to $\pi$, and then for each $j$ choose $x_j$ according to $r_{jy}$
- Let's assume that $A_i = A_j$ for all $i$, $j$.
- which prior for $\pi$ and $r_{jk}$
  - For $y$, We pick the Dirichlet, as it's conjugate for the categorical: 
    - $p(\pi) = Dir(\pi|\alpha) \propto \prod_{y=1}^m \pi(y)^{\alpha_y -1}I(\pi \in S)$
    - $\alpha_y > 0$ for all $y$
    - product over possible $y$
  - For $r_{jk}$, we also do Dirichlet
    - $p(r_{jk}) = Dir(r_{jk}|\beta)\propto \prod_{l\in A} r_{jk}(l)^{\beta_l -1}I(r_{jk} \in S)$
    - $\beta_i > 0$
    - product over possible $x_j\in A$
- To get a prior for the joint distribution of $\theta$, We assume that the priors are independent: $p(\theta) = p(\pi)\prod_{j,k}p(r_{jk})$
- Let's classify a new point given $x$:
  - i.e. the predictive distribution $p(y|x,D)$
  - $p(y|x,D) \propto_y p(y,x,D) = \int_\theta p(x,y,D|\theta)p(\theta) d\theta = \int_\theta p(x,y|\theta)p(\theta)\prod_{i=1}^np(x^i,y^i|\theta)d\theta$
    - using conditional independence $p(x,y,D|\theta) = p(x,y|\theta)p(D|\theta)$
  - $p(x,y|\theta) = \pi(y)\prod_{j,k} r_{jk}(x_j)^{I(y=k)}$ by definition and bayes assumption
  - By independence and definition, $\prod_{i=1}^n p(x^i,y^i|\theta) = \prod_i Cat(y^i|\pi)\prod_{jk}Cat(x^i_j|r_{jk})^{I(y_i=k)}$
  - Assuming that $\pi$ and $r$ are independent, $p(\theta) = Dir(\pi|\alpha)\prod_{j,k}Dir(r_{jk}|\beta)$
  - Thus, $p(y|x,D) \propto \int_\theta p(x,y|\theta) Dir(\pi|\alpha + c)\prod_{jk}Dir(r_{jk}|\beta + d_{jk}) d\theta$
    - $c = (c_1,\dots,c_m)$, where $c_k$ is the number of points in $D$ with $y = k$
    - $d_{jk}(l)$ is the number of points in $D$  where $x^i_j = l$ and $y^i = k$
    - from before, $p(x,y|\theta) = \pi(y)\prod_{j,k}r_{jk}(x_j)^{I(y=k)}$
  - grouping stuff (as $\theta$ has a bunch of components), we have
    - $p(y|x,D) = \big( \int \pi(y)Dir(\pi|\alpha + c)d\pi\big)\prod_{j,k}\int_{r_{jk}} r_{jk}(x_j)^{I(y=k)}Dir(r_{jk}|\beta + d_{jk})dr_{jk}$
      - $\int \pi(y)Dir(\pi|\alpha + c)d\pi$ is the expected value of the dirichlet, so is $\frac{\alpha_y + c_y}{\sum_y(\alpha_y + c_y)}$
      - $\int_{r_{jk}} r_{jk}(x_j)^{I(y=k)}Dir(r_{jk}|\beta + d_{jk})dr_{jk}$ is $1$ if $y\neq k$; if $y=k$, then it's the expected value of the $x_j$ coordinate, \frac{\beta_{x_j} + d_{jy}(x_j)}{\sum (\beta_l + d_{jy}(l))} 
      - note that $\sum c_y = n$, $\sum_l d_{jy}(l) = c_y$
  - $p(y|x,D)\propto \frac{\alpha_y + c_y}{\alpha_0+n}\prod_{j=1}^d \frac{\beta_{x_j} + d_{jy}(x_j)}{\beta_0 + c_y}$
    - $\alpha_0 = \sum_y \alpha_y$, $\beta_0 = \sum_l \beta_l$
    - in other words, the product of the Dirchlet-Categorical models for each $y$ and pairs $(x_j,y)$ considered independently
      - the independence assumption kinda filters through to the result here.
- it's nice that we were able to calculate the integrals exactly using conjugate priors and facts about probability distributions and expectations

# 9: Linear Regression

## 9.1: Linear regression - Nonlinearity via basis functions

- Not just about lines and planes: can fit curves, periodic functions, etc. too
- Setup
  - $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - $x_i\in \mathbb{R}^d$
  - $y_i\in \mathbb{R}$
- Goal: get a function $f: \mathbb{R}^d\rightarrow \mathbb{R}$ to predict $y$ for a new $x$
- Basis Functions:
  - represent nonlinear problems in a linear way
  - Suppose $x\in \mathbb{R}^d$, $y\in \mathbb{R}$
    - simplest solution is a linear combination: $f(x) = w^Tx = \sum_{i=1}^d w_ix(i)$, where $x(i)$ is the $i$-th component of $x$.
    - another solution: $f(x) = w^T\phi(x) = \sum_{i=1}^n w_i\phi(x)$, where $\phi:\mathbb{R}^d\rightarrow \mathbb{R}^m$
      - this can give us nonlinear solutions that are linear in $w$
      - for example, if $d=1$, we can do $f(x) = (1,x,x^2)\in\mathbb{R}^3$, giving us a quadratic model.
      - $\phi$ as identity is the case from before
      - If we define $\phi(x) = (\phi_1(x),\dots,\phi_m(x))$, the $\phi_i$ are the basis functions.
- Other examples
  - polynomials: (even with interaction terms if desired)
  - radial basis functions: each $\phi_i$ is a little gaussian on $x$
  - fourier basis functions: each basis function is a sin curve, can make periodic functions
  - wavelets: essentially attenuated sin curves, (perhaps convolution of radial basis with fourier basis)

## 9.2: Linear regression - Definition & Motivation

- $\phi$ takes $x$ from the original space to the "feature space"
- From now on, let's work with the feature space, where things are linear.
- Motivation via Discriminative approach: want to model $p(y|x)$
  - Assume a family of distributions $p_\theta(y|x)$ parameterized by $\theta \in \Theta$
  - We want to estimate $\theta$ given data $D$, e.g. via MLE
  - easiest family of distributions for $y\in \mathbb{R}$ are Gaussians
    - $p_\theta(y|x) = N(y|\mu(x),\sigma^2(x))$
      - $x\in \mathbb{R}^d$
      - $\theta = (\mu,\sigma^2)$, $\mu: \mathbb{R}^d\rightarrow\mathbb{R}$, $\sigma^2: \mathbb{R}^d\rightarrow\mathbb{R}>0$
      - in other words, $y$ is normally distributed with $\mu$ and $\sigma^2$ as functions of $x$
- Gaussian Linear Regression 
  - models the data $D$ by assuming $p_\theta(y|x) = N(y|w^Tx,\sigma^2)$
    - parameterized $\theta = (w,\sigma^2)$, where $w\in\mathbb{R}^d$, $\sigma^2 > 0$
    - i.e. we assume that the $\mu$ function above is $\mu(x) = w^Tx$ and $\sigma^2$ is a constant.
  - Alternative formulation: $y = w^Tx + \epsilon$, where $\epsilon \sim N(0,\sigma^2)$
    - $\epsilon$ is gaussian noise

## 9.3 Choosing $f$ under linear regression

- Before, we model $y$ as coming from a normal distribution with mean $w^Tx$ and standard deviation $\sigma^2$
  - for each $x$, this gives us a distribution for $y$
  - but we want to predict a value $f(x)$, not a distribution
- Let's try to minimize expected loss
  - let's minimize square loss $\mathcal{L}(y,\hat{y}) = (y-\hat{y})^2$
  - For square loss, $\arg\min_y\mathbb{E}_\theta (\mathcal{L}(Y,y)|X=x) = \mathbb{E}_\theta(Y|X=x) = \mathbb{E}_\theta (w^Tx + \epsilon|X=x) = w^Tx$
  - thus we define $f(x) = w^Tx$ to minimize square loss.
- Choices:
  - Discriminative approach
  - Gaussian distribution $p_\theta(y|x) = N(y|\mu(x),\sigma^2(x))$
  - Linear model for $\mu(x) = w^Tx$
  - if we know $\theta = (w,\sigma^2)$, choose $f$ to minimize expected square loss: i.e. $f(x) = w^Tx$
- missing step: How do we find the true $\theta$, or more relevant, $w$?
  - Estimate using MLE
  - can also do ridge regression, lasso regression, etc to find a different $\theta$

## 9.4-9.6: MLE for linear regression

- Setup:
  - $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - $x_i \in \mathbb{R}^d$
  - $y\in \mathbb{R}$
- Assumptions
  - Gaussian linear regression: $y\sim N(w^Tx,\sigma^2)$
  - $\sigma^2$ is known: so we parameterize by $\theta = w$
  - $y_i$ are iid
- MLE
  - $\theta_{MLE} = \arg\max_{\theta\in\Theta}p(D|\theta)$
    - (more precisely, the MLE satisfies this - there can be more than one)
  - $p(D|\theta) = \prod_i p(y_i|x_i,\theta) = \frac{1}{\sqrt{2\pi\sigma^2}^n} \exp\big(-\frac{1}{2\sigma^2}\sum_i (y_i-w^Tx_i)^2\big)$
    - we are ignoring the distribution of the $x$
    - note that $\sum_i (y_i-w^Tx_i)^2 = (y-Aw)^T(y-Aw) = ||y-Aw||^2$, where
      - $y = (y_1,\dots,y_n)$
      - $A$ is a $n\times d$ matrix where each row is an $x_i$
    - to maximize $p(D|\theta)$, we want to minimize $\mathcal{L}:=(y-Aw)^T(y-Aw) = y^Ty-2y^TAw + w^TA^TAw$
      - take the gradient $\nabla_w\mathcal{L} = -2A^Ty + 2A^TAw$
        - we can check this by writing it out and seeing the derivative in each dim
          - for the $w^TA^TAw$ part, look at $w^TBw$, where $B = A^TA$ is symmetric
            - then $\nabla_w w^TBw = 2Bw$
        - set the gradient equal to 0 and get $A^Ty = A^TAw$, i.e. $w = (A^TA)^{-1}A^Ty$
  - note that $A^TA$ is invertible if the columns of $A$ are linearly independent
    - the $j$-th column is a vector of the $j$-th components of the $x_i$
  - $(A^TA)^{-1}A^T$ is known as $A^+$, the "pseudoinverse" of $A$
    - $A$ is of course an $n\times d$ matrix, which is not necessarily invertible
    - weights are $w = A^+y$
  - to show that we have the minimum, we take the Hessian $\nabla_w^2\mathcal{L} = A^TA$, which is positive semidefinite.
  - Note also that $\arg\min_w\mathcal{L} = \arg\min_w ||y-Aw||^2 = \arg\min_w ||y-Aw||$
    - in other words, the $w$ that we found minimized the euclidean distance between $y$ and $Aw$
      - in the ideal case, $y = Aw$, but as before $A$ is not necessarily invertible; the pseudoinverse is as close as we can get
- Note that we assume that the columns of $A$ are linearly independent

## 9.7: Basis functions MLE

- we considered $f(x) = w^T\phi(x) = w^Tz$, where $\phi: \mathbb{R}^d\rightarrow\mathbb{R}^m$ and $z = \phi(x)$
- define $\Phi$ as a $n\times m$ matrix where each row is $\phi(x_i)$
- Then the weights are $w = \Phi^+y = (\Phi^T\Phi)^{-1}\Phi^Ty$

# 10: Bayesian Linear Regression

## 10.1: Bayesian Linear Regression

- Why not use MLE? Overfitting
- Why not use MAP? No representation of uncertainty in $w$ or the predictions $y$
  - what if the $x$ is far outside of the training data: we should be unsure/have large error bars
- Why Bayesian? It optmizes the loss function and gives us the prediction distribution $p(y|x,D)$, which is what we really want.
- Setup:
  - $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - $x_i\in \mathbb{R}^d$
  - $y_i\in \mathbb{R}$
- Model:
  - $Y_1,\dots, Y_n$ independent given $w$, $Y_i\sim N(w^Tx_i,a^{-1})$
    - $a = 1/\sigma^2$ is the "precision"
  - $w\sim N(0,b^{-1}I)$
    - we model $w$ as normally distribited with variance $b^{-1}I$, where $b>0$
    - $I$ is the identity matrix - thus the $w_i$ are iid
- assume $a$ and $b$ are known, so that our parameters are $\theta = w$

## 10.2-10.3: Posterior for linear regression

- Remark: can replace $x_i$ with $\phi(x_i)$ and thus model nonlinearities
- Likelihood: $p(D|w) \propto \exp\big(-\frac{a}{2}(y-Aw)^T(y-Aw)\big)$
  - $A$ is the $n\times d$ design matrix with rows equal to the $x_i$
  - $y = (y_1,\dots,y_n)^T$
- Posterior: $p(w|D)\propto p(D|w)p(w) \propto \exp \big(-\frac{a}{2}(y-Aw)^T(y-Aw)-\frac{b}{2}w^Tw\big)$
  - the exponent is a quadratic in $w$, so is also Gaussian
  - $a(y-Aw)^T(y-Aw)+bw^Tw = ay^Ty-2aw^TA^Ty + w^T(aA^TA + bI)w$
  - completing the square and matching with the (note) below, we define
    - $\Lambda = aA^TA + bI$
    - $\mu = a\Lambda^{-1}A^Ty$
  - and get $p(w|D)\propto N(\mu, \Lambda^{-1})$
  - check that $\Lambda$ is invertible
    - if $B$ is a square matrix, consider $B+cI$, with $c\in\mathbb{R}$, $I$ is identity
    - Suppose that $\mu$ is an eigenvector of $B$ with eigenvalue $\lambda$
      - then $(B+cI)u = Bu + cIu = \lambda u + cu = (\lambda + c)u$
      - thus $u$ is an eigenvector of $B+cI$ with eigenvalue $\lambda + c$
    - the precision $b > 0$; $aA^TA$ is positive semidefinite
      - thus the eigenvalues of $aA^TA + bI$ are all greater than 0
      - and $\Lambda$ is invertible.
  - the MAP estimate is then $\mu = a\Lambda^{-1}A^Ty = (A^TA + \frac{b}{a}I)^{-1}A^Ty$
    - the MLE estimate from before is $(A^TA)^{-1}A^Ty$
    - the difference is $\frac{b}{a}I$ in the inverse. $\frac{b}{a}I$ is known as the "regularization parameter" related to $-b/2 w^Tw$
      - if $b$ goes to $0$, then the prior becomes more uniform - we approach universal prior and the MLE
- (note) For a general multivariate Gaussian, the exponent contains $(x-\mu)^T\Lambda(x-\mu)$,
  - $\Lambda^{-1}$ is the symmetric covariance matrix
  - This is then $x^T\Lambda x - 2x^T\Lambda \mu + C$, where $C$ is a constant
  - note: the $\Lambda$ is often represented by $\Sigma^{-1}$ in official definitions

## 10.4-10.7: Predictive distribution for linear regression 

- Let's calculate the predictive distribution. Given $x$, we want to calculate  $p(y|x,D)$
- $p(y|x,D) = \int p(y|x,D,w)p(w|x,D) dw = \int p(y|x,w)p(w|D)dw$
  - $\int N(y|w^Tx,a^{-1})N(w|\mu,\Lambda^{-1})dw\propto_y \int \exp \big(\frac{a}{2}(y-w^Tx)^2 - \frac{1}{2}(w-\mu)^T\Lambda(w-\mu)\big)dw$
    - we expand this, complete the square, etc
    - it ends up $\propto \int N(w|m,L)g(y)dw$, where
      - $L = axx^T+\Lambda$
      - $m = L^{-1}(ayx + \Lambda\mu)$
      - $g(y) = \exp\big(\frac{1}{2} m^TLm - \frac{1}{2}ay^2\big)$
    - then we pull the $g(y)$ out, and the integral evaluates to 1
      - note that $m$ depends on $y$, but this doesn't matter as the integral still is 1
    - thus $p(y|x,D)\propto \exp\big(\frac{1}{2} m^TLm - \frac{1}{2}ay^2\big)$
      - now we complete the square again with this function to get another gaussian in $y$
      - $p(y|x,D)\propto N(y|u,\lambda^{-1})$ with
        - $\lambda^{-1} = \big(a(1-ax^TL^{-1}x)\big)^{-1} = \frac{1}{a} + x^T\Lambda^{-1}x$
        - $u = \frac{1}{\lambda}ax^TL^{-1}\Lambda\mu = \mu^Tx$
        - the second equality uses the Sherman-Morrison formula and some more computations
          - have to show that $L$ is invertible, which is another computation
- The mean of the distribution is $\mu^Tx$
  - $\mu$ is the posterior mean for the weights $w$
  - so the mean is like we used the MAP estimate for $\theta$
- all this math worked because the exponents were all quadratics - and then we kept completing the square.

# 11: Estimators

## 11.1 Estimators

- Now assume data is $D = (X_1,\dots,X_n)$, with $X_i$ random variables
  - i.e. the data is a random variable
- Definition: a "statistic" is a random variable $S$ that is a function of the data $D$, e.g. $S = f(D)$
- Definition: an "estimator" is a statistic intended to approximate a parameters governing the distribution of $D$
- Notation: 
  - $\hat{\theta}$ denotes an estimator of $\theta$
  - $\hat{\theta}_n$ emphasizes the dependence on $n$
- Example: $X_1,\dots,X_n\sim N(\mu,\sigma^2)$ iid, we can consider the following estimators as functions of the data $D$
  - sample mean: $\hat{\mu} = \bar{x} = \frac{1}{n}\sum_i x_i$
  - biased sample variance $\hat{\sigma^2} = \frac{1}{n}\sum_i (x_i-\bar{x})^2$
    - $\sigma^2= \mathbb{E}((x-\mu)^2)$
  - unbiased sample variance $\hat{s^2} = \frac{1}{n-1}\sum_i (x_i-\bar{x})^2$
- Definition: the "bias" of an estimator $bias(\hat{\theta}) = \mathbb{E}\hat{\theta} - \theta$
- Definition: An estimator $\hat{\theta}$ is "unbiased" if $bias(\hat{\theta}) = 0$
- Examples: 
  - $\hat{\mu}$ is unbiased: $\mathbb{E}\hat{\mu} = \mathbb{E}\frac{1}{n}\sum_i X_i = \frac{1}{n}\sum \mathbb{E}X_i = \frac{1}{n}\sum _i \mu = \mu$
- Let's prove that the biased/unbiased sample variances are biased/unbiased
  - Note that $\sigma^2 = \mathbb{E}(X^2) - (\mathbb{E}(X))^2 = \mathbb{E}(X^2) - \mu^2$
  - for each $i$, $\mathbb{E}(X_i-\bar{X})^2 = \mathbb{E}(X_i^2) - 2\mathbb{E}(X_i\bar{X}) + \mathbb{E}\bar{X}^2$
    - each part:
      - $\mathbb{E}(X_i^2) = \sigma^2 + \mu^2$
      - $\mathbb{E}(X_i\bar{X}) = (1/n)(\mathbb{E}X_i^2 + (n-1)\mathbb{E}_{i\neq j}(x_ix_j)) = (1/n)(\sigma^2 + \mu^2 + (n-1)(\mu^2)) = \frac{\sigma^2}{n} + \mu^2$
      - $\mathbb{E}(\bar{X}^2) = \frac{1}{n^2}\mathbb{E}(\sum_i x_i)^2 = \frac{1}{n^2}\big(n\mathbb{E}(x^2) + (n^2-n)\mathbb{E}(x)\mathbb{E}(x)\big) = \frac{1}{n^2}\big(n(\sigma^2 + \mu^2) + (n^2-n)\mu^2\big) = \frac{\sigma^2}{n} + \mu^2$
    - combining: $\sigma^2 + \mu^2 - 2(\sigma^2/n + \mu^2) + \sigma^2/n + \mu^2 = \frac{n-1}{n}\sigma^2$
  - Thus $\mathbb{E}\hat{s^2} = \frac{1}{n-1}\mathbb{E}\sum_i (X_i-X)^2 = \frac{n}{n-1}\mathbb{E}(X_i-X)^2 = \frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2$, and the unbiased estimator is unbiased.
  - Similarly, $\mathbb{E}\hat{\sigma^2} = \frac{n-1}{n}\sigma^2$, so underestimates

# 11.2: Decision theory terminology in different contexts

- Contexts: General Setup, Estimators, and Regression/Classification
- In the general setup, we have a state $s$ and data $D$. We take an action $a = \delta(D)$ and get a loss $L(s,a)$, where $\delta$ is a decision rule.
- In the Estimator setup, we have a parameter $\theta$ and data $D$. We estimate $\hat{\theta} = g(D)$ and get a loss $L(\theta, \hat{\theta})$, where $g$ is an estimator function.
- in Regression/Classification, we have a target value $y$ and a point $x$. We predict $\hat{y} = f(x)$ and get a loss $L(y,\hat{y})$, where $f$ is a predictor function.
