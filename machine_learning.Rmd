---
title: "Machine Learning"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Machine learning: overview and applications

- Machine learning is defined here as algorithms for inferring unknowns from knowns
  - subfield of statistics focusing on algorithms
- Many different applications

# 1.2: What is supervised learning?

- Supervised learning: Given $(x^1,y^1),\dots,(x^n,y^n)$, where $x^i$ are data points, $y^i$ are class/value, we want to choose a function $f(x) = y$
  - and be able to generalize to new $x$ points.
  - Classification: $y^i\in \mathcal{Y}$, where $\mathcal{Y}$ is a finite set
  - Regression $y^i \in \mathbb{R}^d$
    - note that the word "regression" is historical

# 1.3: What is unsupervised learning

- Unsupervised learning is well defined
  - Given $(x^1,\dots,x^n)$, typically $x^i\in \mathbb{R}^k$, find "patterns" in the data
    - Clustering: separate data into clusters
    - Density estimation: find a function for the density of the data
    - Dimensionality reduction: find a lower dimensional space to represent data

# 1.4: Variations on supervised and unsupervised

- Semi-supervised Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - the task is to predict the labels $y^{k+1},\dots,y^n$
  - i.e. we know the location of the unlabeled points
- Active Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - can ask for $y^k$'s for particular points $x^k$
  - and after ask for a few points, make predictions on the rest
- Decision theory: How to decide to measure errors/loss functions
  - e.g. weight false positives more than false negatives (e.g. a doctor deciding to do a test)
- Reinforcement Learning: Make actions to maximize long term rewards and minimize overall losses, improve model as go along.

# 1.5: Generative vs discriminative models

- Discrimative: Model $\mathbb{P}(y|x)$, i.e. the class of a given point $x$
- Generative: Model the joint distribution $\mathbb{P}(x,y)$
- can use generative to discriminate by comparing probabilities
- creating generative models takes a lot of data, have to estimate things, etc, so often discriminative models are better - and more appropriate to the problem at hand.

# 1.6: $k$-Nearest Neighbor classification algorithm

- Given data $(x_i, y_i)$, $x_i\in \mathbb{R}^d$, $y_i\in \{0,1\}$, classify a new $x$
- $kNN$: by a majority vote of the $k$ nearest points.
- Distance given by, for example, the euclidean distance
- often works pretty well, has some nice theoretical guarantees as $n$ grows large
- probabilistic interpretation: $p(y)$ is the fraction of points among the $k$ nearest neighbors with label $y$, and we select the $y$ that maximizes this probability.
  - this is a discriminative model
- $k$ is a parameter, so we can select via cross-validation, bias variance tradeoff, etc.

# 2.1: Classification trees (CART)

- Decision trees are very simple, but can have pretty good performance (especially paired with randomization)
- Classification trees
  - main idea is to form a binary tree and minimize the error in each leaf of the tree
    - nodes are either 
      - a question/decision 
      - a leaf (which we can mark with the amount of each class in that leaf)
        - classify by majority vote at the leaf

# 2.2: Regression trees (CART)

- Regression tree
  - again, make a binary tree and minimize error in each leaf
    - for example, split on $x \leq r$, i.e. separate $x$-axis piecewise
      - put a constant model in each leaf (e.g. the average)
    - e.g. error as square error over all $y$'s
    - get a piecewise constant function

# 2.3: Growing a regression tree (CART)

- note that the $x$ can be real valued, discrete, or even unordered
- Growing a tree
  - Greedy: At each step, choose the optimal decision
    - may not get globally optimal tree
    - for example, if $x_i\in \mathbb{R}^d$, we find $s$ and $j$ to minimize the prediction as the minimum $y$ for each part:
      - $\min_y\sum_{i:x_{ij}>s}(y-y_i)^2 + \min_y\sum_{i:x_{ij}\leq s}(y-y_i)^2$
      - as we have a finite number of data points, we have in essence a finite number of split points
      - and repeat
    - how do we stop:
      - otherwise we end up with one point $x$ per leaf
      - only consider splits with regions with at least $n$ points
      - or something else
- we can prune afterwards or do something like random forest afterwards

# 2.4: Growing a classification tree (CART)

- suppose that the points are in $\mathbb{R}^d$, classified into classes
- given a subregion $R$, we classify by a majority/plurality vote
  - define $E_R = \min_y(1/N_R)\sum_{i: x_i\in R} I(y_i\neq y)$, 
    - $N_R$ is the number of train points in $R$
    - $I$ is the indicator function
  - define $R(j,s)$ to be $\{x_i: x_{ij} > s\}$, $R'(j,s)=\{x_i:x_{ij}\leq s\}$, i.e. the point sets defined by a split
- To grow a classification tree, choose a dimension $j$ and a split $s$ to minimize
  - $E_{R(j,s)} + E_{R'(j,s)}$
  - and repeat
- stop when 
  - number of points reach a minimum
  - $R_k$ contains points of only one class
  - or something else
- could also try to minimize entropy/gini index instead

# 2.5: Generalizations for trees (CART)

- impurity measures for classification:
  - $E_R$: the proportion of stuff that would be misclassified (which we used before)
  - $H_R$: entropy, i.e. if we define $p_R(y)$ to be the fraction of points in $R$ with class $y$
    - $H_R = -\sum_{y\in\mathcal{Y}} p_R(y)\log p_R(y)$
    - entropy for $R$ with all in one class is $0$
    - We want to find a split to make it as homogeneous as possible
  - $G_R$: Gini index, i.e.
    - $G_R = \sum_{y\in\mathcal{Y}} p_R(y) (1-p_R(y))$
    - has some nice analytic properties
- $H_R$ and $G_R$ can have some better performance
- more generalizations/topics:
  - the notion of a split if if the $x_i$ are categorical, so that splitting doesn't strictly make sense
  - use a loss matrix for loss instead
  - missing values: recommended solution is "surrogate variables"
  - splits on linear combinations instead of a coordinate
    - but this is less efficient and surprisingly doesn't really give better performance
      - especially with the upcoming aggregation techniques
  - instability: estimator can have high variance, so has low performance in general

# 2.6: Bootstrap Aggregation (Bagging)

- take a simple algorithm and make it better (not always, but quite often)
- reduces the variation of a classification or regression procedure
- here, we do regression
- Given
  - Data $D = (X_i,Y_i)$, distributed by $P$ iid, the $X$ and $Y$ are random variables
  - Want to predict the $y$ for an $x$
- If we had a bunch of data sets $D_i$, each gives a model to predict $x$
  - if each dataset is drawn from $P$, then the average of the $y$s will be pretty good as an estimate as the true $y$
  - $D_i = (X^i_j,Y^i_j)$, as $j$ ranges
  - suppose that we have an "unbiased" estimator, i.e. the mean of $Y_j$ is the true $Y$, i.e. $\mathbb{E}Y=y=f(x)$
  - $\mathbb{E}((Y-y)^2) = E((Y-\mathbb{E}(Y))^2) := \sigma^2(Y)$
  - define $z = (1/m)\sum_1^m Y^i$, then $\mathbb{E}(z)=y$, so $z$ is also an unbiased estimator of $y$
    - i.e. if we use the empirical distribution
  - Then $\mathbb{E}((Z-y)^2) = \mathbb{E}((z-\mathbb{E}(z))^2) = \sigma^2(z)=\sigma^2((1/m)\sum Y^i) = (1/m)^2\sigma^2(\sum Y^i) = (1/m^2)\sum(\sigma^2(Y^i)) = (1/m) \sigma^2(y)$
    - in other words, our expected loss from using $z$ to estimate is $(1/m) \sigma^2(y)$, 
      - i.e. the original estimator divided by $m$
      - so as we take $m$ to infinity, the expected loss goes to 0
- Unfortunately, we don't have a bunch of different datasets
  - but we can simulate this by sampling from our dataset
  - e.g. draw uniformly from the original dataset $D$, iid, with replacement

# 2.7: Bagging for classification

- natural extensions of previous one
  - majority vote of models (each dataset gives us a classifier $C_i$)
  - average the estimated probabilities from each classifier: each $C_i$ gives us an estimated pmf $p^i$, and then we do $p(y) = (1/m)\sum_i p^i(y)$
    - similar to the regression problem from before where we try to estimate the true probability of $y$
      - same analysis holds if the $p^i$ are unbiased - so $p$ is a unbiased estimator
    - and then classify $x$ as the $y$ with the highest $p(y)$ for $x$
  - suppose that the output follows a random distribution $W$, which is independent from $Y$ (the realizations in the form of data)
    - then we want to minimize $\mathbb{E}((Y-W)^2) = \mathbb{E}((Y-\mathbb{E}(Y) + \mathbb{E}(Y) - W)^2)$
    - then this is $\mathbb{E}((Y-\mathbb{E}(Y))^2) + \mathbb{E}((\mathbb{E}(Y) - W)^2)$
      - note that we used independentness to get rid of a few terms
    - $\geq \mathbb{E}((\mathbb{E}(Y)-W)^2)$
    - in other words, using $\mathbb{E}(Y)$ instead of $Y$ reduces expected error
    - as before, we can approximate $\mathbb{E}(Y)$ with $z$, i.e. the data that we have - and reduce randomness again
  - i.e. more rationalization of the bagging algorithm

# 2.8: Random forests

- Combination of CART trees and bagging - and "random subspace"
- Simple idea, but somehow state of the art performance
- Random Forests:
  - We have some observed data points $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - for $i = 1,\dots,B$, create a bootstrap data $D_i$ from $D$ via sampling with replacement
  - construct a tree $T_i$ using $D_i$ s.t.
    - at each node, choose a random subset of the features (e.g. dimensions of $x_i$)
    - only consider splits on those features (this is the "random subspace")
  - Then do the aggregation as before on these $B$ trees $T_i$: majority vote (classification), average of probabilities (classification), average of predictions (regression)
- works for the same reason that bootstrapping works
  - each tree has high variance, but boostrapping reduces the variance
