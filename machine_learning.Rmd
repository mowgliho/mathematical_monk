---
title: "Machine Learning"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1: Overview, example

## 1.1: Machine learning: overview and applications

- Machine learning is defined here as algorithms for inferring unknowns from knowns
  - subfield of statistics focusing on algorithms
- Many different applications

## 1.2: What is supervised learning?

- Supervised learning: Given $(x^1,y^1),\dots,(x^n,y^n)$, where $x^i$ are data points, $y^i$ are class/value, we want to choose a function $f(x) = y$
  - and be able to generalize to new $x$ points.
  - Classification: $y^i\in \mathcal{Y}$, where $\mathcal{Y}$ is a finite set
  - Regression $y^i \in \mathbb{R}^d$
    - note that the word "regression" is historical

## 1.3: What is unsupervised learning

- Unsupervised learning is well defined
  - Given $(x^1,\dots,x^n)$, typically $x^i\in \mathbb{R}^k$, find "patterns" in the data
    - Clustering: separate data into clusters
    - Density estimation: find a function for the density of the data
    - Dimensionality reduction: find a lower dimensional space to represent data

## 1.4: Variations on supervised and unsupervised

- Semi-supervised Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - the task is to predict the labels $y^{k+1},\dots,y^n$
  - i.e. we know the location of the unlabeled points
- Active Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - can ask for $y^k$'s for particular points $x^k$
  - and after ask for a few points, make predictions on the rest
- Decision theory: How to decide to measure errors/loss functions
  - e.g. weight false positives more than false negatives (e.g. a doctor deciding to do a test)
- Reinforcement Learning: Make actions to maximize long term rewards and minimize overall losses, improve model as go along.

## 1.5: Generative vs discriminative models

- Discrimative: Model $\mathbb{P}(y|x)$, i.e. the class of a given point $x$
- Generative: Model the joint distribution $\mathbb{P}(x,y)$
- can use generative to discriminate by comparing probabilities
- creating generative models takes a lot of data, have to estimate things, etc, so often discriminative models are better - and more appropriate to the problem at hand.

## 1.6: $k$-Nearest Neighbor classification algorithm

- Given data $(x_i, y_i)$, $x_i\in \mathbb{R}^d$, $y_i\in \{0,1\}$, classify a new $x$
- $kNN$: by a majority vote of the $k$ nearest points.
- Distance given by, for example, the euclidean distance
- often works pretty well, has some nice theoretical guarantees as $n$ grows large
- probabilistic interpretation: $p(y)$ is the fraction of points among the $k$ nearest neighbors with label $y$, and we select the $y$ that maximizes this probability.
  - this is a discriminative model
- $k$ is a parameter, so we can select via cross-validation, bias variance tradeoff, etc.

# 2: Trees

## 2.1: Classification trees (CART)

- Decision trees are very simple, but can have pretty good performance (especially paired with randomization)
- Classification trees
  - main idea is to form a binary tree and minimize the error in each leaf of the tree
    - nodes are either 
      - a question/decision 
      - a leaf: classify by majority vote at the leaf

## 2.2: Regression trees (CART)

- Regression tree
  - again, make a binary tree and minimize error in each leaf
    - for example
      - split on $x \leq r$, i.e. separate $x$-axis piecewise
      - put a constant model in each leaf (e.g. the average)
    - e.g. error as square error over all $y$'s
    - get a piecewise constant function

## 2.3: Growing a regression tree (CART)

- note that the $x$ can be real valued, discrete, or even unordered
- Growing a tree
  - Greedy: At each step, choose the optimal decision
    - may not get globally optimal tree
    - for example, if $x_i\in \mathbb{R}^d$, we find $s$ and $j$ to minimize the prediction as the minimum $y$ for each part:
      - $\min_y\sum_{i:x_{ij}>s}(y-y_i)^2 + \min_y\sum_{i:x_{ij}\leq s}(y-y_i)^2$
        - $y$ is the prediction for each part
      - as we have a finite number of data points, we have in essence a finite number of split points
      - and repeat
    - how do we stop:
      - otherwise we end up with one point $x$ per leaf
      - only consider splits with regions with at least $n$ points
      - or something else
- we can prune afterwards or do something like random forest afterwards

## 2.4: Growing a classification tree (CART)

- suppose that the points are in $\mathbb{R}^d$, classified into classes
- given a subregion $R$, we classify by a majority/plurality vote
  - define $E_R = \min_y(1/N_R)\sum_{i: x_i\in R} I(y_i\neq y)$, 
    - $N_R$ is the number of train points in $R$
    - $I$ is the indicator function
  - define $R(j,s)$ to be $\{x_i: x_{ij} > s\}$, $R'(j,s)=\{x_i:x_{ij}\leq s\}$, i.e. the point sets defined by a split
- To grow a classification tree, choose a dimension $j$ and a split $s$ to minimize
  - $E_{R(j,s)} + E_{R'(j,s)}$
  - and repeat
- stop when 
  - number of points reach a minimum
  - $R_k$ contains points of only one class
  - or something else
- could also try to minimize entropy/gini index instead

## 2.5: Generalizations for trees (CART)

- impurity measures for classification:
  - $E_R$: the proportion of stuff that would be misclassified (which we used before)
  - $H_R$: entropy, i.e. if we define $p_R(y)$ to be the fraction of points in $R$ with class $y$
    - $H_R = -\sum_{y\in\mathcal{Y}} p_R(y)\log p_R(y)$
    - entropy for $R$ with all in one class is $0$
    - We want to find a split to make it as homogeneous as possible
  - $G_R$: Gini index, i.e.
    - $G_R = \sum_{y\in\mathcal{Y}} p_R(y) (1-p_R(y))$
    - has some nice analytic properties
- $H_R$ and $G_R$ can have some better performance
- more generalizations/topics:
  - the notion of a split if if the $x_i$ are categorical, so that splitting doesn't strictly make sense
  - use a loss matrix for loss instead
  - missing values: recommended solution is "surrogate variables"
  - splits on linear combinations instead of a coordinate
    - but this is less efficient and surprisingly doesn't really give better performance
      - especially with the upcoming aggregation techniques
  - instability: estimator can have high variance, so has low performance in general

## 2.6: Bootstrap Aggregation (Bagging)

- take a simple algorithm and make it better (not always, but quite often)
- reduces the variation of a classification or regression procedure
- here, we do regression
- Given
  - Data $D = (X_i,Y_i)$, distributed by $P$ iid, the $X$ and $Y$ are random variables
  - Want to predict the $y$ for an $x$
- If we had a bunch of data sets $D_i$, each gives a model to predict $x$
  - if each dataset is drawn from $P$, then the average of the $y$s will be pretty good as an estimate as the true $y$
  - $D_i = (X^i_j,Y^i_j)$, as $j$ ranges
  - suppose that we have an "unbiased" estimator, i.e. the mean of $Y_j$ is the true $Y$, i.e. $\mathbb{E}Y=y=f(x)$
  - $\mathbb{E}((Y-y)^2) = E((Y-\mathbb{E}(Y))^2) := \sigma^2(Y)$
  - define $z = (1/m)\sum_1^m Y^i$, then $\mathbb{E}(z)=y$, so $z$ is also an unbiased estimator of $y$
    - i.e. if we use the empirical distribution
  - Then $\mathbb{E}((Z-y)^2) = \mathbb{E}((z-\mathbb{E}(z))^2) = \sigma^2(z)=\sigma^2((1/m)\sum Y^i) = (1/m)^2\sigma^2(\sum Y^i) = (1/m^2)\sum(\sigma^2(Y^i)) = (1/m) \sigma^2(y)$
    - in other words, our expected loss from using $z$ to estimate is $(1/m) \sigma^2(y)$, 
      - i.e. the original estimator divided by $m$
      - so as we take $m$ to infinity, the expected loss goes to 0
- Unfortunately, we don't have a bunch of different datasets
  - but we can simulate this by sampling from our dataset
  - e.g. draw uniformly from the original dataset $D$, iid, with replacement

## 2.7: Bagging for classification

- natural extensions of previous one
  - majority vote of models (each dataset gives us a classifier $C_i$)
  - average the estimated probabilities from each classifier: each $C_i$ gives us an estimated pmf $p^i$, and then we do $p(y) = (1/m)\sum_i p^i(y)$
    - similar to the regression problem from before where we try to estimate the true probability of $y$
      - same analysis holds if the $p^i$ are unbiased - so $p$ is a unbiased estimator
    - and then classify $x$ as the $y$ with the highest $p(y)$ for $x$
  - suppose that the output follows a random distribution $W$, which is independent from $Y$ (the realizations in the form of data)
    - then we want to minimize $\mathbb{E}((Y-W)^2) = \mathbb{E}((Y-\mathbb{E}(Y) + \mathbb{E}(Y) - W)^2)$
    - then this is $\mathbb{E}((Y-\mathbb{E}(Y))^2) + \mathbb{E}((\mathbb{E}(Y) - W)^2)$
      - note that we used independentness to get rid of a few terms
    - $\geq \mathbb{E}((\mathbb{E}(Y)-W)^2)$
    - in other words, using $\mathbb{E}(Y)$ instead of $Y$ reduces expected error
    - as before, we can approximate $\mathbb{E}(Y)$ with $z$, i.e. the data that we have - and reduce randomness again
  - i.e. more rationalization of the bagging algorithm

## 2.8: Random forests

- Combination of CART trees and bagging - and "random subspace"
- Simple idea, but somehow state of the art performance
- Random Forests:
  - We have some observed data points $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - for $i = 1,\dots,B$, create a bootstrap data $D_i$ from $D$ via sampling with replacement
  - construct a tree $T_i$ using $D_i$ s.t.
    - at each node, choose a random subset of the features (e.g. dimensions of $x_i$)
    - only consider splits on those features (this is the "random subspace")
  - Then do the aggregation as before on these $B$ trees $T_i$: majority vote (classification), average of probabilities (classification), average of predictions (regression)
- works for the same reason that bootstrapping works
  - each tree has high variance, but boostrapping reduces the variance

# 3: The Big Picture

## 3.1: Decision Theory (Basic Framework)

- Decision Theory: minimize expected loss
- Loss Matrix:
  - e.g., look at FP, TP, FN, TN table, each has a loss
  - Define a loss function $L(y,\hat{y})$
    - "0-1 loss": $L(y,\hat{y}) = I(y\neq \hat{y})$, $I$ being the indicator function
    - "Square loss": $L(y,\hat{y}) = (y-\hat{y})^2$
- General Framework
  - State $s$ (true/unknown)
  - Have an observation (known)
  - take an action $a$
  - get a loss $L(s,a)$
- note that we can do the opposite and do "reward/utility" instead

## 3.2: Minimizing conditional expected loss

- Decision theory for supervised learning
  - Setup
    - Data $\{(x_i,y_i)\}$
  - Possible Situation 1:
    - get a new $x$, predict $\hat{y}$ for the true $y$
    - given $x$, we want to pick $y$ to minimize loss $L(y,\hat{y})$
    - but we don't know the true $y$
  - Possible Situation 2:
    - choose $f$ to minimize $L(y,f(x))$, but we don't know $x$ or $y$
    - i.e. choose a general function to minimize the $x$ that may come
- We move to Probabilistic framework: Want small loss on average
  - Data comes as $(X,Y)$, where $X$ and $Y$ are random variables according to $p$
    - for now, we assume that they are discrete
  - Possible Situation 1 now turns into minimizing 
    - $\mathbb{E}(L(Y, \hat{y})|X = x) = \sum_{y\in\mathcal{Y}}L(y,\hat{y})p(y|x)$
    - e.g. for 0-1 loss, we get $\sum_{y\neq \hat{y}}p(y|x) = 1 - p(\hat{y}|x)$
      - so we should pick $\hat{y}$ to maximize $p(\hat{y}|x)$
      - i.e. minimizing the expected 0-1 loss tells us to predict the most likely $y$

## 3.3: Choosing $f$ to minimize expected loss

- Now let's look at Possible Situation 2 from above
  - $\hat{y} = f(x)$
  - $\mathbb{E}(L(Y,\hat{Y})) = \mathbb{E}(L(Y,f(x))) = \sum_{x,y} L(y,f(x))p(x,y) = \sum_{x,y}L(y,f(x))p(y|x)p(x) = \sum_xp(x)\sum_yL(y,f(x))p(y|x)$
  - setting $g(x,f(x)):= \sum_y L(y,f(x))p(y|x)$, this turns into the expectation $\mathbb{E}^Xg(X,f(X))$ over the distribution $p(x)$.
  - Suppose for some $x',t$, $g(x',f(x')) > g(x',t)$.
    - define $f_0(x)$ to be $f(x)$ if $x\neq x'$, and $t$ if $x = x'$
    - then $g(x,f(x))\geq g(x,f_0(x))$
    - in particular, $\mathbb{E}^X(g,X,f(X)) \geq \mathbb{E}^X(g(X,f_0(X)))$
      - in other words to minimize the loss, we should choose $f$ to minimize $g(x,f(x))$, i.e. $\arg\min_t g(x,t)$
  - The minimization problem for situation 2 reduces to many cases of the minimization problem for situation 1
    - the fact that we don't know $x$ yet doesn't really matter
    - more important is $p(y|x)$

## 3.4: Square Loss

- Consider the square loss $L(y,\hat{y}) = (y-\hat{y})^2$, with $X,Y$ given by $p$
  - no longer discrete, suppose that $p$ is a smooth and has a derivative
  - $\mathbb{E}(L(Y,\hat{y})|X=x) = \int L(y,\hat{y})p(y|x)dy = \int(y-\hat{y})^2p(y|x)dy$
  - take the derivative wrt $\hat{y}$ and set to 0
    - $0 = \int 2(\hat{y}-y)p(y|x)dy = 2\hat{y} \int p(y|x)dy - 2\int yp(y|x)dy = 2\hat{y} - 2\mathbb{E}(Y|X=x)$
    - $\hat{y} = \mathbb{E}(Y|X=x)$
    - if we take another derivative, we note that it is positive $2$, so that we've found a minimum
  - in other words, the prediction at $x$ to minimize the expected square loss is the expectation of $Y$ given $x$
    - this is a nice property of the square loss
- the solution to the minimum problem for the square loss is to set $f(x):=\mathbb{E}(Y|X=x)$, i.e. the expectation of $Y$ given $x$ at each $x$

## 3.4: The Big Picture (part 1)

- Specifically focusing on supervised learning, but the concepts generalize
- many of the core concepts of machine learning come from trying to minimizing $\mathbb{E}(L(Y,f(X)))$
- $p(y|x)$, not $p(x)$ was key to minimizing expected loss
  - Discriminative methods: Estimate $p(y|x)$ directly using data $D$
    - e.g. kNN, Trees, SVM
    - easier to compute
  - Generative methods: Estimate the joint distribution $p(y,x)$ and then recover $p(y|x) = p(x,y)/p(x)$
    - harder to compute, but are a richer family
- parameters/latent variables: we denote as $\theta$. We can think of it as a random variable and consider $p(x,y|\theta)$
- we can also think of the data as being random: $p(y|x,D) = \int_\theta p(y|\theta,x,D)p(\theta|x,D)d\theta$
  - i.e. integrate over possible parameters $\theta$
  - $p(y|x,D,\theta)$ is typically nice, can get a defined/analytic model
  - $p(\theta|x,D)$ is nasty: usually can't get a closed form expression
  - the integral sucks too.
  - this problem often intractable

## 3.5: The Big Picture (part 2)

- notation:
  - $p(y|x,D)$ is the "predictive distribution" on $y$
  - $p(\theta|x,D)$ is the "posterior distribution on $\theta$
- approaches to solve the intractable problem from before
  - Exact inference: assume a nice enough model that has nice distributions
    - Multivariate Gaussian, Conjugate prior, Graphical models
  - Point estimates of $\theta$: 
    - e.g. 
      - MLE: $\theta_{MLE} = \arg\max_\theta \mathcal{L}(\theta|y)$, where $\mathcal{L}$ is the likelihood function
      - MAP: $\theta_{MAP} = \arg\max_\theta p(\theta | x,D)$
    - then estimate $p(y|x,D)\approx p(y|x,D,\theta)$
    - to solve for $\theta$, may need to use optimization techniques or expectation maximization
    - Empirical Bayes kind of fits in here too
  - Deterministic Approximations of the integral
    - Laplace Approximation, Variational methods, Expectation Propagation
  - Stochastic Approximations
    - Markov Chain Monte Carlo (Gibbs, Metropolis-Hastings), Importance Sampling
- many methods combine these techniques

## 3.6: The Big Picture (part 3)

- Extensions of methods above to unsupervised learning
- Density Estimation (Unsupervised)
  - $D = (x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$, iid
  - Goal is to estimate the density of the $x$
  - given parameters $\theta$, we can have a probability distribution $p_\theta$ that generates the $x_i$
  - we want to calculate $p(x|D) = \int p(x|D,\theta)p(\theta|D)d\theta$
    - it turns out that we can reduce to $p(x|\theta,D) = p(x|\theta)$
    - $p(\theta|D)$ is the posterior of $\theta$ given $D$
  - same problem as before, and we can apply the same types of methods as before:
    - $p(x|\theta)$ is nice: just the model
    - $p(\theta|D)$ and the integral are not nice.

# 4: MLE

## 4.1: Maximum Likelihood Estimation (MLE) (part 1)

- Setup:
  - Given data $D = (x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$
  - assume a set of distributions $\{p_\theta:\theta \in\Theta\}$ on $\mathbb{R}^d$
  - Assume that $D$ is an iid sample according to $p_\theta$ for some $\theta\in\Theta$
- Goal: Estimate the true value of $\theta$ that generated the data
- Definition: $\theta_{MLE}$ is a MLE for $\theta$ if $\theta_{MLE}=\arg\max_\theta p(D|\theta)$
  - more precisely, it maximizes $p(D|\theta_{MLE}) = \max_\theta p(D|\theta)$, as there can be multiple $\theta$ that maximize it in theory.
  - The likelihood $p(D|\theta) = \prod_1^n p(x_i|\theta)$, as they are iid.
- Remark:
  - MLE may not be unique
  - MLE may fail to exist (e.g. maximum of $p(D|\theta)$ may not be achieved for $\theta \in \Theta$)
- Pros:
  - Easy to compute
  - interpretable
  - has some nice asymptotic properties:
    - consistent: as $n$ goes to infinity, it converges to the true $\theta$
    - normal: as $n$ goes to infinity, the distrubution of $\theta$ is normal
    - efficient: it's the best possible estimate for the true $\theta$ because it has the lowest asymptotic variance/error
  - invariant under reparameterization of $\theta$: $g(\theta_{MLE})$ is the MLE for $g(\theta)$ for some function $g$
- Cons:
  - Point estimate, so has no representation of uncertainty (i.e. we don't consider $p(\theta|D)$)
    - for example, perhaps the likelihood spikes to a high maximum, but has a larger/more probable distribution elsewhere

## 4.2: Maximum Likelihood Estimation (MLE) (part 2)

- more cons of MLE
  - Overfitting
    - regression: e.g. curvy line when straight line is better
    - black swan paradox: give 0 probability to stuff not in data
  - Wrong objective? e.g. MLE disregards the loss function
  - Existence and Uniqueness is not guaranteed

## 4.3: MLE for univariate Gaussian mean

- Remember: $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
- We assume a univariate gaussian $X \sim N(\theta,\sigma^2)$
- $p(x|\theta) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp (-\frac{1}{2\sigma^2}(x-\theta)^2)$
- Data is $D=(x_1,\dots,x_n)$, $X_i \sim N(\theta, \sigma^2)$ iid
- Then $p(D|\theta) = p(x_1,\dots,x_n|\theta) = \prod_1^np(x_i|\theta)$
- If we plug in the equation for $p(x_i|\theta)$ and take the derivative
  - note that maximizing the probability is the same as maximizing the log,
  - so we can maximize $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_1^n(x_i-\theta)^2$
  - differentiate and set to 0 w.r.t $\theta$: $0 = \frac{1}{2\sigma^2}\sum_i 2(x_i - \theta)$
    - $n\theta = \sum_i x_i$, so $\theta = \frac{1}{n}\sum_i x_i$
  - take the second derivative wrt $\theta$ we get $-n/\sigma^2 < 0$, so that this is a maximum
  - thus $\theta$ is given by the average of the $x_i$'s, ie the sample mean
- if you take the derivative wrt to $\sigma$, you obtain $\sigma^2 = \frac{\sum(x_i-\theta)^2}{n}$, i.e. the sample variance

## 4.4: MLE for a PMF on a finite set (part 1)

- $X \sim p$, $X\in \{1,\dots,m\}$
  - for example, look at a die: $m = 6$
  - Given data, the MLE finds the distribution of $p$
- Data $D = (x_1,\dots, x_n)$, $X_1,\dots,X_n\sim p$ iid
- We parameterize $\theta = (\theta_1,\dots,\theta_n)$, where $\theta = p_i = p(i|\theta) = p_\theta(i)$
- We want to find the MLE $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
- $p(x|\theta) = \theta_x$, so $p(D|\theta) = \prod_i \theta_{x_i} = \prod_{i=1}^n \prod_{j=1}^m \theta_j^{I(x_i = j)} = \prod_{j=1}^m\theta_j^{\sum_{i=1}^mI(x_i=j)} = \prod_{j=1}^m \theta_j^{n_j}$
  - $n_j$ is the number of $j$s in the data
- to maximize $p(D|\theta)$, we can take the log, so we want to maximize $\sum_{j=1}^m n_j\log \theta_j$
- we can also max if we divided by $n$, i.e. $\sum_{j=1}^m \frac{n_j}{n}\log \theta_j$

## 4.5: MLE for a PMF on a finite set (part 2)

- continuing from last time
- set $q_j = n_j/n$
  - note that $\sum_j q_j = 1$
- Now consider relative entropy or Kullback-Leibler divergence, $D(\theta ||q)$
  - $D(\theta ||q) = \sum_{j=1}^m q_j \log (q_j/\theta_j)$
  - $D(\theta ||q) \geq 0$ and is $0$ iff $\theta = q$
- we are trying to maximize $\sum_{j=1}^m q_j \log \theta_j = \sum_{j=1}^m q_j \log(\theta_j/q_j) + \sum_{j=1}^m q_j\log q_j = -D(\theta || q) - H(q)$
  - As $H(q)$ is unaffected by $\theta$, this is maximized when $\theta = q$, 
- $\theta_{MLE} = (\theta_1,\dots,\theta_n)$, where $\theta_j = n_j/n$, or the proportion of the data that is $j$
  - i.e. the empirical distribution
 
# 5: Exponential Families

## 5.1-5.2: Exponential Families

- many common distributions are cases of exponential families
- have some nice properties
- Definition: An exponential family is a set $\{p_\theta: \theta \in \Theta\}$ of pmfs or pdfs on $\mathbb{R}^d$ such that $p_\theta(x) = \exp (\sum_{i=1}^m\eta_i(\theta)s_i(x))h(x)/z(\theta)$ where:
  - $\theta\in\mathbb{R}^k$: parameter
  - $x\in\mathbb{R}^d$: input
  - $\eta_i:\Theta \rightarrow\mathbb{R}$: a function of the parameters
  - $s_i:\mathbb{R}^d\rightarrow\mathbb{R}$: "sufficient statistics"
  - $h:\mathbb{R}^d\rightarrow[0,\infty)$: support/scaling of the distribution
  - $z:\Theta\rightarrow[0,\infty)$: "partition function", normalizing constant
- in vector form: $p_\theta(x) = \exp(\eta(\theta)^Ts(x)) h(x)/z(\theta)$
- Examples
  - Exponential distribution: $\theta > 0$, $p_\theta(x)=\theta e^{-\theta x}I(x\geq 0)$
    - $\eta(\theta) = \theta$, $s(x) = -x$, $h(x)=I(x\geq 0)$, $z(\theta)=1/\theta$
    - $\Theta = (0,\infty)\subset \mathbb{R}$, $k=1$, $d=1$
  - Bernoulli distribution: $p_\theta(x) = \theta^{I(x = 1)}(1-\theta)^{I(x = 0)}$, $x\in \{0,1\}$ (0 otherwise), $\theta\in (0,1)$
    - this is equal to $\exp\log(\theta^{I(x = 1)}(1-\theta)^{I(x = 0)}) = \exp(I(x=1)\log\theta + I(x=0)\log(1-\theta))$
      - $\eta_1(\theta) = \log(\theta)$, $\eta_2(\theta)=\log(1-\theta)$, $s_1(x) = I(x=1)$, $s_2(x) = I(x = 0)$, $h(x) = I(x\in\{0,1\})$ $z(\theta) = 1$
    - we can also write as $p_\theta(x) = h(x)\theta^{I(x=1)}(1-\theta)^{1-I(x=1)}$
      - or $(1-\theta)(\theta/(1-\theta))^{I(x=1)}h(x)$
      - do $\exp\log$, get $h(x)(1-\theta)\exp(I(x=1))\log(\theta/(1-\theta))$
      - $\eta(\theta)=\log(\theta/(1-\theta))$, $s(x)=I(x=1)$, $z(\theta)=1/(1-\theta)$
    - in other words, we can also write it in a different way, with a different number of functions
- We say that it is in "natural" or "canonical" form if $\eta(\theta) = \theta$, i.e. $\eta_i(\theta)=\theta_i$ for $i = 1,\dots,m$ (the projection of $\theta \in \mathbb{R}^k$ to one dimension)
  - Turns out that all exponential families can be put into canonical form
- Other examples of Exponential Families
  - PDF: Exponential, Normal, Beta, Gamma, $\chi^2$
  - PMF: Bernoulli, Binomial, Poisson, Geometric, Multinomial
- Exponential families
  - always have conjugate priors
  - arise as solutions to maximum entropy problems
- Exponential of a non-exponential family: The uniform distribution on $(0,\theta)$
  - support depends on $\theta$ not $x$
  - note that if we consider $\theta$ to be a constant (i.e. not a parameter), this can trivially be put into exponential family form.

## 5.3-5.4: MLE for an exponential family

- Assume that our family is in natural form, i.e.
  - $x\in\mathbb{R}^d$, $\theta \in \mathbb{R}^k$
  - $p_\theta(x)=e^{\theta^Ts(x)}h(x)/z(\theta)$
- have some data $D=(x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$ generated by $p_\theta$ iid 
- $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
  - $p(x|\theta) = e^{\theta^Ts(x)}h(x)/z(\theta)$
  - $p(D|\theta) = \prod_{i=1}^n p(x_i|\theta) = z(\theta)^{-n}e^{\theta^T\sum_{i=1}^ns(x_i)}\prod_{i=1}^nh(x_i)$
  - let $s(D):= \sum_{i=1}^ns(x_i)$
  - we want to maximize wrt to $\theta$
    - same as maximizing $\log p(D|\theta) = -n\log(z(\theta)) + \theta^Ts(D) + \sum_{i=1}^n\log(h(x_i))$
    - We set the gradient wrt each $\theta_j$ to zero
      - $\frac{\partial}{\partial \theta_j} \log p(D|\theta) = -n \frac{\partial}{\partial\theta_j} \log z(\theta) + s_j(D) = 0$
      - $\frac{\partial}{\partial\theta_j}\log z(\theta)$
        - Note that $\int_{\mathbb{R}^d}p(x|\theta) dx = 1$
        - $z(\theta) = \int_{\mathbb{R}^d}e^{\theta^Ts(x)}h(x)dx$
        - assuming smoothness
        - $\frac{\partial}{\partial \theta_j}\log z(\theta) = \frac{1}{z(\theta)}\frac{\partial}{\partial \theta_j}z(\theta) = \frac{1}{z(\theta)}\int_{\mathbb{R}^d}s_j(x)e^{\theta^Ts(x)}h(x)dx = \int_{\mathbb{R}^d} s_j(x)p_\theta(x)dx = \mathbb{E}_\theta(s_j(X))$
          - i.e. $\frac{\partial}{\partial\theta_j}\log z(\theta)$ is the expected value of $s_j$ as $x$ varies (for a given $\theta$.)
        - in other words $\nabla\log z(\theta) = \mathbb{E}_\theta s(X)$
      - $0 = -n\mathbb{E}_\theta s(X) + s(D)$
      - $n\mathbb{E}_\theta s(X) = s(D) = \sum_{i=1}^ns(x_i)$
      - $\mathbb{E}_\theta s(X) = \frac{1}{n}\sum_{i=1}^n s(x_i)$
      - in other words, the critical point of $p(D|\theta)$ satisfies the property that the mean of the sufficient statistics $s$ is the same as the sample mean of the suffiient statistics
        - if the MLE exists and $\theta_{MLE}\in Int(\Theta)$, i.e. the interior of $\Theta$, it also satisfies this
        - it turns out that if $\theta$ satisfies this property and is in the interior of $\Theta$, then $\theta$ is the MLE
          - by taking another derivative and showing that it's positive definite
- Example: Take an exponential distribution $p_\theta(x) = \theta e^{-\theta x} I(x\geq 0)$
  - $z(\theta) = 1/\theta$, $s(x) = -x$, $\eta(\theta) = \theta$
  - So we can apply the results from before
    - mean
      - from the definition of $z$, $\frac{\partial}{\partial \theta}\log z(\theta) = -1/\theta$
      - from the definition of $s$ and the result above, $\mathbb{E}s(x) = -\mathbb{E}(x) = -1/\theta$
      - $\mathbb{E}(x) = 1/\theta$
    - MLE
      - $-1/\theta = \mathbb{E}_\theta s(x) = (1/n)\sum_{i=1}^n(-x_i)$
      - $1/\theta = (1/n)\sum_{i=1}^n x_i$
      - $\theta_{MLE} = \frac{1}{\frac{1}{n}\sum_{i=1}^n x_i}$

# 6: MAP

## 6.1: Maximum a posteriori (MAP) estimation

- Setup
  - data $D=(x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$
  - assume a joint distribution $p(D,\theta)$, $\theta$ as a random variable
    - can factorize as $p(D|\theta)p(\theta)$, where $p(\theta)$ is the prior
  - Goal: choose a value of $\theta_{MAP}$ for $D$ that satisfies $\arg\max_\theta p(\theta|D)$
  - contrast with $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
  - in practice, MAP is done by maximizing $p(D|\theta)p(\theta)/p(D)$ by Bayes rule
    - and we drop the $p(D)$ because it doesn't change with $\theta$
- MLE maximimizes the likelihood, MAP maximizes the posterior distribution
- MAP Pros
  - Easy to compute
  - interpretable
  - Avoids overfitting: "Regularization"/"Shrinkage"
  - Tends to look like the MLE asymptotically
    - eventually, your priors get overwhelmed by your data
- MAP Cons
  - Point estimate: no representation of uncertainty in $\theta$
  - not invariant under reparameterization
    - if $\tau = g(\theta)$, then $\tau_{MLE} = g(\theta_{MLE})$: this doesn't hold for MAP
  - Must assume a prior distribution on $\theta$

## 6.2: MAP for univariate Gaussian mean

- Data $D = (x_1,\dots,x_n)$, $x_i\in\mathbb{R}$
- Suppose $\theta$ is a random variable distributed normally with unit variance: $N(\mu,1)$
- $X_1,\dots,X_n$ are conditionally independent given $\theta$ and distributed as $N(\theta,\sigma^2)$
  - we assume $\sigma^2$ is known.
  - $p(D|\theta) = \prod_{i=1}^n p(x_i|\theta)$
- $\theta_{MAP} = \arg\max_\theta p(\theta|D) = \arg\max_\theta p(\theta|D)p(\theta)/p(D) = \arg\max p(\theta|D)p(\theta) = \arg\max_\theta (\log p(D|\theta) + \log p(\theta))$
- take the derivative, set to 0
  - $0 = \frac{1}{\sigma^2}(\sum_{i=1}^n x_i - n\theta) + (\mu - \theta)$
  - $\theta = (\frac{1}{\sigma^2}\sum x_i + \mu)/(n/\sigma^2 + 1) = \frac{\sum x_i \mu\sigma^2}{n + \sigma^2} = \frac{n}{n+\sigma^2}\bar{x} + \frac{\sigma^2}{n+\sigma^2}\mu$
- verify that the second deriative is negative
- so the new estimate is a convex combination of the sample mean $\bar{x}$ and the prior mean $\mu$

## 6.3: Interpretation of MAP as convex combination

- from before, the MAP is a convex combination of the sample mean $\bar{x}$ and the prior mean $\mu$ for the univariate Gaussian mean
  - as we get more data, we lean towards the sample mean $\bar{x}$ (the MLE estimate)
  - with less data, we default to the prior mean $\mu$
- this happens often with the MAP: that it is a convex combination of the prior and the MLE

# 7: Bayesian Inference

## 7.1: Bayesian inference - A simple example

- Bayesian inference: put distributions on all the things and then use rules of probability
- Suppose Tom is going to XY University and joins the football team
  - Coach suspects that the football field isn't actually 100 yards long
  - Tom only has a yardstick
    - suppose he measures $(x_1,x_2,x_3) = (101, 100.5, 101.5)$
    - Assume that the $x_i$ are normally iid distributed $N(\theta,1)$, where $\theta$ is the true length
    - $\theta_{MLE} = \bar{x} = 101$
    - haven't accounted for the prior knowledge of how long football fields should be
    - make a prior $\theta ~ N(100,1)$
    - then $\theta_{MAP} = 100.75$
  - apparently coach is convinced that it is shorter than 100 yards
  - Tom considers $P(\theta < 100 | D)$
    - so he considers the posterior $P(\theta|D) = P(D|\theta)P(\theta)/P(D)$
      - $P(D|\theta) = \prod_i P(x_i|\theta)$
      - turns out that $P(\theta|D)$ is also a gaussian
    - and then can solve this problem
  - what about $P(x|D)$, i.e. the predictive distribution
    - $P(x|D) = \int_\theta P(x,\theta|D) d\theta = \int_\theta p(x|\theta) p(\theta|D)$ due to conditional independence
      - this is also a gaussian, nicely

## 7.2: Aspects of Bayesian inference

- Bayesian Inference: Assume a prior distribution $p(\theta)$, use probability to infer the answers that we're looking for
- Bayesian procedures: Minimize expected loss, averaging over $\theta$
- Objective vs Subjective Bayes approach
  - Subjective: choose a prior based on beliefs
  - Objective: choose "non-informative" priors
- Pros
  - Directly answer questions
  - Avoid pathological behaviors with frequentist approaches
  - by using a prior, can avoid some overfitting
  - Model selection ("Occam's razor")
  - Admissible (not always the best, but there is no other procedure that is "always better")
- Cons
  - must assume a prior
  - Exact computation can be intractable (have to take an integral over the $\theta$, etc.)
- Priors:
  - Non-informative
  - Improper: techinically not a probability distribution, but we use it proportionally
    - e.g. $\theta \in \mathbb{R}$ universally distributed, do $p(\theta) = 1$
  - Conjugate priors: very nice to work with

## 7.3: Proportionality

- Notation $f\propto g$ if $g(x) = cf(x)$ for all $x$ for a $c\neq 0$
  - also implies $g\propto f$
- If $f$ is a pdf and $f\propto g$, then $g$ uniquely determines $f$, and $f(x) = g(x)\int_x g(x) dx$
  - i.e. $g$ is "unnormalized", $f$ is normalized

## 7.4: Conjugate priors

- allow us to get posterior distributions in closed form
- Definition: A family $\mathcal{F}$ of prior distributions $p(\theta)$ is conjugate to a likelihood $p(D|\theta)$ if the resulting posterior $p(\theta|D)$ is in $\mathcal{F}$
- Examples:
  - Beta is conjugate to Bernoulli: If data comes from a Bernoulli process for a parameter $\theta$, then if we parameterize $\theta$ with a Beta distribution, the posterior is also Beta.
  - Gaussian is conjugate to Gaussian (if the $\theta$ we're considering is the mean of the distriubution)
  - Any exponential family has a conjugate prior: so if the data comes from an exponential family source, we have a conjugate prior that we can use to get analytic posterior.

## 7.5: Beta-Bernoulli model (part 1)

- Sequence of binary outcomes (e.g. coin flip) modeled with Bernoulli random variables with with beta prior distribution on the probability of heads
  - Beta is the conjugate prior for a Bernoulli
- Setup:
  - $X_1,\dots,X_N \sim Bern(\theta)$
  - $\theta \sim Beta(\theta|a,b)$
  - $P(X=1|\theta) = \theta$
  - $p(\theta) = \theta^{a-1}(1-\theta)^{b-1}/B(a,b) \propto \theta^{a-1}(1-\theta)^{b-1}$
  - $D = (x_1,\dots,x_n)$
- $p(\theta|D) \propto p(D|\theta)p(\theta) = p(\theta)\prod p(x_i|\theta)$
  - substituting, we get $\theta^{a-1}(1-\theta)^{b-1}\theta^{n_1}(1-\theta)^{n_0} = \theta^{a+n_1-1}(1-\theta)^{b+n_0-1}\propto Beta(\theta|a+n_1-1,b+n_0-1)$
    - where $n_i$ is the number of datapoints where $x_j = i$
  - thus $p(\theta|D)$ is also a beta distribution
  - we are operating under the support $[0,1]$
    - some examples of how the $a$ and $b$ affect the shape of the prior
- If $X\sim Beta(a,b)$, then $\mathbb{E} X = a/(a+b)$, $\sigma^2(X) = ab/((a+b)^2(a+b+1))$, mode (maximum density) is $(a-1)/(a+b-2)$
  - thus if $a:= a+n_1$, $b:= b+n_0$ i.e. the posterior calculated above, then we instantly get the summary statistics
    - $\mathbb{E}X = (a+n_1)/(a+b+n)$ as $n_0 + n_1 = n$
    - mode is $(a+n_1-1)/(a+b+n-2)$

## 7.6: Beta-Bernoulli model (part 2)

- With the same setup as before
  - $\theta_{MLE}$ is the empirical distribution, i.e. $\theta_{MLE} = n_1/n$
  - $\theta_{MAP}$ is the mode from the posterior that we calculated above: $\theta_{MAP} = (a+n_1-1)/(a+b+n-2)$
  - $\mathbb{E}(\theta|D) = (a+n_1)/(a+b+n) = \frac{a+b}{a+b+n}\frac{a}{a+b} + \frac{n}{a+b+n}\frac{n_1}{n}$
    - thus it's a convex combination of the prior mean and the MLE
    - when $n$ goes to $\infty$, get the MLE; when $n$ is 0 we get the prior mean
  - $p(x=1|D) = \int_\theta p(x=1|D,\theta)p(\theta|D)$
    - $x$ is conditionally independent of data given $\theta$, so this becomes $\int_\theta p(X=1|\theta)p(\theta|D)$
    - plugging in, $\int_\theta \theta Beta(\theta|a+n_1,b + n_0) = \mathbb{E}\theta = \frac{a+n_1}{a+b+n}$
    - so we get the predictive distribution in closed form


## 7.7.A1: Dirichlet distribution

- Distribution on probability distributions
- for $\theta = (\theta_1,\dots,\theta_m) = \theta \sim Dir(\alpha)$ if $p(\theta) = \frac{1}{B(\alpha)}\prod_{i=1}^m \theta_i^{\alpha_i -1} I(\theta\in S)$
  - $\alpha = (\alpha_1,\dots,\alpha_m)$, $\alpha_i > 0$
  - $S$ is the probability simplex given by $\{x\in \mathbb{R}^m|x_i\geq 0, \sum_i x_i = 1\}$
  - $B(\alpha)$ is the multivariate beta function $\frac{\prod_i\Gamma(\alpha_i)}{\Gamma(\alpha_0)}$, and normalizes the distribution
    - $\alpha_0 = \sum_i \alpha_i$
- Is a function on the probability distribution that can take a variety of shapes on the simlex depending on the $\alpha$
- often used as a prior on pmfs on a finite set
  - useful for Bayesian inference
- Useful statistics
  - $\mathbb{E} \theta_i = \frac{\alpha_i}{\alpha_0}$, i.e. proportional to the $i$-th parameter $\alpha_i$
  - the mode of $\theta = \big(\frac{\alpha_i-1}{a_0 -n}\big)$
  - $\sigma^2(\theta_i) = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}$
- If all the $\alpha_i=1$, then $p(\theta)$ is the uniform distribution on the simplex
- If all $\alpha_j=1$ for $j\neq i$, then 
  - as we raise $\alpha_i$, $\mathbb{E}\theta_i$ goes to $1$, all others go to $0$: get some kind of hump at the $i$ corner
  - as we lower $\alpha_i$ to $0$, then the density $Dir$ goes to infinity as $\theta_i$ goes to 0.

## 7.7A2: Expectation of a Dirichlet random variable

- as before, $p(\theta) = \frac{\Gamma(\alpha_0)}{\prod \Gamma(\alpha_i)}\prod_i \theta_i^{\alpha_i-1}I(\theta\in S)$
- Property of the $\Gamma$ function: $\Gamma(x+1) = x\Gamma(x)$
- $\mathbb{E}\theta_i = \int_\theta \theta_i p(\theta) d\theta = \int \theta_i\frac{\Gamma(\alpha_0)}{\prod \Gamma(\alpha_i)}\prod \theta_i^{\alpha_i-1} d\theta$
  - Looking at $\theta_1$, this becomes $\int \theta_1^{\alpha-1}\frac{\Gamma(\alpha_0)}{\prod \Gamma(\alpha_i)}\prod_{i>1} \theta_i^{\alpha_i-1} d\theta$
    - Define $\beta = (\alpha_1+1, \alpha_2,\alpha_3,\dots,\alpha_n)$
      - thus $\beta_0 = \sum \beta_i = 1 + \alpha_0$
      - so that $\Gamma(\alpha_0) = \Gamma(\beta_0)/\alpha_0$
    - Note $\Gamma(\alpha_1+1) = \alpha_1\Gamma(\alpha_1)$
    - plugging in, we get $\int \frac{\alpha_1}{\alpha_0}\frac{\Gamma(\beta_0)}{\prod \Gamma(\beta_i)}\prod_i \theta^{\beta_i-1} = \frac{\alpha_1}{\alpha_0}$
      - integral of a pdf is 1
  - similar arguments for the other $i$
  - thus, $\mathbb{E}\theta_i = \frac{\alpha_i}{\alpha_0}$

## 7.8-7.9: Dirichlet-Categorical model

- Setup
  - $\theta = (\theta_1,\dots,\theta_m)$, $\theta_i \geq 0$, $\sum \theta_i = 1$, $X_i\in \{1,\dots,m\}$
  - $X_1,\dots,X_n \sim Cat(\theta)$
    - $Cat(\theta)$ is defined by $P(X_i = j|\theta) = \theta_j$
  - $\theta \sim Dir(\alpha)$, i.e. $p(\theta) \propto \prod_j \theta_j^{\alpha_j-1} I(\theta \in S)$
  - $D = (x_1,\dots,x_n)$
- Then
  - $p(D|\theta) = \prod_1^n \theta_{x_i} = \prod_{i=1}^n\prod_{j=1}^m \theta^{I(x_i =j)} = \prod_{j=1}^m \theta_j^{c_j}$
    - $c_j$ is the number of times $j$s in $D$, $c = (c_1,\dots,c_m)$
  - $p(\theta|D) = p(D|\theta)p(\theta) = \prod_{j=1}^m \theta_j^{c_j + \alpha_j - 1} I(\theta\in S)\propto Dir(\alpha + c)$
  - thus the Dirichlet is the conjugate prior for Categorical, and we get the posterior in closed form
    - Briefly, $(\prod_i Cat(x_i|\theta))Dir(\theta|\alpha) = Dir(\theta|c+\alpha)$
  - $p(x|D) = \int p(x|\theta)p(\theta|D) d\theta = \int \theta_x Dir(\alpha + c)d\theta$
    - this is $\mathbb{E} \theta_x$ if $\theta$ is distributed by the posterior $Dir(\alpha + c)$
      - $p(x|D) = \frac{c_x + \alpha_x}{n+a_0}$
        - If we consider the $\alpha_i$ as "pseudo counts" (kind of like occurrences of $i$), then this is the empirical distribution if we added the new counts ($(c_i)$ with $\sum c_i = n$) to our pseudo counts

## 7.9-7.10: Posterior distribution for univariate Gaussian

- we are looking at the posterior distribution for the mean of a univariate Gaussian, i.e. $p(\theta|D)$
- Setup
  - $D = (x_1,\dots,x_n)$, $x_i\in\mathbb{R}$, $X_1,\dots X_n \sim N(\mu,\sigma^2)$ iid given $\mu$
  - $\mu \sim N(\mu_0, \sigma_0^2)$
  - Assume $\sigma^2$, $\mu_0$, $\sigma_0^2$ are known
  - We are looking for $\theta := \mu$
- $p(\theta|D) = p(\mu|D) \propto p(D|\mu)p(\mu) = p(\mu)\prod_i N(\mu,\sigma^2)\propto \exp \big(-\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 + \frac{-1}{2\sigma^2}\sum(x_i-\mu)^2\big)$
  - the exponent is quadratic in $\mu$, so the pdf is a gaussian
    - if we put it in form $-\frac{1}{2\sigma_n^2}(\mu-m)^2 + c$, we get
      - the new variance $\sigma_n^2 = \frac{\sigma_0^2\sigma^2}{\sigma^2 + n\sigma_0^2}$
        - is related to the harmonic mean of $(\sigma_0^2,\sigma^2,\dots\sigma^2)$: $\sigma_0^2$ once and $\sigma^2$ $n$ times
      - the new mean $m =  \sigma_n^2\big( \frac{\mu_0}{\sigma_0^2} + \frac{\sum x_i}{\sigma^2}\big)$
        - is a convex combination of the prior and the MLE (prior is $\mu_0$, MLE is $\bar{x} = \sum x_i/n$)
        - $m = \frac{\sigma_n^2}{\sigma_0^2}\mu_0 + \frac{\sigma_n^2}{\sigma^2}n\mu_{MLE}$
  - posterior distribution $p(\mu|D)\sim N(m,\sigma_n^2)$
    - $m$ is the MAP
    - as $n$ increases, the MAP goes to the MLE, $\sigma^2_n$ goes to 0
      - we get a point mass at the MLE

# 8: Naïve Bayes

## 8.1: Naïve Bayes Classification

- Naïve Bayes Model 
  - a simple family of classifiation models
  - not necessarily "Bayesian"
  - performance not great, but it's a good baseline/intro to generative models
- Setup
  - Given $D = ((x^1,y^1),\dots (x^n,y^n))$
  - $x^i = (x^i_1,\dots,x^i_d) \in \mathbb{R}^d$
  - $y_i\in \mathcal{Y}$, where $\mathcal{Y}$ is a finite set, e.g. $\{1,\dots,m\}$
- Assume a family of distributions $p_\theta$ such that for $x\in \mathbb{R}^d$ and $y\in\mathcal{Y}$, $p_\theta(x,y) = p_\theta(x|y)p_\theta(y) = p_\theta(x_1|y)\dots p_\theta(x_d|y)p_\theta(y)$
  - in other words, we assume that coordinates $x_i$ of $x$ are conditionally independent given $y$
  - we assume that our data came from $p_\theta$ for some $\theta$
- Classification: Given $x\in\mathbb{R}^d$, we predict $y$
  - Estimate $\theta$ from $D$
  - Compute $\hat{y} = \arg\max_{y\in\mathcal{Y}} p_\theta(y|x) = \arg\max_y p_\theta (x|y)p_\theta(y)/p_\theta(x)$
    - denominator is irrelevant when varying $y$
    - $p_\theta(x|y) = \prod p_\theta(x_i|y)$ from our assumption/model

## 8.2: More about Naïve Bayes

- How do we choose $p_\theta$: depending on the problem, can do
  - $p_\theta(y) = \pi_y$ where $\pi = (\pi_1,\dots,\pi_m)$
    - i.e. some arbitrary distribution with $\pi$ as part of the parameters $\theta$
  - $p_\theta(x_i|y)$
    - if $X_i\in \{1,\dots,N\}$, i.e. a finite set, then $p_\theta(x_i|y) = q(x_i,y)$, i.e. a set of $nm$ numbers (one for each $y$ class and $x$ dimension)
    - if $X_i\in \{1,2,\dots\}$, can choose a Poisson, Geometric etc.
    - if $X_i\in \mathbb{R}$, can choose Gaussian, Gamma, etc
- How to estimate $\theta$
  - MLE - MAP (assuming a prior on $\theta$)
  - Fully Bayesian Naïve Bayes: integrate over $\theta$ and get the predictive distribution $p(y|x)$
- Why do we assume conditional independence?
  - of course, we lose out on dependencies etc
  - the benefit is that we can estimate $\theta$ more accurately with less data
    - i.e. overfit less because data is denser if we restrict to each dimension than if we consider the points as elements of $\mathbb{R}^d$
  - a wrong but simple model can perform better than a correct but complicated model in practice.

## 8.3-8.6: Bayesian Naïve Bayes

- Bayesian approach to Naïve Bayes model
- "Fully Bayesian approach" to estimate $p(y|x,D)$
- we'll consider the Categorical
- Setup
  - $D = ((x^1,y^1),\dots,(x^n,y^n))$
  - $x^i = (x^i_1,\dots,x^i_d)$ with $x^i_j\in A_j$
  - $A_j$ are finite sets
  - $y^i \in \mathcal{Y} = \{1,\dots,m\}$
  - $(X^i,Y^i)$ are iid from a probability distribution $p(x,y|\theta)$
- Naïve Bayes assumption: $p(x,y|\theta) = p(y|\theta)\prod_{j=1}^d p(x_j,y)$
  - features are conditionally independent given the class $y$ and $\theta$
- Let's define the distributions
  - $p(y|\theta) = \pi(y)$ with $\pi = (\pi(1),\dots,\pi(m))$ i.e. a parameterized vector, $\sum \pi(y) = 1$
  - $p(x_j|y,\theta) = r_{jy}(x_j)$, i.e. $r_{jy}$ as a vector with $dm$ components, $\sum_{k\in A_j} r_{jy}(k) = 1$ for all $j,y$
  - $\theta = (\pi,\{r_{jy}\})$
  - can be seen as choose $y$ according to $\pi$, and then for each $j$ choose $x_j$ according to $r_{jy}$
- Let's assume that $A_i = A_j$ for all $i$, $j$.
- which prior for $\pi$ and $r_{jk}$
  - For $y$, We pick the Dirichlet, as it's conjugate for the categorical: 
    - $p(\pi) = Dir(\pi|\alpha) \propto \prod_{y=1}^m \pi(y)^{\alpha_y -1}I(\pi \in S)$
    - $\alpha_y > 0$ for all $y$
    - product over possible $y$
  - For $r_{jk}$, we also do Dirichlet
    - $p(r_{jk}) = Dir(r_{jk}|\beta)\propto \prod_{l\in A} r_{jk}(l)^{\beta_l -1}I(r_{jk} \in S)$
    - $\beta_i > 0$
    - product over possible $x_j\in A$
- To get a prior for the joint distribution of $\theta$, We assume that the priors are independent: $p(\theta) = p(\pi)\prod_{j,k}p(r_{jk})$
- Let's classify a new point given $x$:
  - i.e. the predictive distribution $p(y|x,D)$
  - $p(y|x,D) \propto_y p(y,x,D) = \int_\theta p(x,y,D|\theta)p(\theta) d\theta = \int_\theta p(x,y|\theta)p(\theta)\prod_{i=1}^np(x^i,y^i|\theta)d\theta$
    - using conditional independence $p(x,y,D|\theta) = p(x,y|\theta)p(D|\theta)$
  - $p(x,y|\theta) = \pi(y)\prod_{j,k} r_{jk}(x_j)^{I(y=k)}$ by definition and Bayes assumption
  - By independence and definition, $\prod_{i=1}^n p(x^i,y^i|\theta) = \prod_i Cat(y^i|\pi)\prod_{jk}Cat(x^i_j|r_{jk})^{I(y_i=k)}$
  - Assuming that $\pi$ and $r$ are independent, $p(\theta) = Dir(\pi|\alpha)\prod_{j,k}Dir(r_{jk}|\beta)$
  - Thus, $p(y|x,D) \propto \int_\theta p(x,y|\theta) Dir(\pi|\alpha + c)\prod_{jk}Dir(r_{jk}|\beta + d_{jk}) d\theta$
    - $c = (c_1,\dots,c_m)$, where $c_k$ is the number of points in $D$ with $y = k$
    - $d_{jk}(l)$ is the number of points in $D$  where $x^i_j = l$ and $y^i = k$
    - from before, $p(x,y|\theta) = \pi(y)\prod_{j,k}r_{jk}(x_j)^{I(y=k)}$
  - grouping stuff (as $\theta$ has a bunch of components), we have
    - $p(y|x,D) = \big( \int \pi(y)Dir(\pi|\alpha + c)d\pi\big)\prod_{j,k}\int_{r_{jk}} r_{jk}(x_j)^{I(y=k)}Dir(r_{jk}|\beta + d_{jk})dr_{jk}$
      - $\int \pi(y)Dir(\pi|\alpha + c)d\pi$ is the expected value of the dirichlet, so is $\frac{\alpha_y + c_y}{\sum_y(\alpha_y + c_y)}$
      - $\int_{r_{jk}} r_{jk}(x_j)^{I(y=k)}Dir(r_{jk}|\beta + d_{jk})dr_{jk}$ is $1$ if $y\neq k$; if $y=k$, then it's the expected value of the $x_j$ coordinate, \frac{\beta_{x_j} + d_{jy}(x_j)}{\sum (\beta_l + d_{jy}(l))} 
      - note that $\sum c_y = n$, $\sum_l d_{jy}(l) = c_y$
  - $p(y|x,D)\propto \frac{\alpha_y + c_y}{\alpha_0+n}\prod_{j=1}^d \frac{\beta_{x_j} + d_{jy}(x_j)}{\beta_0 + c_y}$
    - $\alpha_0 = \sum_y \alpha_y$, $\beta_0 = \sum_l \beta_l$
    - in other words, the product of the Dirchlet-Categorical models for each $y$ and pairs $(x_j,y)$ considered independently
      - the independence assumption kinda filters through to the result here.
- it's nice that we were able to calculate the integrals exactly using conjugate priors and facts about probability distributions and expectations

# 9: Linear Regression

## 9.1: Linear regression - Nonlinearity via basis functions

- Not just about lines and planes: can fit curves, periodic functions, etc. too
- Setup
  - $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - $x_i\in \mathbb{R}^d$
  - $y_i\in \mathbb{R}$
- Goal: get a function $f: \mathbb{R}^d\rightarrow \mathbb{R}$ to predict $y$ for a new $x$
- Basis Functions:
  - represent nonlinear problems in a linear way
  - Suppose $x\in \mathbb{R}^d$, $y\in \mathbb{R}$
    - simplest solution is a linear combination: $f(x) = w^Tx = \sum_{i=1}^d w_ix(i)$, where $x(i)$ is the $i$-th component of $x$.
    - another solution: $f(x) = w^T\phi(x) = \sum_{i=1}^n w_i\phi(x)$, where $\phi:\mathbb{R}^d\rightarrow \mathbb{R}^m$
      - this can give us nonlinear solutions that are linear in $w$
      - for example, if $d=1$, we can do $f(x) = (1,x,x^2)\in\mathbb{R}^3$, giving us a quadratic model.
      - $\phi$ as identity is the case from before
      - If we define $\phi(x) = (\phi_1(x),\dots,\phi_m(x))$, the $\phi_i$ are the basis functions.
- Other examples
  - polynomials: (even with interaction terms if desired)
  - radial basis functions: each $\phi_i$ is a little gaussian on $x$
  - fourier basis functions: each basis function is a sin curve, can make periodic functions
  - wavelets: essentially attenuated sin curves, (perhaps convolution of radial basis with fourier basis)

## 9.2: Linear regression - Definition & Motivation

- $\phi$ takes $x$ from the original space to the "feature space"
- From now on, let's work with the feature space, where things are linear.
- Motivation via Discriminative approach: want to model $p(y|x)$
  - Assume a family of distributions $p_\theta(y|x)$ parameterized by $\theta \in \Theta$
  - We want to estimate $\theta$ given data $D$, e.g. via MLE
  - easiest family of distributions for $y\in \mathbb{R}$ are Gaussians
    - $p_\theta(y|x) = N(y|\mu(x),\sigma^2(x))$
      - $x\in \mathbb{R}^d$
      - $\theta = (\mu,\sigma^2)$, $\mu: \mathbb{R}^d\rightarrow\mathbb{R}$, $\sigma^2: \mathbb{R}^d\rightarrow\mathbb{R}>0$
      - in other words, $y$ is normally distributed with $\mu$ and $\sigma^2$ as functions of $x$
- Gaussian Linear Regression 
  - models the data $D$ by assuming $p_\theta(y|x) = N(y|w^Tx,\sigma^2)$
    - parameterized $\theta = (w,\sigma^2)$, where $w\in\mathbb{R}^d$, $\sigma^2 > 0$
    - i.e. we assume that the $\mu$ function above is $\mu(x) = w^Tx$ and $\sigma^2$ is a constant.
  - Alternative formulation: $y = w^Tx + \epsilon$, where $\epsilon \sim N(0,\sigma^2)$
    - $\epsilon$ is gaussian noise

## 9.3 Choosing $f$ under linear regression

- Before, we model $y$ as coming from a normal distribution with mean $w^Tx$ and standard deviation $\sigma^2$
  - for each $x$, this gives us a distribution for $y$
  - but we want to predict a value $f(x)$, not a distribution
- Let's try to minimize expected loss
  - let's minimize square loss $\mathcal{L}(y,\hat{y}) = (y-\hat{y})^2$
  - For square loss, $\arg\min_y\mathbb{E}_\theta (\mathcal{L}(Y,y)|X=x) = \mathbb{E}_\theta(Y|X=x) = \mathbb{E}_\theta (w^Tx + \epsilon|X=x) = w^Tx$
  - thus we define $f(x) = w^Tx$ to minimize square loss.
- Choices:
  - Discriminative approach
  - Gaussian distribution $p_\theta(y|x) = N(y|\mu(x),\sigma^2(x))$
  - Linear model for $\mu(x) = w^Tx$
  - if we know $\theta = (w,\sigma^2)$, choose $f$ to minimize expected square loss: i.e. $f(x) = w^Tx$
- missing step: How do we find the true $\theta$, or more relevant, $w$?
  - Estimate using MLE
  - can also do ridge regression, lasso regression, etc to find a different $\theta$

## 9.4-9.6: MLE for linear regression

- Setup:
  - $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - $x_i \in \mathbb{R}^d$
  - $y\in \mathbb{R}$
- Assumptions
  - Gaussian linear regression: $y\sim N(w^Tx,\sigma^2)$
  - $\sigma^2$ is known: so we parameterize by $\theta = w$
  - $y_i$ are iid
- MLE
  - $\theta_{MLE} = \arg\max_{\theta\in\Theta}p(D|\theta)$
    - (more precisely, the MLE satisfies this - there can be more than one)
  - $p(D|\theta) = \prod_i p(y_i|x_i,\theta) = \frac{1}{\sqrt{2\pi\sigma^2}^n} \exp\big(-\frac{1}{2\sigma^2}\sum_i (y_i-w^Tx_i)^2\big)$
    - we are ignoring the distribution of the $x$
    - note that $\sum_i (y_i-w^Tx_i)^2 = (y-Aw)^T(y-Aw) = ||y-Aw||^2$, where
      - $y = (y_1,\dots,y_n)$
      - $A$ is a $n\times d$ matrix where each row is an $x_i$
    - to maximize $p(D|\theta)$, we want to minimize $\mathcal{L}:=(y-Aw)^T(y-Aw) = y^Ty-2y^TAw + w^TA^TAw$
      - take the gradient $\nabla_w\mathcal{L} = -2A^Ty + 2A^TAw$
        - we can check this by writing it out and seeing the derivative in each dim
          - for the $w^TA^TAw$ part, look at $w^TBw$, where $B = A^TA$ is symmetric
            - then $\nabla_w w^TBw = 2Bw$
        - set the gradient equal to 0 and get $A^Ty = A^TAw$, i.e. $w = (A^TA)^{-1}A^Ty$
  - note that $A^TA$ is invertible if the columns of $A$ are linearly independent
    - the $j$-th column is a vector of the $j$-th components of the $x_i$
  - $(A^TA)^{-1}A^T$ is known as $A^+$, the "pseudoinverse" of $A$
    - $A$ is of course an $n\times d$ matrix, which is not necessarily invertible
    - weights are $w = A^+y$
  - to show that we have the minimum, we take the Hessian $\nabla_w^2\mathcal{L} = A^TA$, which is positive semidefinite.
  - Note also that $\arg\min_w\mathcal{L} = \arg\min_w ||y-Aw||^2 = \arg\min_w ||y-Aw||$
    - in other words, the $w$ that we found minimized the euclidean distance between $y$ and $Aw$
      - in the ideal case, $y = Aw$, but as before $A$ is not necessarily invertible; the pseudoinverse is as close as we can get
- Note that we assume that the columns of $A$ are linearly independent

## 9.7: Basis functions MLE

- we considered $f(x) = w^T\phi(x) = w^Tz$, where $\phi: \mathbb{R}^d\rightarrow\mathbb{R}^m$ and $z = \phi(x)$
- define $\Phi$ as a $n\times m$ matrix where each row is $\phi(x_i)$
- Then the weights are $w = \Phi^+y = (\Phi^T\Phi)^{-1}\Phi^Ty$

# 10: Bayesian Linear Regression

## 10.1: Bayesian Linear Regression

- Why not use MLE? Overfitting
- Why not use MAP? No representation of uncertainty in $w$ or the predictions $y$
  - what if the $x$ is far outside of the training data: we should be unsure/have large error bars
- Why Bayesian? It optmizes the loss function and gives us the prediction distribution $p(y|x,D)$, which is what we really want.
- Setup:
  - $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - $x_i\in \mathbb{R}^d$
  - $y_i\in \mathbb{R}$
- Model:
  - $Y_1,\dots, Y_n$ independent given $w$, $Y_i\sim N(w^Tx_i,a^{-1})$
    - $a = 1/\sigma^2$ is the "precision"
  - $w\sim N(0,b^{-1}I)$
    - we model $w$ as normally distribited with variance $b^{-1}I$, where $b>0$
    - $I$ is the identity matrix - thus the $w_i$ are iid
- assume $a$ and $b$ are known, so that our parameters are $\theta = w$

## 10.2-10.3: Posterior for linear regression

- Remark: can replace $x_i$ with $\phi(x_i)$ and thus model nonlinearities
- Likelihood: $p(D|w) \propto \exp\big(-\frac{a}{2}(y-Aw)^T(y-Aw)\big)$
  - $A$ is the $n\times d$ design matrix with rows equal to the $x_i$
  - $y = (y_1,\dots,y_n)^T$
- Posterior: $p(w|D)\propto p(D|w)p(w) \propto \exp \big(-\frac{a}{2}(y-Aw)^T(y-Aw)-\frac{b}{2}w^Tw\big)$
  - the exponent is a quadratic in $w$, so is also Gaussian
  - $a(y-Aw)^T(y-Aw)+bw^Tw = ay^Ty-2aw^TA^Ty + w^T(aA^TA + bI)w$
  - completing the square and matching with the (note) below, we define
    - $\Lambda = aA^TA + bI$
    - $\mu = a\Lambda^{-1}A^Ty$
  - and get $p(w|D)\propto N(\mu, \Lambda^{-1})$
  - check that $\Lambda$ is invertible
    - if $B$ is a square matrix, consider $B+cI$, with $c\in\mathbb{R}$, $I$ is identity
    - Suppose that $\mu$ is an eigenvector of $B$ with eigenvalue $\lambda$
      - then $(B+cI)u = Bu + cIu = \lambda u + cu = (\lambda + c)u$
      - thus $u$ is an eigenvector of $B+cI$ with eigenvalue $\lambda + c$
    - the precision $b > 0$; $aA^TA$ is positive semidefinite
      - thus the eigenvalues of $aA^TA + bI$ are all greater than 0
      - and $\Lambda$ is invertible.
  - the MAP estimate is then $\mu = a\Lambda^{-1}A^Ty = (A^TA + \frac{b}{a}I)^{-1}A^Ty$
    - the MLE estimate from before is $(A^TA)^{-1}A^Ty$
    - the difference is $\frac{b}{a}I$ in the inverse. $\frac{b}{a}I$ is known as the "regularization parameter" related to $-b/2 w^Tw$
      - if $b$ goes to $0$, then the prior becomes more uniform - we approach universal prior and the MLE
- (note) For a general multivariate Gaussian, the exponent contains $(x-\mu)^T\Lambda(x-\mu)$,
  - $\Lambda^{-1}$ is the symmetric covariance matrix
  - This is then $x^T\Lambda x - 2x^T\Lambda \mu + C$, where $C$ is a constant
  - note: the $\Lambda$ is often represented by $\Sigma^{-1}$ in official definitions

## 10.4-10.7: Predictive distribution for linear regression 

- Let's calculate the predictive distribution. Given $x$, we want to calculate  $p(y|x,D)$
- $p(y|x,D) = \int p(y|x,D,w)p(w|x,D) dw = \int p(y|x,w)p(w|D)dw$
  - $\int N(y|w^Tx,a^{-1})N(w|\mu,\Lambda^{-1})dw\propto_y \int \exp \big(\frac{a}{2}(y-w^Tx)^2 - \frac{1}{2}(w-\mu)^T\Lambda(w-\mu)\big)dw$
    - we expand this, complete the square, etc
    - it ends up $\propto \int N(w|m,L)g(y)dw$, where
      - $L = axx^T+\Lambda$
      - $m = L^{-1}(ayx + \Lambda\mu)$
      - $g(y) = \exp\big(\frac{1}{2} m^TLm - \frac{1}{2}ay^2\big)$
    - then we pull the $g(y)$ out, and the integral evaluates to 1
      - note that $m$ depends on $y$, but this doesn't matter as the integral still is 1
    - thus $p(y|x,D)\propto \exp\big(\frac{1}{2} m^TLm - \frac{1}{2}ay^2\big)$
      - now we complete the square again with this function to get another gaussian in $y$
      - $p(y|x,D)\propto N(y|u,\lambda^{-1})$ with
        - $\lambda^{-1} = \big(a(1-ax^TL^{-1}x)\big)^{-1} = \frac{1}{a} + x^T\Lambda^{-1}x$
        - $u = \frac{1}{\lambda}ax^TL^{-1}\Lambda\mu = \mu^Tx$
        - the second equality uses the Sherman-Morrison formula and some more computations
          - have to show that $L$ is invertible, which is another computation
- The mean of the distribution is $\mu^Tx$
  - $\mu$ is the posterior mean for the weights $w$
  - so the mean is like we used the MAP estimate for $\theta$
- all this math worked because the exponents were all quadratics - and then we kept completing the square.

# 11: Estimators

## 11.1 Estimators

- Now assume data is $D = (X_1,\dots,X_n)$, with $X_i$ random variables
  - i.e. the data is a random variable
- Definition: a "statistic" is a random variable $S$ that is a function of the data $D$, e.g. $S = f(D)$
- Definition: an "estimator" is a statistic intended to approximate a parameters governing the distribution of $D$
- Notation: 
  - $\hat{\theta}$ denotes an estimator of $\theta$
  - $\hat{\theta}_n$ emphasizes the dependence on $n$
- Example: $X_1,\dots,X_n\sim N(\mu,\sigma^2)$ iid, we can consider the following estimators as functions of the data $D$
  - $\sigma^2= \mathbb{E}((x-\mu)^2)$ is the true variance
  - sample mean: $\hat{\mu} = \bar{x} = \frac{1}{n}\sum_i x_i$
  - biased sample variance $\hat{\sigma^2} = \frac{1}{n}\sum_i (x_i-\bar{x})^2$
  - unbiased sample variance $\hat{s^2} = \frac{1}{n-1}\sum_i (x_i-\bar{x})^2$
- Definition: the "bias" of an estimator $bias(\hat{\theta}) = \mathbb{E}\hat{\theta} - \theta$
- Definition: An estimator $\hat{\theta}$ is "unbiased" if $bias(\hat{\theta}) = 0$
- Examples: 
  - $\hat{\mu}$ is unbiased: $\mathbb{E}\hat{\mu} = \mathbb{E}\frac{1}{n}\sum_i X_i = \frac{1}{n}\sum \mathbb{E}X_i = \frac{1}{n}\sum _i \mu = \mu$
- Let's prove that the biased/unbiased sample variances are biased/unbiased
  - Note that $\sigma^2 = \mathbb{E}(X^2) - (\mathbb{E}(X))^2 = \mathbb{E}(X^2) - \mu^2$
  - for each $i$, $\mathbb{E}(X_i-\bar{X})^2 = \mathbb{E}(X_i^2) - 2\mathbb{E}(X_i\bar{X}) + \mathbb{E}\bar{X}^2$
    - each part:
      - $\mathbb{E}(X_i^2) = \sigma^2 + \mu^2$
      - $\mathbb{E}(X_i\bar{X}) = (1/n)(\mathbb{E}X_i^2 + (n-1)\mathbb{E}_{i\neq j}(x_ix_j)) = (1/n)(\sigma^2 + \mu^2 + (n-1)(\mu^2)) = \frac{\sigma^2}{n} + \mu^2$
      - $\mathbb{E}(\bar{X}^2) = \frac{1}{n^2}\mathbb{E}(\sum_i x_i)^2 = \frac{1}{n^2}\big(n\mathbb{E}(x^2) + (n^2-n)\mathbb{E}(x)\mathbb{E}(x)\big) = \frac{1}{n^2}\big(n(\sigma^2 + \mu^2) + (n^2-n)\mu^2\big) = \frac{\sigma^2}{n} + \mu^2$
    - combining: $\sigma^2 + \mu^2 - 2(\sigma^2/n + \mu^2) + \sigma^2/n + \mu^2 = \frac{n-1}{n}\sigma^2$
  - Thus $\mathbb{E}\hat{s^2} = \frac{1}{n-1}\mathbb{E}\sum_i (X_i-X)^2 = \frac{n}{n-1}\mathbb{E}(X_i-X)^2 = \frac{n}{n-1}\frac{n-1}{n}\sigma^2 = \sigma^2$, and the unbiased estimator is unbiased.
  - Similarly, $\mathbb{E}\hat{\sigma^2} = \frac{n-1}{n}\sigma^2$, so underestimates

## 11.2: Decision theory terminology in different contexts

- Contexts: General Setup, Estimators, and Regression/Classification
- In the general setup, we have a state $s$ and data $D$. We take an action $a = \delta(D)$ and get a loss $L(s,a)$, where $\delta$ is a decision rule.
- In the Estimator setup, we have a parameter $\theta$ and data $D$. We estimate $\hat{\theta} = g(D)$ and get a loss $L(\theta, \hat{\theta})$, where $g$ is an estimator function.
- in Regression/Classification, we have a target value $y$ and a point $x$. We predict $\hat{y} = f(x)$ and get a loss $L(y,\hat{y})$, where $f$ is a predictor function.

## 11.3: Frequentist risk, Bayesian expected loss, and Bayes risk

- Setup:
  - $D = (X_1,\dots, X_n)$, $D\sim p_\theta$
  - $\theta \sim \pi$: i.e. $\theta$ is a random variable
  - $\hat{\theta} = f(D)$
- Loss function $L(\theta, \hat{\theta}) = L(\theta, f(D))$.
  - options to minimize the loss
    - (1) Average over the $\theta$ (via $\pi$): $\mathbb{E} (L(\theta,f(D))|D)$
      - This is the Bayesian expected loss $\rho(\pi,f(D))$
    - (2) Average over the data $D$: $\mathbb{E}(L(\theta, f(D))|\theta)$
      - This is the Frequentist Risk $R(\theta,f)$
    - (3) if we take either of these and then average over the one we hadn't yet, we get the Bayes Risk $r(\pi,f)$.
  - (1) and (3) are "Bayes world", (2) and (3) are "Risk world"
- Bayesian Perspective: We know the data $D$, so we now we minimize the risk of the action that we take - so it's silly to "average over $D$"
- Frequentist Perspective: We don't know $\pi$, or in fact $\theta$ is a real/true thing - so doesn't have a distribution - so it's silly to "average over $\theta$"

## 11.4: Choosing a decision rule - Bayesian and frequentist

- How to choose a procedure $f$
- Bayesian: Assume some prior $\pi$ on $\theta$
  - Case (1) above: Know $D$: choose $f(D)$ to minimize $\rho(\pi, f(D))$
  - Case (3) above: Don't know $D$: choose $f$ to minimize $r(\pi,f)$
- Frequentist:
  - Minimizing $R$ is not well defined, so we Introduce a further principle to guide your choice
    - For example: Unbiasedness, Admissibility, Minimax, Invariance, etc.

## 11.5: Bias-Variance decomposition

- We'll show $MSE = bias^2 + variance$
- Definition the Mean-Squared Error (MSE) of an estimator $\hat{\theta} = f(D)$ is $\mathbb{E}((\hat{\theta}-\theta)^2|\theta)$
  - $\theta$ is fixed
  - this is the same as $R(\theta,f)$ under the square loss
- Recall $bias(\hat{\theta}) = \mathbb{E}\hat{\theta} - \theta$
- Let $\mu = \mathbb{E}\hat{\theta}$
- $MSE(\hat{\theta}) = \mathbb{E}(\hat{\theta}-\theta)^2 = \mathbb{E}((\hat{\theta}-\mu) + (\mu-\theta))^2 = \mathbb{E}(\hat{\theta}-\mu)^2 + 2\mathbb{E}(\hat{\theta}-\mu)(\mu - \theta) + \mathbb{E}(\mu-\theta)^2$
  - the first term is the variance by definition
  - $\mu$ and $\theta$ are fixed, so the middle term is $(\mu-\theta)\mathbb{E}(\hat{\theta}-\mu) = 0$
  - The last term is $\mathbb{E}(\mu - \theta)^2 = (\mu - \theta)^2 = bias^2$
- Example: 
  - Suppose $X\sim N(\theta, 1)$, $\theta$ is not random but unknown. 
  - data is $D = ((x))$, i.e. one point
  - then the "natural" estimate of $\theta$ is the sample mean $\hat{\theta} = x$
    - the bias is 0, as the sample mean is an unbiased estimator of the mean
    - the variance the variance of $x$ so is 1.
    - Thus the MSE is 1
  - a "silly" estimate of $\theta$ is $\hat{\theta} = 0$
    - then the bias is $\theta$
    - the variance is 0
    - and the MSE is $\theta^2$
- We could perhaps do a biased estimator and reduce the variance more than we increase the bias - and thus reduce the MSE:
  - shrinkage
  - Stein's paradox

## 11.6: Inadmissibility

- Inadmissibility is a way to reject certain estimators
- $R(\theta, \delta) = \mathbb{E}(L(\theta, \delta(D))|\theta)$
- Example $X\sim N(\theta,1)$, $\theta\in\mathbb{R}$
  - Square loss $L(\theta,\hat{\theta}) = (\theta - \hat{\theta})^2$
  - get one point $D = \{x\}$
  - natural estimator $\delta_1(D) = x$, $MSE(\delta_1) = R(\theta, \delta_1) = 1$
  - silly estimator $\delta_0(D) = 0$, $MSE(\delta_0) = R(\theta, \delta_0) = \theta^2$
  - in other words, if $\theta\in [-1,1]$, the silly estimator is better than the natural estimator
- Definition: Given decision rules $\delta$ and $\delta'$, we say that $\delta$ "dominates" $\delta'$ if $R(\theta,\delta)\leq R(\theta,\delta')$ for all $\theta\in\Theta$, and $R(\theta,d) < R(\theta,\delta')$ for some $\theta \in\Theta$
- Definition: A decision rule $\delta$ is "inadmissible" if there is another decision rule $\delta'$ that dominates $\delta$
- Definition: A decision rule that is not inadmissible then it is "admissible", i.e. there is no decision rule that dominates it.
- Fact: Both $\delta_1$ and $\delta_0$ from above are admissible

## 11.7: A fun exercise on inadmissibility

- Let $X_1,\dots,X_n \sim N(\mu,\sigma^2)$ assume $\mu$ is known, estimate $\sigma^2$
- Use the square loss $L(\sigma_1^2,\sigma_2^2) = (\sigma_1^2-\sigma_2^2)^2$
- $\hat{\sigma}^2 = \frac{1}{n}\sum_i (x_i-\bar{x})^2$, $s^2 = \frac{n}{n-1}\hat{\sigma}^2$
- let $s_c^2 = c\hat{\sigma}^2$ for $c\geq 0$
- Questions
  - Does $\hat{\sigma}^2$ dominate $s^2$ or vice versa?
  - Are $\hat{\sigma}^2$ and $s^2$ admissible?
  - is there a "best" $s_c^2$ for some $c\geq 0$?
- note that the results hinge on the fact that we're using the square loss.

## 11.8: Bayesian decision theory

- Previous stuff was from a frequentist perspective (expectation of data given parameter)
- Now we do a Bayesian perspective: given a decision rule $\delta$ on $D$
  - Average over $\theta$: Bayesian expected loss: $\rho(\pi, \delta(D)) = \mathbb{E}(L(\theta,\delta(D))|D)$
  - Average over $\theta$ and $D$: Bayesian Risk: $r(\pi,\delta) = \mathbb{E}(L(\theta,\delta(D)))$
- We can rewrite the Bayesian Risk as $\mathbb{E}(\mathbb{E}(L(\theta,\delta(D))|D)) = \mathbb{E}(\rho(\pi,\delta(D)))$
  - in other words, to reduce the Risk we can just reduce the expected loss for each $D$
- Definition: A "generalized Bayes rule" is a decision rule $\delta$ minimizing $\rho(\pi,\delta(D))$ for each $D$
- Definition: A "Bayes rule" is a decision rule that minimizes $r(\pi,\delta)$
- Remark: A generalized Bayes rule is a Bayes rule, but a Bayes rule is not necessarily a generalized Bayes rule
  - If $r(\pi,d) = \infty$ for all $\delta$ then anything is a Bayes rule (e.g. improper prior), but a GBR still makes sense
  - On sets of $\pi$ where $\pi = 0$ (e.g. 0 prior), then a Bayes rule can be arbitrary on these sets, but the GBR is still sensible.
- Complete Class Theorems: Under mild conditions
  - Every GBR (for a proper $\pi$) is admissible
  - Every admissible decision rule is a GBR for some (possibly improper) prior $\pi$
- note that admissibility does not guarantee "non-silliness" as we saw above.

# 12: Model Selection

## 12.1: Model selection: introduction and examples

- Not about different types of models
- About different complexities of models i.e. "complexity selection" or the flexibility of a model to fit/explain data
- Example: Linear Regression using MLE to find $f(x) = w^T\phi(x)$
  - $x \in \mathbb{R}$
  - Polynomial basis functions $\phi_k(x) = x^k$, $\phi = (\phi_0,\dots,\phi_B)$
  - If $B=1$, we get a line, $B=3$ is a cubic, etc.
  - As $B$ increases, the complexity increases - can overfit
- Example: Bayesian Linear Regression or MAP
  - same setup as before
  - $w \sim N(0,\sigma^2I)$
  - Now we have to choose $B$ or $\sigma^2$ as complexity controlling parameters
    - if $\sigma^2$ is small, we bias towards functions that have small $w$
- Example: Classification (kNN)
  - have to choose the parameter $k$
  - smaller $k$, we overfit
  - $k=1$, we get a Voronoi diagram

## 12.2: Bias-variance in model selection

- For an estimator, the mean squared error for the estimator $MSE = bias^2 + variance$
  - for small model complexity, have high bias and low variance
    - model doesn't change too much as data changes
      - can bias around wrong value
      - but doesn't change too much
  - for large model complexity, have low bias and high variance
    - model changes a lot as data changes
      - low bias: overfits as it fits the data better (the data centers around the truth)
      - but variance is large as data changes
- There's a tradeoff/need to find the right model complexity
- can think of bias/variance for a particular candidate $x$, or integrate over the possibilities of $x$
  - same conclusions
- Note: the Equation only applies to square loss, but the concept is useful to think of in general

## 12.3: Model complexity parameters

- Complexity controlling parameters
  - not parameters used to fit the data
    - e.g. $B$ in 12.1 (but not $w$)
  - In Bayesian models, tend to be hyperparameters, e.g. parameters of the prior distribution
    - e.g. $\sigma^2$ in 12.1
- isn't really a clear cut distinction - use best judgement
- control ability to fit the data

## 12.4: Bayesian model selection

- Setup
  - We're looking for $p(y|x,\theta,m)$
  - $m$ defines the model, e.g.
    - number of basis functions
    - variance for hyperparameter/prior
    - other complexity parameters
  - both $\theta$ and $M\sim m$ are random variables 
- In "Bayesian model averaging", we expand $p(y|x,D) = \int_M p(y|x,D,m)p(m|x,D) dm$
  - we assume $x$ doesn't affect the choice of $m$, this becomes $\int_M p(y|x,D,m)p(m|D)dm$
  - $y$ is independent of the data given $\theta$, this becomes $\int_M\int_\theta p(y|x,\theta,m)p(\theta|D,m)d\theta p(m|D)dm$
  - this isn't really model selection per se as we are averaging on models
- we can also approximate with a point mass $p(y|x,D)\approx p(y|x,D,m')$, where $m' \in \arg\max_M p(m|D)$
  - $m'$ is essentially a MAP estimate for $m$
- Type II MAP is $m' \in \arg\max_M p(m|D)$ (not a standard name)
- Type II MLE/Evidence Approximation/Empirical Bayes is $m' \in \arg\max_M p(D|m)$
  - $p(D|m)$ is the marginal likelihood
- as before, the MLE is a MAP with a uniform prior on models $m$ (improper if necessary)

## 12.5-12.7: Cross-validation

- $D = ((x_1,y_1),\dots,(x_n,y_n))$, $m\in \{1,\dots,c\}$
  - the possible model complexities are finite set of size $c$ (if $m$ is characterized by a continuous function, take some points from it)
- We want to find $m$ to minimize expected loss, i.e. $\epsilon_m = \mathbb{E} \mathcal{L}(Y,f_m(X))$
  - "expected" under the true distribution $p \sim (X,Y)$, which we don't know of course
- Thought experiment
  - suppose we had (with both distributed as $p$)
    - Data: $D = ((x_1,y_1),\dots,(x_n,y_n))$
    - Test set $T = ((x'_1,y'_1),\dots,(x'_n,y'_n))$
  - then we can train on the dataset and test on the test set, and take the best model
- so we just split $D$ into Training and Validation Sets
  - for each option of $m$, Train on training set, get error on validation set
  - this is "validation"
  - test on test set at the end.
- maybe the split is not representative
  - do a bunch of splits instead: $k$-fold Cross Validation
    - randomly permute the data
    - split the data into $k$ equally sized parts or "folds"
    - for each fold, use it as the validation set - and the rest of the data as the training set.
    - get $k$ rounds
    - for each round $i$ and model $m$, we get an estimate $\hat{\epsilon}_m(i)$
    - we estimate $\hat{\epsilon}_m = \frac{1}{k}\sum_i \hat{\epsilon}_m(i)$
    - choose $m'$ that minimizes $\hat{\epsilon}_m$
    - Train a model on all of $D$ with $m'$
- CV options
  - people often use $k$ fold with $k$ as 5 or 10
  - or Leave-one-out CV: each point is a fold, so do $n$ folds
  - Random subsamples: have $R$ rounds, for each round take a random subset of size $s$ from $D$ - use it as the validation set and do the same.
    - could be bad because a "hard" point could show up multiple times, so we overestimate error
      - or not show up at all, so we understimate rror
    - is nice as we size of the validation set $s$ and the number of rounds $R$
      - instead of $s = n/k$ and $R = k$ as in $k$-fold
- if do $k$-fold, how do we choose $k$?
  - Given a $k$, CV gives us an estimator $\hat{\epsilon}_m^k$ of $\epsilon_m$
  - Thus we should want to minimize the loss $\mathbb{E}\mathcal{L}(\epsilon_m, \hat{\epsilon}^k_m)$
  - i.e. we consider the process for choosing the parameter $k$ to choose the parameter $m$
  - If we pick $\mathcal{L}$ as the MSE, we can decompose into bias/variance
    - If $k$ is large (e.g. $n$ - leave one out cross validation)
      - models don't change too much from each other: low bias with respect to $\epsilon_m$ for the $m$ that we pick at the end
      - we get a very different estimate of $\epsilon_m$ for each fold: high variance
    - if $k$ is small (e.g. $k=2$)
      - train with less data, so we are biased to estimate higher error $\epsilon_m$ (models are just worse)
      - low variance as lots of validation points
    - people generally use $k=10$ 
- note: bayesian model selection can estimate in a more continous way with higher dimensions
  - so could be better in these situations than $k$-fold CV

## 12.8: Other approaches to model selection

- options
  - Bayesian Model Selection
  - Cross Validation
  - Information Criterion
    - AIC (Akaike Information Criterion)
    - BIC (Bayesian Information Criterion)
    - penalize model for having more parameters
  - MDL (Minimum Description Length)
    - basic idea is to choose a model with the smallest "description length"
  - VC (Vapnik-Chervonenkis dimension)
    - for classification, idea is to penalize models that "separate" more complex data
    - i.e. "have more wiggly" decision boundaries
- AIC, BIC, and MDL are similar to the "Type II MAP": get $m$ to maximize $p(m|D)\propto p(D|m)p(m)$
  - or maximize $\log p(D|m) + \log p(m)$
  - AIC, BIC, MDL in some sense put a prior (perhaps improper) on $p(m)$

# 13: Graphical models

## 13.1-13.2: Directed graphical models - introductory examples

- (Directed) graphical models:
  - Also known as "Bayesian" networks, but this is a bit of a misnomer
  - not really a model, but more of a "conditional independence diagram"
  - about factorizing the probability distribution
  - good notational device that allows us to visualize conditionally independent distributions and inference algorithms (like dynamic programming, markov chain monte carlo, etc)
- Conditional Independence is important for tractable inference
- Thinking "Graphically"/Network
  - Setup 1
    - Let $A$, $B$, and $C$ be random variables.
    - Consider $p(a,b,c) = p(c|a,b)p(a,b) = p(c|a,b)p(b|a)p(a)$
      - note that if $p(a,b)=0$, then $p(c|a,b)$ is undefined - but here we interpret $p(c|a,b)p(a,b) = 0$
      - A natural graph for this could have
        - a node for $A$, $B$, and $C$
        - $C$ is conditioned on $A$ and $B$, so make edges (directed) from $A$ and $B$ to $C$
        - $B$ is conditioned on $A$, so make an edge from $A$ to $B$
        - This is a DAG (Directed Acyclic Graph (no cycles))
    - We could have factored in a different way, and gotten a different graph though
    - Notation: $C\perp B |A$ signifies that $C$ is conditionally independent of $B$ given $A$.
    - If $C\perp B|A$, then $p(c|a,b) = p(c|a)$
      - now our diagram has two edges from $A$ to $B$ and $C$
      - graph now represents something about the distribution
  - Setup 2
    - $p(x_1,\dots,x_5)$
    - make a graph with 5 nodes for the $x_i$
    - Let's factor $p(x_1,\dots,x_5) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_1, x_2, x_3)p(x_5|x_1,x_2,x_3,x_4)$
      - we can build the same graph as before
        - edge from 1 to 2
        - edges from 1 and 2 to 3
        - edges from 1 and 2 and 3 to 4
        - edges from 1 and 2 and 3 and 4 to 5
      - this is a DAG too
      - suppose $x_3\perp x_2|x_1$, then we can erase an edge
      - given more CI, we can also erase more edges, and we get a graph that codifies stuff about the distribution
  - Setup 3
    - Given a DAG, we can follow the edges and reverse engineer a rule for probability distribution.
      - $p(x_1,\dots,x_n) = \prod_i p(x_i|pa(i))$, where $pa(i)$ are the $x_j$ with arrows that go to $x_i$
      - this doesn't specify/define a distribution, but characterizes some properties that probability distributions with this DAG have

## 13-3-13.4: Directed graphical models - formalism

- Notation: A DAG (Directed Acyclic Graph)
  - Directed: edges are oriented, have start/end
  - Acyclic: no directed cycles, which is equivalent to having an ordering of nodes such that an arrow from $i$ to $j$ implies that $i < j$
    - under such an ordering, we call the DAG "ordered"
- Notation: $pa(i)$ is the "parents" of vertex $i$ - i.e. all the vertices $j$ such that there's an arrow $j$ to $i$
- Notation: Given a vector $x = (x_1,\dots,x_n)$, we define $x_A = (x_i | i\in A)$, i.e. the tuple defined by $A$. Eg $x_{pa(i)}$ are the components of the parents of $i$
- Definition: Given $X = (X_1,\dots,X_n)\sim p$ and an ordered DAG $G$ on $n$ vertices, we say "$X$ respects $G$ (or $p$ respects $G$)" if we can factor the joint distribution $p(x_1,\dots,_n) = \prod_i p(x_i|x_{pa(i)})$ for all $x = (x_1,\dots,x_n)$
- Remark: This does not imply that any variables are conditionally dependent. Only implies that certain variables are conditionally independent.
  - Example: Suppose we have $x_1,x_2,x_3$ that are mutually independent. Then $p(x_1,x_2,x_3) = p(x_1)p(x_2)p(x_3)$.
    - Then this distribution respects all DAGs with 3 nodes.
      - e.g. $p(x_i|x_j) = p(x_i)$ for all $i \neq j$
- Terminology: A "complete directed graph" is a graph in which there are edges between all pairs of vertices $i$ and $j$.
  - Example: any distribution on $n$ variables respects any complete DAG. (decompose $p(x)$)
- Remark: We can combine random variables into vectors
  - e.g. look at $(X,Y,Z)$ where $X = (X_1,\dots,X_{n_x})$ etc.
    - i.e. $X$, $Y$, and $Z$ are themselves vectors of random variables
- Remark: if the factors in the decomposition are normalized conditional distributions, then the product of the joint is also a normalized distribution.
- DAGs are useful for tractable/efficient inference

## 13.5: Generative process specification

- a handy convention to specify a probability distribution in a compact way
- Example:
  - Setup
    - $X_1$, $X_2$ are independent random variables distributed as $Bernoulli(1/2)$
    - $X_3 \sim N(X_1 + X_2, \sigma^2)$
    - $X_4 \sim N(aX_2 + b,1)$
    - $X_5$ is $1$ if $X_4\geq 0$, $0$ otherwise
  - $X_i$ depends only on $X_j$ with $j < i$
  - Then $X = (X_1,\dots,X_N)$ respects the DAG
- So we can look at a graph and imagine a generative process for creating it
  - start with $x_i$ where $pa(i)$ is the empty set, and then go down the distribution
  - gives a nice way to sample and visualize the distribution

## 13.6: Graphical model for Bayesian linear regression

- Examples of Directed Graphical Models
  - Bayesian Linear Regression:
    - Setup: $D = ((x_1,y_1),\dots,(x_n,y_n))$, $x_i\in\mathbb{R}^d$, $y_i\in\mathbb{R}$
    - We want $f(x) = w^T\phi(x)$ for new values
      - $w$ is the weight vectors
      - $\phi$ are the basis functions
    - Generative model: 
      - $w\sim N(0,\sigma_0^2I)$
      - $y_i\sim N(w^T\phi(x_i),\sigma^2)$ Conditionally independent given $w$
    - Corresponds to the graph with $n+1$ nodes
      - one node for $w$
      - one node for each $y_i$
      - an arrow from $w$ to each $y_i$
      - We can add dots to signify non-random parameters:
        - one dot for $\sigma_0^2$ with an edge going to $w$
        - one dot for each $x_i$ with an edge going to $y_i$
        - a dot for $\sigma^2$ with an edge to each $y_i$
    - the joint distribution $p(w,y_i) = p(w)\prod_i p(y_i|w)$ respects this graph
    - this graph is a bit confusing, so we use "plate notation"
      - graph with $w$ going to a $y_i$ plate
        - $y_i$ plate:
          - has dot for $x_i$
          - write $n$ on it so we know how many identical $i$ plates there are
        - i.e. stack all the $i$ on top of each other
    - can include another node for the $y$ to predict
      - use shading to indicate the variables that we condition on e.g. the $y_i$
    - $y_i$ are observed random variables
    - $w$ is a latent/hidden variable

## 13.7: Graphical model for Bayesian Naïve Bayes

- Setup:
  - $D = ((x^1,y^1),\dots,(x^n,y^n))$, $x^i\in\mathbb{R}^d$, $y^i\in\{1,\dots,m\}$
  - $\pi\sim Dir(\alpha)$, $\alpha \in \mathbb{R}^m$, $\alpha_i > 0$
  - $r_{jy}\sim Dir(\beta)$ for $j = 1,\dots,d$, $y\in\{1,\dots,m\}$
    - $\beta = (\beta_i)$ for $\beta_i \in \mathbb{R}_+$
  - $Y\sim \pi)$, $y^i \sim \pi$
  - $X_j \sim r_{jy}$, $x^i_j \sim r_{jy}$ are independent given $y$
- Graphical model which the Setup respects:
  - dot for $\alpha$ going to a $\pi$ node
  - edge from $\pi$ node to $y$ node
  - $md$ nodes for the $r_{jy}$
  - $d$ nodes for the $X_i$ (coordinates of $x$)
  - edge from $y$ to the $x_i$ nodes
    - Naïve Bayes assumption: no arrows between the $x_i$
  - edges from the $r_{jy}$ nodes to the $x_i$ nodes.
  - dot for $\beta$ that goes to all of the $r_{jy}$
- Simplify with plate notation:
  - Stack the $r_{jy}$ for each $j$
  - Stack the data points $y^i$ along with the $x^i_j$
    - one more copy of this for the prediction task of $y$ from $x$
  - shade the $y_i$, $x^i_j$ data
    - others are latent/hidden variables
    - if predicting, also shade the new $x = (x_1,\dots,x_d)$

## 13.8-13.9: Conditional independence in graphical models

- Consider a graph with $C$ going to $A$ and $B$. If the joint distribution respects this graph, then we have
  - $p(a,b,c) = p(a|c)p(b|c)p(c)$
  - Now consider $p(a,b|c) = p(a,b,c)/p(c) = p(a|c)p(b|c)$, so that $A\perp B|C$
    - Remark: this assumes that $p(c)>0$
  - This is a "tail-tail" relationship between $A$ and $B$
  - For example, suppose that a boiler causes a noise and alarm. The noise and alarm seem linked, but if we condition on the boiler state they are independent.
- Consider a graph with $A$ going to $C$ going to $B$. Then we have
  - $p(a,b,c) = p(a)p(c|a)p(b|c)$
  - Let's condition on $c$, so that $p(a,b|c) = p(a,b,c)/p(c) = p(a|c)p(b|c)$
    - we assume $p(c) > 0$
    - $p(a)p(c|a) = p(a,c) = p(c)p(a|c)$
  - again, $A\perp B|C$
  - This is a "head-tail" relationship
  - e.g., a boiler causes steam which causes an alarm. The boiler and the alarm are not independent, but if we condition on the steam, they are independent.
    - the boiler causes the alarm through the steam 
- Consider a graph with $A$ and $B$ going to $C$:
  - $p(a,b,c) = p(a)p(b)p(c|a,b)$
  - $p(a,b|c) = p(a,b,c)/p(c) \neq p(a|c)p(b|c)$
  - Example: 
    - suppose that $A$ and $B$ are independent Bernoulli r.v. with $p=0.5$
    - suppose that $C$ is 1 if $A=B$ and $0$ otherwise
    - if we condition on $C$, then knowing $A$ causes us to know $B$, and vice versa
      - $A$ and $B$ are not conditionally independent given $C$
  - $p(a,b) = \sum_c p(a,b,c) = p(a)p(b)\sum p(c|a,b) = p(a)p(b)$
  - in other words, we know that $A$ and $B$ are independent $A\perp B$
    - just not necessarily conditionally independent given $C$
  - This is a "head-head" relationship
  - e.g. an alarm can go off based off of the boiler going on or of firealarm testing
    - the two are independent, but if we know it's time for testing, than when the alarm goes off we can conclude that it's not the boiler.

## 13.10-13.11: $d$-separation

- Way to read off conditional independence properties from the graph
- remember the tail-tail, head-tail, and head-head relationships from before
- Terminology: The "descendents" of a graph vertex in a DAG are all the vertices that can be reached from this vertex by following arrows.
- Definition: A path is a sequence of vertices where there are consecutive edges
  - note that we don't have to follow the direction of the arrows
- Let $G$ be a DAG and $A$, $B$, and $C$ be disjoint subsets of vertices (don't require that all vertices in the graph belong to one of them)
  - Definition: A path between two vertices is "blocked" with respect to $C$ if it passes through a vertex $v$ such that either
    - I: $v\in C$ and the arrows in the path are head-tail or tail-tail at $v$
    - II: $v\notin C$ and none of the descendents of $v$ are in $C$ and the arrows in the path are head-head at $v$
  - Definition: $A$ and $B$ are $d$-separated by $C$ if all paths from a vertex in $A$ to a vertex in $B$ are blocked with respsect to $C$
  - Theorem ($d$-separation): If $A$ and $B$  are $d$-separated by $C$, then $A\perp B|C$
    - Remark: the $d$ stands for "directed"?
    - Remark: does not say if and only if
    - Proof is a bit tricky.

## 13.12-13.13: How to use $d$-separation - illustrative examples

- Graph with 8 nodes
  - Setup Directed Edges:
    - 1 and 2 go to 3
    - 3 goes to 4 and 8
    - 4 and 5 go to 6
    - 6 goes to 7
  - $d$-separation conditioning on $C = \{3\}$, consider pairs:
    - 1 and 4 are $d$-separated, 1 and 2 are not, 4 and 5 are, 4 and 7 are, etc
    - can annotate graph with "stop signs" at vertices to improve readability
      - consider vertices in $C$, look for head-tail or tail-tail arrows
      - consider head-head vertices that are not in $C$
      - marks where paths can't go through
  - $d$-separation conditioning on $C = \{7\}$
    - no "stop signs"
    - Nothing is conditionally independent given 7
  - same graph as before, but add an edge from 1 to 5, condition on $C = \{3\}$ again
    - same stop signs as before
    - for some pairs $i,j$, now we might have 2 paths instead
      - less pairs are blocked, as can go around stop signs (e.g. 2/5 and 2/7)

# 14: Markov Models

## 14.1: Markov models - motivating examples

- Markov Models
  - the future is independent of the past given the present
  - temporal data, e.g. weather, finance, language, music, etc.
- Motivating Examples
  - Speech Recognition software: Dragon NaturallySpeaking
  - Artificial Musical Compositions
  - Language: Mark V Shaney
  - Temporal data
    - CO2 vs years
    - Lat/Lon for robot location
    - Fill in the blank/language model: What is the word at the end of this ...
- Sequential Data: $D = (x_1,\dots,x_n)$
  - modeled by RV $X_1,\dots,X_n$
    - simplest model is iid, but this doesn't capture time dependencies
    - think of weather - a good prediction for tomorrow is today's weather
      - have $X_t$ depend on the most recent points $X_{t-1},\dots,X_{t-m}$ for some fixed $m$
      - Simplest case: $m=1$

## 14.2-14.3: Markov chains (discrete-time)

- Discrete time Markov Chain
  - Assumptions
    - Markov assumption $X_t$ only depends on $X_{t-1}$
    - Discrete time and discrete space: $t$ and $x$ occur discretely (not continuous)
  - Definition: Discrete random variables $X_1,\dots,X_n$ form a (discrete-time) Markov Chain if the joint distribution respects the Graphical Model $X_1\rightarrow X_2\rightarrow\dots\rightarrow X_n$
    - i.e. $p(x_t|x_1,\dots,x_{t-1}) = p(x_t|x_{t-1})$
- Second order Markov Chain
  - $m=2$
  - $X_t$ only depends on $X_{t-1}$ and $X_{t-2}$
  - i.e. the graph has arrows from $X_{i-1}$ to $X_i$ and $X_{i-2}$ to $X_i$
- $n$-th order Markov Chain: arrows from $X_{i-j}$ to $X_i$ for $j = 1,\dots,n$
- Continuous-time MC
  - Brownian Motion: Used for stock prices
  - Poisson Process
- Concrete examples of Discrete-Time Discrete-Space first order MC
  - Random walk on the integers: go up/down with $p=1/2$
  - States: Sunny, Rainy, Snow, Cloudy
    - model transition between the states
      - if today is sunny, there are probabilities to each of the other states
- But
  - often, can't observe the true state of the system
    - e.g. 
      - model a robot's lan/lon, but there is some noise in transmission of the true state
      - CO2 measurements could be noisy
  - e.g. there is some hidden information that we're not seeing
    - model with hidden/latent variables - HMM

## 14.4-14.5: Hidden Markov Models (HMMs)

- widely used for sequential data
- simple enough to estimate the parameters and do inference
- rich enough to handle real world applications
- Setup
  - Random variables
    - Hidden/Latent variables: $z_1,\dots,z_n\in \{1,\dots,m\}$
    - Observed Random Variables: $x_1,\dots,x_n\in \mathcal{X}$, e.g. discrete, $\mathbb{R}$, $\mathbb{R}^d$
  - Random variables respect the "trellis diagram" graph:
    - arrows from $z_i$ to $x_i$
    - arrows from $z_i$ to $z_{i+1}$
  - can think of time as $i$
- As we respect the graph, $p(x_1\dots,x_n,z_1,\dots,z_n) = p(z_1)p(x_1|z_1)\prod_{k=2}^n p(z_k|z_{k-1})p(x_k|z_k)$
- Parameters
  - Transition probabilities from $i$ to $j$: $T(i,j) = p(z_{k+1} = j|z_k = i)$
    - can view as $m\times m$
  - Emission probabilities $\epsilon_i(x) = p(x_k|z_k = i)$ for $i\in \{1,\dots,m\}$, $x\in\mathcal{X}$
  - Initial distribution $\pi(i) = p(z_1 = i)$, $i \in \{1,\dots,m\}$
- The joint distribution then becomes $p(x_1,\dots,x_n,z_1,\dots,z_n) = \pi(i)\epsilon_{z_1}(x_1)\prod_{k=2}^n T(z_{k-1},z_k)\epsilon_{z_k}(x_k)$
- Remark: $\epsilon_i$ can be pretty arbitrary: 
  - $x_i$ can be discrete (e.g. poisson etc), real valued (e.g. Gaussian), etc.
  - depends on the observed output and how we want to model it
- Example
  - $z_k\in \{-1,1\}$
  - $x_k\in \mathbb{R}$
  - $T(i,j) \approx 1$ if $i = j$ (i.e. tends to stick at $1$ or $-1$ and occasionally flip)
  - $\epsilon_i(x)$ are Gaussian distributions centered around $i$ ($i\in \{-1,1\}$)

## 14.6-14.9: Forward-Backward algorithm for HMMs

- Perfect example of Dynamic Programming
  - Invented by Richard Bellman
  - problem is broken down into subproblems, solve subproblems, save results and optimize
  - i.e. reuse earlier results/computations
  - usually to find max/min range of query
- Forward-Backward
  - assume $p(x_k|z_k)$, $p(z_k|z_{k-1})$, $p(z_1)$ are known
  - algorithm lets us compute $p(z_k|x)$
    - i.e. the probability distribution of one of the hidden nodes given the observed output
  - Notation:
    - $x_{i:j} = (x_i,x_{i+1},\dots,x_{j-1})$
    - $x = (x_1,\dots,x_n) = x_{1:n}$
  - Parts
    - Forward Algorithm: Compute $p(z_k,x_{1:k})$ for all $k=1,\dots,n$
    - Backward Algorithm: Compute $p(x_{k+1:n}|z_k)$ for all $k=1,\dots,n$
  - Then $p(z_k|x)\propto p(z_k,x) = p(x_{k+1:n}|z_k,x_{1:k})p(z_k,x_{1:k})$
    - looking at the trellis diagram, we see that $x_{1:k}$ is conditionally independent of $x_{k+1:n}$ given $z_k$
      - by $d$-separation
      - also known as the Markov Assumption
      - thus $p(x_{k+1:n}|z_k,x_{1:k}) = p(x_{k+1:n}|z_k)$
    - Then the product is the backward part times the forward part.
- Then can do
  - Inference: e.g. $p(z_k\neq z_{k+1}|x)$ or other things
  - Estimate parameters: optimize initial, transition, and emission parameters via the Baum-Welch algorithm
    - Expectation Maximization with Forward/Backward Algorithm
  - Sample from the posterior
    - get most likely $z$: Viterbi Algorithm
    - sample: follow distribution
- Forward Algorithm:
  - Goal: Compute $p(z_k,x_{1:k})$
  - $p(z_k,x_{1:k}) = \sum_{z_{k-1}}^m p(z_k,z_{k-1},x_{1:k}) = \sum_{z_{k-1}} p(x_k|z_k,z_{k-1},x_{1:k-1})p(z_k|z_{k-1},x_{1:k-1})p(z_{k-1}|z_{k-1},x_{1:k-1})$
    - the $p(z_{k-1},x_{1:k-1})$ gives us some handy recursion
    - By Markovness/$d$-separation
      - $p(x_k|z_k,z_{k-1},x_{1:k-1}) = p(x_k|z_k)$, i.e. the emission probability
      - $p(z_k|z_{k-1},x_{1:k-1}) = p(z_k|z_{k-1})$, i.e. the transition probability
    - Thus $p(z_k,x_{1:k}) = \sum_{z_{k-1}}p(x_k|z_k)p(z_k|z_{k-1})p(z_{k-1},x_{1:k-1})$
    - Denote $\alpha_k(z_k) := p(z_k,x_{1:k})$
      - for $k\geq 2$, $\alpha_{k}(z_k) = \sum_{z_{k-1}}p(x_k|z_k)p(z_k|z_{k-1})\alpha_{k-1}(z_{k-1})$
      - for $k=1$, $\alpha_1(z_1) = p(z_1,x_1) = p(z_1)p(x_1|z_1)$
        - i.e. product of initial and emission probabilities
    - Starting with $\alpha_1$, we can use the recursion to compute $\alpha_i$ up to $i=n$
      - computing all of them is on the order of $nm^2$
        - $O(f(n))$ means that $\exists c, N$ such that the $\forall n > N$ $time_n \leq cf(n)$.
          - i.e. the time is bounded by $cf(n)$ after $N$
        - $\Theta(f(n))$ means that $\exists c, c' > 0, N$ s.t. $\forall n > N$, $c'f(n)\leq time_n\leq cf(n)$
          - i.e. it grows roughly proportionally to $f(n)$
  - consider the naïve approach 
    - $p(z_k|x) = p(z_k,x)/p(x)$
      - $p(z_k,x) = \sum_{z_i, i\neq k} p(x,z)$
        - so have to calculate $p(x,z)$ $m^n$ times - i.e. all possible sequences of $z$
    - Dynamic programming is much better
      - reuse old computation
      - in particular, we reuse the $\alpha_i(z_i)$, i.e. the subproblem for shorter sequences
- Common problems with Markov Chains 
  - Hitting times, expected number of visits to a particular state, etc.
  - for many of these problems, we can set up a recursion
    - can sometimes then solve analytically
    - or can use Dynamic Programming
