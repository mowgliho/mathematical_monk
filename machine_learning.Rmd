---
title: "Machine Learning"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Machine learning: overview and applications

- Machine learning is defined here as algorithms for inferring unknowns from knowns
  - subfield of statistics focusing on algorithms
- Many different applications

# 1.2: What is supervised learning?

- Supervised learning: Given $(x^1,y^1),\dots,(x^n,y^n)$, where $x^i$ are data points, $y^i$ are class/value, we want to choose a function $f(x) = y$
  - and be able to generalize to new $x$ points.
  - Classification: $y^i\in \mathcal{Y}$, where $\mathcal{Y}$ is a finite set
  - Regression $y^i \in \mathbb{R}^d$
    - note that the word "regression" is historical

# 1.3: What is unsupervised learning

- Unsupervised learning is well defined
  - Given $(x^1,\dots,x^n)$, typically $x^i\in \mathbb{R}^k$, find "patterns" in the data
    - Clustering: separate data into clusters
    - Density estimation: find a function for the density of the data
    - Dimensionality reduction: find a lower dimensional space to represent data

# 1.4: Variations on supervised and unsupervised

- Semi-supervised Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - the task is to predict the labels $y^{k+1},\dots,y^n$
  - i.e. we know the location of the unlabeled points
- Active Learning: get labeled data $(x^1,y^1),\dots,(x^k,y^k)$, unlabeled $x^{k+1},\dots,x^n$
  - can ask for $y^k$'s for particular points $x^k$
  - and after ask for a few points, make predictions on the rest
- Decision theory: How to decide to measure errors/loss functions
  - e.g. weight false positives more than false negatives (e.g. a doctor deciding to do a test)
- Reinforcement Learning: Make actions to maximize long term rewards and minimize overall losses, improve model as go along.

# 1.5: Generative vs discriminative models

- Discrimative: Model $\mathbb{P}(y|x)$, i.e. the class of a given point $x$
- Generative: Model the joint distribution $\mathbb{P}(x,y)$
- can use generative to discriminate by comparing probabilities
- creating generative models takes a lot of data, have to estimate things, etc, so often discriminative models are better - and more appropriate to the problem at hand.

# 1.6: $k$-Nearest Neighbor classification algorithm

- Given data $(x_i, y_i)$, $x_i\in \mathbb{R}^d$, $y_i\in \{0,1\}$, classify a new $x$
- $kNN$: by a majority vote of the $k$ nearest points.
- Distance given by, for example, the euclidean distance
- often works pretty well, has some nice theoretical guarantees as $n$ grows large
- probabilistic interpretation: $p(y)$ is the fraction of points among the $k$ nearest neighbors with label $y$, and we select the $y$ that maximizes this probability.
  - this is a discriminative model
- $k$ is a parameter, so we can select via cross-validation, bias variance tradeoff, etc.

# 2.1: Classification trees (CART)

- Decision trees are very simple, but can have pretty good performance (especially paired with randomization)
- Classification trees
  - main idea is to form a binary tree and minimize the error in each leaf of the tree
    - nodes are either 
      - a question/decision 
      - a leaf: classify by majority vote at the leaf

# 2.2: Regression trees (CART)

- Regression tree
  - again, make a binary tree and minimize error in each leaf
    - for example
      - split on $x \leq r$, i.e. separate $x$-axis piecewise
      - put a constant model in each leaf (e.g. the average)
    - e.g. error as square error over all $y$'s
    - get a piecewise constant function

# 2.3: Growing a regression tree (CART)

- note that the $x$ can be real valued, discrete, or even unordered
- Growing a tree
  - Greedy: At each step, choose the optimal decision
    - may not get globally optimal tree
    - for example, if $x_i\in \mathbb{R}^d$, we find $s$ and $j$ to minimize the prediction as the minimum $y$ for each part:
      - $\min_y\sum_{i:x_{ij}>s}(y-y_i)^2 + \min_y\sum_{i:x_{ij}\leq s}(y-y_i)^2$
        - $y$ is the prediction for each part
      - as we have a finite number of data points, we have in essence a finite number of split points
      - and repeat
    - how do we stop:
      - otherwise we end up with one point $x$ per leaf
      - only consider splits with regions with at least $n$ points
      - or something else
- we can prune afterwards or do something like random forest afterwards

# 2.4: Growing a classification tree (CART)

- suppose that the points are in $\mathbb{R}^d$, classified into classes
- given a subregion $R$, we classify by a majority/plurality vote
  - define $E_R = \min_y(1/N_R)\sum_{i: x_i\in R} I(y_i\neq y)$, 
    - $N_R$ is the number of train points in $R$
    - $I$ is the indicator function
  - define $R(j,s)$ to be $\{x_i: x_{ij} > s\}$, $R'(j,s)=\{x_i:x_{ij}\leq s\}$, i.e. the point sets defined by a split
- To grow a classification tree, choose a dimension $j$ and a split $s$ to minimize
  - $E_{R(j,s)} + E_{R'(j,s)}$
  - and repeat
- stop when 
  - number of points reach a minimum
  - $R_k$ contains points of only one class
  - or something else
- could also try to minimize entropy/gini index instead

# 2.5: Generalizations for trees (CART)

- impurity measures for classification:
  - $E_R$: the proportion of stuff that would be misclassified (which we used before)
  - $H_R$: entropy, i.e. if we define $p_R(y)$ to be the fraction of points in $R$ with class $y$
    - $H_R = -\sum_{y\in\mathcal{Y}} p_R(y)\log p_R(y)$
    - entropy for $R$ with all in one class is $0$
    - We want to find a split to make it as homogeneous as possible
  - $G_R$: Gini index, i.e.
    - $G_R = \sum_{y\in\mathcal{Y}} p_R(y) (1-p_R(y))$
    - has some nice analytic properties
- $H_R$ and $G_R$ can have some better performance
- more generalizations/topics:
  - the notion of a split if if the $x_i$ are categorical, so that splitting doesn't strictly make sense
  - use a loss matrix for loss instead
  - missing values: recommended solution is "surrogate variables"
  - splits on linear combinations instead of a coordinate
    - but this is less efficient and surprisingly doesn't really give better performance
      - especially with the upcoming aggregation techniques
  - instability: estimator can have high variance, so has low performance in general

# 2.6: Bootstrap Aggregation (Bagging)

- take a simple algorithm and make it better (not always, but quite often)
- reduces the variation of a classification or regression procedure
- here, we do regression
- Given
  - Data $D = (X_i,Y_i)$, distributed by $P$ iid, the $X$ and $Y$ are random variables
  - Want to predict the $y$ for an $x$
- If we had a bunch of data sets $D_i$, each gives a model to predict $x$
  - if each dataset is drawn from $P$, then the average of the $y$s will be pretty good as an estimate as the true $y$
  - $D_i = (X^i_j,Y^i_j)$, as $j$ ranges
  - suppose that we have an "unbiased" estimator, i.e. the mean of $Y_j$ is the true $Y$, i.e. $\mathbb{E}Y=y=f(x)$
  - $\mathbb{E}((Y-y)^2) = E((Y-\mathbb{E}(Y))^2) := \sigma^2(Y)$
  - define $z = (1/m)\sum_1^m Y^i$, then $\mathbb{E}(z)=y$, so $z$ is also an unbiased estimator of $y$
    - i.e. if we use the empirical distribution
  - Then $\mathbb{E}((Z-y)^2) = \mathbb{E}((z-\mathbb{E}(z))^2) = \sigma^2(z)=\sigma^2((1/m)\sum Y^i) = (1/m)^2\sigma^2(\sum Y^i) = (1/m^2)\sum(\sigma^2(Y^i)) = (1/m) \sigma^2(y)$
    - in other words, our expected loss from using $z$ to estimate is $(1/m) \sigma^2(y)$, 
      - i.e. the original estimator divided by $m$
      - so as we take $m$ to infinity, the expected loss goes to 0
- Unfortunately, we don't have a bunch of different datasets
  - but we can simulate this by sampling from our dataset
  - e.g. draw uniformly from the original dataset $D$, iid, with replacement

# 2.7: Bagging for classification

- natural extensions of previous one
  - majority vote of models (each dataset gives us a classifier $C_i$)
  - average the estimated probabilities from each classifier: each $C_i$ gives us an estimated pmf $p^i$, and then we do $p(y) = (1/m)\sum_i p^i(y)$
    - similar to the regression problem from before where we try to estimate the true probability of $y$
      - same analysis holds if the $p^i$ are unbiased - so $p$ is a unbiased estimator
    - and then classify $x$ as the $y$ with the highest $p(y)$ for $x$
  - suppose that the output follows a random distribution $W$, which is independent from $Y$ (the realizations in the form of data)
    - then we want to minimize $\mathbb{E}((Y-W)^2) = \mathbb{E}((Y-\mathbb{E}(Y) + \mathbb{E}(Y) - W)^2)$
    - then this is $\mathbb{E}((Y-\mathbb{E}(Y))^2) + \mathbb{E}((\mathbb{E}(Y) - W)^2)$
      - note that we used independentness to get rid of a few terms
    - $\geq \mathbb{E}((\mathbb{E}(Y)-W)^2)$
    - in other words, using $\mathbb{E}(Y)$ instead of $Y$ reduces expected error
    - as before, we can approximate $\mathbb{E}(Y)$ with $z$, i.e. the data that we have - and reduce randomness again
  - i.e. more rationalization of the bagging algorithm

# 2.8: Random forests

- Combination of CART trees and bagging - and "random subspace"
- Simple idea, but somehow state of the art performance
- Random Forests:
  - We have some observed data points $D = ((x_1,y_1),\dots,(x_n,y_n))$
  - for $i = 1,\dots,B$, create a bootstrap data $D_i$ from $D$ via sampling with replacement
  - construct a tree $T_i$ using $D_i$ s.t.
    - at each node, choose a random subset of the features (e.g. dimensions of $x_i$)
    - only consider splits on those features (this is the "random subspace")
  - Then do the aggregation as before on these $B$ trees $T_i$: majority vote (classification), average of probabilities (classification), average of predictions (regression)
- works for the same reason that bootstrapping works
  - each tree has high variance, but boostrapping reduces the variance

# 3.1: Decision Theory (Basic Framework)

- Decision Theory: minimize expected loss
- Loss Matrix:
  - e.g., look at FP, TP, FN, TN table, each has a loss
  - Define a loss function $L(y,\hat{y})$
    - "0-1 loss": $L(y,\hat{y}) = I(y\neq \hat{y})$, $I$ being the indicator function
    - "Square loss": $L(y,\hat{y}) = (y-\hat{y})^2$
- General Framework
  - State $s$ (true/unknown)
  - Have an observation (known)
  - take an action $a$
  - get a loss $L(s,a)$
- note that we can do the opposite and do "reward/utility" instead

# 3.2: Minimizing conditional expected loss

- Decision theory for supervised learning
  - Setup
    - Data $\{(x_i,y_i)\}$
  - Possible Situation 1:
    - get a new $x$, predict $\hat{y}$ for the true $y$
    - given $x$, we want to pick $y$ to minimize loss $L(y,\hat{y})$
    - but we don't know the true $y$
  - Possible Situation 2:
    - choose $f$ to minimize $L(y,f(x))$, but we don't know $x$ or $y$
    - i.e. choose a general function to minimize the $x$ that may come
- We move to Probabilistic framework: Want small loss on average
  - Data comes as $(X,Y)$, where $X$ and $Y$ are random variables according to $p$
    - for now, we assume that they are discrete
  - Possible Situation 1 now turns into minimizing 
    - $\mathbb{E}(L(Y, \hat{y})|X = x) = \sum_{y\in\mathcal{Y}}L(y,\hat{y})p(y|x)$
    - e.g. for 0-1 loss, we get $\sum_{y\neq \hat{y}}p(y|x) = 1 - p(\hat{y}|x)$
      - so we should pick $\hat{y}$ to maximize $p(\hat{y}|x)$
      - i.e. minimizing the expected 0-1 loss tells us to predict the most likely $y$

# 3.3: Choosing $f$ to minimize expected loss

- Now let's look at Possible Situation 2 from above
  - $\hat{y} = f(x)$
  - $\mathbb{E}(L(Y,\hat{Y})) = \mathbb{E}(L(Y,f(x))) = \sum_{x,y} L(y,f(x))p(x,y) = \sum_{x,y}L(y,f(x))p(y|x)p(x) = \sum_xp(x)\sum_yL(y,f(x))p(y|x)$
  - setting $g(x,f(x)):= \sum_y L(y,f(x))p(y|x)$, this turns into the expectation $\mathbb{E}^Xg(X,f(X))$ over the distribution $p(x)$.
  - Suppose for some $x',t$, $g(x',f(x')) > g(x',t)$.
    - define $f_0(x)$ to be $f(x)$ if $x\neq x'$, and $t$ if $x = x'$
    - then $g(x,f(x))\geq g(x,f_0(x))$
    - in particular, $\mathbb{E}^X(g,X,f(X)) \geq \mathbb{E}^X(g(X,f_0(X)))$
      - in other words to minimize the loss, we should choose $f$ to minimize $g(x,f(x))$, i.e. $\arg\min_t g(x,t)$
  - The minimization problem for situation 2 reduces to many cases of the minimization problem for situation 1
    - the fact that we don't know $x$ yet doesn't really matter
    - more important is $p(y|x)$

# 3.4: Square Loss

- Consider the square loss $L(y,\hat{y}) = (y-\hat{y})^2$, with $X,Y$ given by $p$
  - no longer discrete, suppose that $p$ is a smooth and has a derivative
  - $\mathbb{E}(L(Y,\hat{y})|X=x) = \int L(y,\hat{y})p(y|x)dy = \int(y-\hat{y})^2p(y|x)dy$
  - take the derivative wrt $\hat{y}$ and set to 0
    - $0 = \int 2(\hat{y}-y)p(y|x)dy = 2\hat{y} \int p(y|x)dy - 2\int yp(y|x)dy = 2\hat{y} - 2\mathbb{E}(Y|X=x)$
    - $\hat{y} = \mathbb{E}(Y|X=x)$
    - if we take another derivative, we note that it is positive $2$, so that we've found a minimum
  - in other words, the prediction at $x$ to minimize the expected square loss is the expectation of $Y$ given $x$
    - this is a nice property of the square loss
- the solution to the minimum problem for the square loss is to set $f(x):=\mathbb{E}(Y|X=x)$, i.e. the expectation of $Y$ given $x$ at each $x$

# 3.4: The Big Picture (part 1)

- Specifically focusing on supervised learning, but the concepts generalize
- many of the core concepts of machine learning come from trying to minimizing $\mathbb{E}(L(Y,f(X)))$
- $p(y|x)$, not $p(x)$ was key to minimizing expected loss
  - Discriminative methods: Estimate $p(y|x)$ directly using data $D$
    - e.g. kNN, Trees, SVM
    - easier to compute
  - Generative methods: Estimate the joint distribution $p(y,x)$ and then recover $p(y|x) = p(x,y)/p(x)$
    - harder to compute, but are a richer family
- parameters/latent variables: we denote as $\theta$. We can think of it as a random variable and consider $p(x,y|\theta)$
- we can also think of the data as being random: $p(y|x,D) = \int_\theta p(y|\theta,x,D)p(\theta|x,D)d\theta$
  - i.e. integrate over possible parameters $\theta$
  - $p(y|x,D,\theta)$ is typically nice, can get a defined/analytic model
  - $p(\theta|x,D)$ is nasty: usually can't get a closed form expression
  - the integral sucks too.
  - this problem often intractable

# 3.5: The Big Picture (part 2)

- notation:
  - $p(y|x,D)$ is the "predictive distribution" on $y$
  - $p(\theta|x,D)$ is the "posterior distribution on $\theta$
- approaches to solve the intractable problem from before
  - Exact inference: assume a nice enough model that has nice distributions
    - Multivariate Gaussian, Conjugate prior, Graphical models
  - Point estimates of $\theta$: 
    - e.g. 
      - MLE: $\theta_{MLE} = \arg\max_\theta \mathcal{L}(\theta|y)$, where $\mathcal{L}$ is the likelihood function
      - MAP: $\theta_{MAP} = \arg\max_\theta p(\theta | x,D)$
    - then estimate $p(y|x,D)\approx p(y|x,D,\theta)$
    - to solve for $\theta$, may need to use optimization techniques or expectation maximization
    - Empirical Bayes kind of fits in here too
  - Deterministic Approximations of the integral
    - Laplace Approximation, Variational methods, Expectation Propagation
  - Stochastic Approximations
    - Markov Chain Monte Carlo (Gibbs, Metropolis-Hastings), Importance Sampling
- many methods combine these techniques

# 3.6: The Big Picture (part 3)

- Extensions of methods above to unsupervised learning
- Density Estimation (Unsupervised)
  - $D = (x_1,\dots,x_n)$, $x_i\in \mathbb{R}^d$, iid
  - Goal is to estimate the density of the $x$
  - given parameters $\theta$, we can have a probability distribution $p_\theta$ that generates the $x_i$
  - we want to calculate $p(x|D) = \int p(x|D,\theta)p(\theta|D)d\theta$
    - it turns out that we can reduce to $p(x|\theta,D) = p(x|\theta)$
    - $p(\theta|D)$ is the posterior of $\theta$ given $D$
  - same problem as before, and we can apply the same types of methods as before:
    - $p(x|\theta)$ is nice: just the model
    - $p(\theta|D)$ and the integral are not nice.

# 4.1: Maximum Likelihood Estimation (MLE) (part 1)

- Setup:
  - Given data $D = (x_1,dots,x_n)$, $x_i\in \mathbb{R}^d$
  - assume a set of distributions $\{p_\theta:\theta \in\Theta\}$ on $\mathbb{R}^d$
  - Assume that $D$ is an iid sample according to $p_\theta$ for some $\theta\in\Theta$
- Goal: Estimate the true value of $\theta$ that generated the data
- Definition: $\theta_{MLE}$ is a MLE for $\theta$ if $\theta_{MLE}=\arg\max_\theta p(D|\theta)$
  - more precisely, it maximizes $p(D|\theta_{MLE}) = \max_\theta p(D|\theta)$, as there can br multiple $\theta$ that maximize it in theory.
  - The likelihood $p(D|\theta) = \prod_1^n p(x_i|\theta)$, as they are iid.
- Remark:
  - MLE may not be unique
  - MLE may fail to exist (e.g. maximum of $p(D|\theta)$ may not be achieved for $\theta \in \Theta$)
- Pros:
  - Easy to compute
  - interpretable
  - has some nice asymptotic properties:
    - consistent: as $n$ goes to infinity, it converges to the true $\theta$
    - normal: as $n$ goes to infinity, the distrubution of $\theta$ is normal
    - efficient: it's the best possible estimate for the true $\theta$ because it has the lowest asymptotic variance/error
  - invariant under reparameterization of $\theta$: $g(\theta_{MLE})$ is the MLE for $g(\theta)$ for some function $g$
- Cons:
  - Point estimate, so has no representation of uncertainty (i.e. we don't consider $p(\theta|D)$)
    - for example, perhaps the likelihood spikes to a high maximum, but has a larger/more probable distribution elsewhere

# 4.2: Maximum Likelihood Estimation (MLE) (part 2)

- more cons of MLE
  - Overfitting
    - regression: e.g. curvy line when straight line is better
    - black swan paradox: give 0 probability to stuff not in data
  - Wrong objective? e.g. MLE disregards the loss function
  - Existence and Uniqueness is not guaranteed

# 4.3: MLE for univariate Gaussian mean

- Remember: $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
- We assume a univariate gaussian $X \sim N(\theta,\sigma^2)$
- $p(x|\theta) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp (-\frac{1}{2\sigma^2}(x-\theta)^2)$
- Data is $D=(x_1,\dots,x_n)$, $X_i \sim N(\theta, \sigma^2)$ iid
- Then $p(D|\theta) = p(x_1,\dots,x_n|\theta) = \prod_1^np(x_i|\theta)$
- If we plug in the equation for $p(x_i|\theta)$ and take the derivative
  - note that maximizing the probability is the same as maximizing the log,
  - so we can maximize $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_1^n(x_i-\theta)^2$
  - differentiate and set to 0 w.r.t $\theta$: $0 = \frac{1}{2\sigma^2}\sum_i 2(x_i - \theta)$
    - $n\theta = \sum_i x_i$, so $\theta = \frac{1}{n}\sum_i x_i$
  - take the second derivative wrt $\theta$ we get $-n/\sigma^2 < 0$, so that this is a maximum
  - thus $\theta$ is given by the average of the $x_i$'s, ie the sample mean
- if you take the derivative wrt to $\sigma$, you obtain $\sigma^2 = \frac{\sum(x_i-\theta)^2}{n}$, i.e. the sample variance

# 4.4: MLE for a PMF on a finite set (part 1)

- $X \sim p$, $X\in \{1,\dots,m\}$
  - for example, look at a die: $m = 6$
  - Given data, the MLE finds the distribution of $p$
- Data $D = (x_1,\dots, x_n)$, $X_1,\dots,X_n\sim p$ iid
- We parameterize $\theta = (\theta_1,\dots,\theta_n)$, where $\theta = p_i = p(i|\theta) = p_\theta(i)$
- We want to find the MLE $\theta_{MLE} = \arg\max_\theta p(D|\theta)$
- $p(x|\theta) = \theta_x$, so $p(D|\theta) = \prod_i \theta_{x_i} = \prod_{i=1}^n \prod_{j=1}^m \theta_j^{I(x_i = j)} = \prod_{j=1}^m\theta_j^{\sum_{i=1}^mI(x_i=j)} = \prod_{j=1}^m \theta_j^{n_j}$
  - $n_j$ is the number of $j$s in the data
- to maximize $p(D|\theta)$, we can take the log, so we want to maximize $\sum_{j=1}^m n_j\log \theta_j$
- we can also max if we divided by $n$, i.e. $\sum_{j=1}^m \frac{n_j}{n}\log \theta_j$

# 4.5: MLE for a PMF on a finite set (part 2)

- continuing from last time
- set $q_j = n_j/n$
  - note that $\sum_j q_j = 1$
- Now consider relative entropy or Kullback-Leibler divergence, $D(\theta ||q)$
  - $D(\theta ||q) = \sum_{j=1}^m q_j \log (q_j/\theta_j)$
  - $D(\theta ||q) \geq 0$ and is $0$ iff $\theta = q$
- we are trying to maximize $\sum_{j=1}^m q_j \log \theta_j = \sum_{j=1}^m q_j \log(\theta_j/q_j) + \sum_{j=1}^m q_j\log q_j = -D(\theta || q) - H(q)$
  - As $H(q)$ is unaffected by $\theta$, this is maximized when $\theta = q$, 
- $\theta_{MLE} = (\theta_1,\dots,\theta_n)$, where $\theta_j = n_j/n$, or the proportion of the data that is $j$
  - i.e. the empirical distribution
 
