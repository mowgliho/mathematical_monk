---
title: "Information Theory"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Outline of Topics

- efficient (i.e. compression/source coding) and reliable (i.e. error correction/channel coding) transfer of data from a source to a destination
- examples
  - compression of images, videos, text files, audio files
  - RAM, space stuff, bar codes, tv, dsl,
  - etc.
- axis 1: efficiency vs reliablity
  - efficiency: compression; source coding
  - reliability: error correction; channel coding
- axis 2: math vs algorithms
  - math: Information Theory
  - algorithms: Coding methods
- topics:
  - compression & information theory:
    - Lossless compression
      - source coding theory (Shannon)
      - Kraft-Mcmillan inequality
    - Lossy compression
      - Rate-Distortion theorem
  - error-correction & information theory
    - (noisy) channel coding theorem
    - Channel capacity
    - Typicality & Asymptotic Equipartition Property
  - compression & coding methods
    - Symbol codes
      - Huffman 
    - Stream codes
      - Arithmetic coding
      - Lempel-Zim
  - error-correction & coding methods
    - Hamming 
    - BCH 
    - Reed-Solomon
    - Turbo codes
    - Gallager codes (LDPC)
- closely related fields
  - cryptography/cryptoanalysis
  - algorithmic information theory
  - kolmogorov complexity
  - minimum description length
  - network information theory
- also related
  - statistics
  - machine learning
  - portfolio theory/gambling

# 1.2: Applications of Compression Codes/Source coding

- focus on lossless compression algorithms
- Applications
  - Huffman
    - in some sense, an optimal compression algorithm (among "symbol codes")
    - simple to understand, implement
    - efficient in smallish applications
    - png, jpg, mpeg, winzip, gzip, mp3, aac
  - Arithmetic coding
    - sometimes, a symbol code is not the best to use (so not Hoffman)
    - scales better than Huffman
    - had IP and computation issues, but these are better now
    - jpg, jbig, mpeg, skype, flash, ppm, paq, DjVu
  - Lempel-Ziv
    - was shipped was unix
    - is asymptotically optimal for "universal codes"
    - efficient and simple to implement, useful for files generally on computers
    - png, gif, pkzip, gzip, pdf

# 1.3: Applications of Error Correction Codes (ECC)/Channel Coding

- Hamming (1950)
  - first reasonably good error correction code
  - are better ones now
  - still used in
    - DRAM (dynamic memory): reading and writing from chips is not error-free: due to magnetic fluctuations etc.
    - Static memory: RAID 2
- Reed-Solomon (1960)
  - uses abstract algebra, RSV (V is viterbi, RS is Reed-Solomon)
  - robust to burst errors (lots of errors together) - good for scratches on CDs
  - Bar codes, CD, DVD, Blu-Ray, DSL, RAID6, DVB
  - Space Missions: Voyager 1/2 (1977), Galileo (1989), Cassini (1997), Mars Pathfinder (1996), MER (2003)
- Turbocodes (1993)
  - much closer to the "Shannon limit": State of the Art
  - 3G, 4G, LTE, MedioFLO, WiMax: what enables you to get data at such a high rate
  - Space Missions: Mars Reconnaisance Orbiter
- Gallager (Low Density Parity Check) 1960
  - Written in Gallager's PhD Thesis, but not good for the computers at the time
  - Rediscovered in 1995, also State of the Art
  - Ethernet, WiFi 802.11n, Internet over power lines (ITU-T Ghn), "Smart Grid", Satellite TV (DVB-S2), CMMB (China MultiMedia Broadcasting), DTMB

# 1.4: Source-channel separation

- Overview
  - Source: emits message from some set (usually have little control)
  - Encoder: transforms message and sends to channel: we can design the encoder
  - Channel: typically noisy
  - Decoder: takes output of channel and tries to transform back
  - Destination
- can be limited to sending messages at a certain rate
  - want to send 
    - as few bits as possible
    - lowest chance of having an error
- compression and error-correction are in the encoder and decoder
- Forward Error Correction: e.g. Decoder can't ask Encoder to resend something
  - unlike some internet protocols.
- Source-channel Separation Theorem (Shannon):
  - This process can be rewritten as
    - Source
    - Source Encoder (Compression)
    - Channel Encoder (Error Correction encoding)
    - Noisy Channel
    - Channel Decoder (error correction decoding)
    - Source Decoder (decompression)
    - Destination
  - I.e. you can design the source and channel encoder separately and do just as well
    - doesn't say anything about computational complexity involved - could design together in theory with equal performance and less computational complexity.
  - holds for all sources that satisfy the "Acquisition Equipartition Property"

# 1.5: Examples of source-encoder-channel pipelines

- Pale Blue Dot image from Voyager
  - Image of earth on vehicle (S) -> Difference Mapping (SE) -> RSV (CE) -> electromagnetic waves (C) -> RSV (CD) -> Difference Mapping (SD) -> image file on earth (D)
- Internet Connection
  - directed connected to internet/modem
  - no source encoder/decoder
  - Computer (S) -> Ethernet encoder (CE) -> Cat-5 cable (C) -> Ethernet decoder (CD) -> Router/modem (D)
- Rip a CD and Play it
  - source and destination can be same physical location
    - Hard disk is the channel
  - CD Reader (S) -> mp3 (SE) -> CRC & RSV (CE) -> Hard Disk (C) -> CRC & RSV (CD) -> mp3 (SD) -> human ears (D)
- Skype over 4G
  - can chain bits together
  - Webcam (S) -> mpeg:SILK (SE) -> 4G encoding: turbocodes (CE) -> electromagnetic waves (C) -> 4G (CD) -> internet encoder (CE) -> internet (C) -> internet decoder (CD) -> mpeg decoder (SD) -> your friend

# 1.6: A different notion of "information"

- The information-theoretic notion of "information"
  - mathematical formulation vs everyday notion:
    - everyday notion: wikipedia article has more information than image of static
    - if judge by number of bits to encode/entropy, then it's the other way around
    - static is much more random/unpredictable
    - no concept of "utility" in information theory
    - information is exhibited through randomnmess: i.e. how unpredictable it is
  - we're trying to faithfully transmit messages that are sent by humans/computeres/etc
    - so the source is useful from the start
    - or the real-world usefulness of a message is an assumption that we don't bother with
    - what if we're doing lossy compression though?
      - then we make judgements on what parts of a message are "unimportant"
      - measure "utility" via distortion theory/function
        - i.e. accept a particular amount of distortion, try to send a minimal amount of information

# 2.1: A puzzle on weighing coins

- 12 coins, either all the same weight, or one has a different weight
- have a scale
- 3 weighings to figure out if one is different, and if so which one and if heavier or lighter
- can use basic ideas from information theory to simplify problems of this sort

# 2.2: Symbol codes - terminology and notation

- 1830s, telegraph technology
- Samuel Morse and Alfed Veil invent Morse Code
  - text messages over the telegram
  - can be seen as sequence of 0s/1s
- symbol code/variable length code
  - each symbol has different lengths
  - morse code can be seen as each letter goes to a binary code word
    - more frequent letters have shorter codes
    - i.e. lossless message compression using a probabilistic model
      - where letters are assumed to follow a iid probability
- terms
  - source: sequence of random variables $x_1,\dots,x_n$
  - memoryless: iid
  - Discrete memoryless source: $x_1,\dots,x_n \in \mathcal{X}$, where $\mathcal{X}$ is countable.
  - alphabet: set of elements
    - generally a source alphabet $\mathcal{X}$ and a code alphabet $\mathcal{A}$
    - morse code: $\mathcal{X} = \{A,B,C...,0\}$, $\mathcal{A} = \{0,1\}$
  - $\mathcal{A}_0 = \{a_1,\dots,a_k: k >= 0, a_i\in \mathcal{A} \forall i\}$, i.e. all finite length sequences of letter in $\mathcal{A}$

# 2.3: Symbol codes - definition and examples

- Symbol Code: (Variable length code) is a function $C: \mathcal{X} \rightarrow \mathcal{A}_{0}$.
- The extension of $C$: the function $C_{0}: \mathcal{X}_{0} \rightarrow \mathcal{A}_{0}$ such that $\forall n\geq 0, \forall x_i \in \mathcal{X}, C_{0} (x_1\dots x_n) = C(x_1)\dots C(x_n)$
  - i.e. image of the concatenation is the concatenation of the images
    - sequence of letters -> sequence of code words
- Codewords of $C$: The sequences $a_1\dots a_k \in \mathcal{A}_0$ such that $C(x) = a_1\dots a_k$ for some $x \in \mathcal{X}$
- Examples
  - Morse Code
    - $\mathcal{X} = \{A,B,\dots,0\}$
    - $\mathcal{A} = \{0,1,\text{pause}\}$
    - $C$ is the mapping
  - Toy example (A)
    - $\mathcal{X} = \{a,b,c,d\}$, $\mathcal{A} = \{0,1\}$, $X$ is $a$, $b$, $c$, and $d$ with probability $1/2, 1/4, 1/8, 1/8$.
    - $C(a) = 0, C(b) = 10, C(c) = 110, C(d) = 111$
    - e.g. $C_{0}(aacb) = 0011010 \in \mathcal{A}_{0}, aacb \in \mathcal{X}_{0}$
  - Other toy examples
    - (B) same as before, but $C(a) = 101, C(b) = 00, C(c) = 0001, C(d) = 1$
    - (C) same as before, but $C(a) = 0, C(b) = 1, C(c) = 01, C(d) = 10$
      - is a valid code, but $C_{0}$ is non-injective (or not uniquely decodeable): $C_{0}(ab) = C_{0}(c)$

# 2.4 Decoding - prefix vs. non-prefix

- Definition: $C$ is "uniquely decodable" if $C_{0}$ is injective.
  - necessary for lossless compression
- Desirable properties for compression
  - efficiency
  - computational speed
  - simplicity
- Computational speed
  - encoding: if $\mathcal{X}$ is finite, then is easy (lookup table)
  - decoding: in some examples, (A) is easier to decode than (B) as you have to look farther ahead to decode.
- (A) is an example of a "prefix code" - we can decode upon finishing the letter: don't have to look farther forward

# 2.5 Prefix codes

- Terminology: $a_1\dots a_n\in \mathcal{A}_{0}$ is a "prefix" of $b_1\dots b_m$ if $n\leq m$ and $a_1\dots a_n = b_1\dots b_n$
- Definition: $C$ is a "prefix code", aka prefix-free aka instantaneous if no codeword is a prefix of another codeword.
- Corollary: any prefix code is uniquely decodable
- can also think of a code as a tree
  - start at a root node, then a child for each letter option, a leaf for options
  - a prefix code is such that the path to a code word node doesn't pass through another code word node.
- there's also a concept of right-prefix and left-prefix codes.
  - left-prefix and right-prefix codes are both uniquely decodable.

# 2.6 Prefix codes - remarks and what's next

- Prefix codes are decodable in linear time.
- As we will see, prefix codes are also efficient
- Kraft-McMillan inequality will allow us to focus on only prefix codes
  - tells us that for any uniquely decodable code, there's a prefix code that's "just as good"

# 2.7 Expected codeword length

- measure of compression
- we want to make the encoded message "short on average"
- let $X \in \mathcal{X}$ be a discrete random variable with probability density function $p$.
  - $\mathbb{P}(X = x) = p(x)$
  - words in $X$ are iid
- Notation: if $\alpha = \alpha_1\dots\alpha_k\in\mathcal{A}_{0}$ with $\alpha_i\in\mathcal{A}$, then $|\alpha| = k$. Also denote $l(x) = |C(x)|$.
- Definition: The expected codeword length for $C$ for $\mathcal{X}$ is
  \[L = \sum_{x\in \mathcal{X}}l(x)p(x)\]

# 2.8 Kraft-McMillan Inequality

- Terminology: A $B$-ary code is a code $C:\mathcal{X} \rightarrow \mathcal{A}_{0}$ s.t. $|A| = B \geq 1$
  - e.g. the codes (A), (B), and (C) are 2-ary, e.g. Binary
- Theorem:
  - McMillan (a): For any uniquely decodable $B$-ary code $C$,
\[ \sum_{x\in\mathcal{X}} \frac{1}{B^{l(x)}} \leq 1 \text{ where }l(x) = |C(x)|\]
  - Kraft (b): if $l: \mathcal{X} \rightarrow \{0,1,2,\dots\}$ satisfies
\[ \sum_{x\in\mathcal{X}} \frac{1}{B^{l(x)}} \leq 1\]
    then there exists a $B$-ary prefix code $C$ s.t. $|C(x)| = l(x) \forall x\in \mathcal{X}$
- worked out examples with (A), (B), and (C)
- i.e. not all words can have short code words if want to be uniquely decodable
  - have to strike a balance between long and short code words

# 2.9: Proof of (a)

- Terminology: $x_{i:k} = x_1\dots x_k = (x_1,\dots,x_k)\in \mathcal{X}^k$
- Proof:
  - Case 1: $\mathcal{X}$ is finite.
    - Let $k,s \in \mathbb{Z}$ with $k > 0$, $s\geq 0$.
    - Consider the set $S = \{ x_{1:k}\in \mathcal{X}^k\text{ s.t. }\sum_{i=1}^kl(x_i)=s\}$
      - Claim that  $|S| \leq B^s$
        - $|C_{0}(x_{1:k})| = |C(x_1)\dots C(x_k)| = \sum l(x_i) = s$, $C_{0}(x_{1:k})\in \mathcal{A}^s$
        - $|\mathcal{A}^s| = B^s$
        - Thus the image of $S$ in $\mathcal{A}^s$ has size less than or equal to $B^s$.
        - $C_{0}$ is a 1-1 function, as $C$ is uniquely decodable
        - done
    - Consider $(\sum_{x\in\mathcal{X}} B^{-l(x)})^k = \sum_{x_1}\dots\sum_{x_k} B^{-l(x_1)}\dots B^{-l(x_k)} = \sum_{x_{1:k}\in \mathcal{X}^k} B^{-\sum l(x_i)}$
    - as $\mathcal{X}$ is finite, $\sum_{i=1}^k l(x_i) \leq kl_{max}$, where $l_{max} = max_{x \in \mathcal{X}} l(x)$
    - group by like terms by the sums of their lengths: 
\[\sum_{x_{1:k}\in \mathcal{X}^k} B^{-\sum l(x_i)} = \sum_{s = 0}^{kl_{max}} \sum_{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s} B^{-\sum l(x_i)} = \sum_{s = 0}^{kl_{max}} \sum_{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s} B^{-s} = \sum_{s = 0}^{kl_{max}} B^{-s} \sum_{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s} 1\]
\[= \sum_{s = 0}^{kl_{max}} B^{-s} |\{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s\}| \leq \sum_{s = 0}^{kl_{max}} B^{-s} B^s = kl_{max} + 1\]
    - let $r = sum_{x \in \mathcal{X}}B^{-l(x)}$
    - we've shown that $r^k \leq kl_{max}+1$
    - suppose that $r > 1$. then as we increase the value of $k$, $r^k$ goes to $\infty$ exponentially, while $kl_{max}+1$ goes only linearly. So at some point the inequality fails.
    - thus $r <= 1$
  - Case 2: $\mathcal{X}$ is countably infinite
    - let $\mathcal{X} = \{1,2,\dots\}$
    - $\sum_{x\in\mathcal{X}}B^{-l(x)} = lim_{n\rightarrow\infty}\sum_{x = 1}^n B^{-l(x)}$
    - let $\mathcal{X}' = \{1,\dots,n\}$. Then we can create a code $C'$ on the subset $\mathcal{X}'$.
    - Then the $r\leq 1$ for $C'$.
    - limit of things $\leq 1$ is itself $\leq 1$

# 2.10: Examples for (b)

- formal proof is a bit confusing, so easier to work out some examples
- Setup
  - Let $\mathcal{X} = \{x_1,x_2,\dots\}$, $l_i = l(x_i)$, $l = (l_1, l_2,\dots)$
  - Suppose $l_1 \leq l_2 \leq l_3 \dots$
  - $B$-ary expansions
    - e.g. $B=2$, then $0.1 = 1/2, 0.01 = 1/4, \dots$
    - normal decimals are $B=10$
- General idea: divide up the interval 0-1 into lengths given by $1/2^{l(x)}$, read off the $B$-ary expansions of the intervals -> take the partial sum of the expansion as the encoding, expanding to $i$ places for $l_i$
- Examples
  - (A) from before: $l = (1,2,3,3)$, $x_1 = a, x_2 = b, x_3 = c, x_4 = d$
    - we try to create a $2$-ary prefix code
    - divide $[0,1]$ into $1/2, 1/4, 1/8, 1/8$, i.e. $0.1, 0.01, 0.001, 0.001$
    - partial sums are $0, 1/2, 3/4, 7/8$, i.e. $0, 0.1, 0.11, 0.111$
    - take to $l_i$ places, get $0.0, 0.10, 0.110, 0.111$
    - the fractional part is the code: 0, 10, 110, 111

# 2.11: Proof sketch for (b)

- why does the procedure above always give us a prefix code?
  - Fact $\alpha_1\dots\alpha_j$ is a prefix of $\beta_1\dots\beta_m$ if and only if the $B$-ary number $0.\alpha_1\dots\alpha_j \leq 0.\beta_1\dots\beta_m < 0.\alpha_1\dots\alpha_j + 1/2^j$
    - i.e. $0.1\beta_1\dots\beta_m\in[0.\alpha_1\dots\alpha_j, 0.\alpha_1\dots\alpha_j + 1/2^j]$
  - So by doing this interval process and adding the $0$s for the padding, we make impossible for one to be a prefix of the other.

# 3.1: Entropy as a lower bound on expected length (part 1)

- $L = \sum_{x\in \mathcal{X}}l(x)p(x)$, $p$ is pmf for Discrete Memoryless Source, i.i.d.
- want to find a uniquely decodable code to minimize this expected length
- note that we can classify codes by their lengths for the purposes here.
  - Kraft-McMillan gives us which lengths are possible
  - So we want to minimize $L$ given the condition from Kraft-McMillan
- Notation
  - $p_i = p(x_i)$, $l_i = l(x_i)$, for $\mathcal{X} = \{x_1,\dots,\}$
  - assume $p_i > 0\forall i$
- Problem Statement: minimize $\sum_i l_ip_i$ given that $\sum_i 1/B^{l_i} \leq 1$
  - to simplify, assume that we allow $l_i\in\mathbb{R}\forall i$, which allows us to use calculus
  - constrained minimization problem: we'll use Lagrange Multipliers
- Suppose $l$ is such that $\sum_il_ip_i$ is minimal and $\sum_i1/B^{l_i} < 1$
  - take an $l_i$:
    - $1/B^{l_i} \geq 0$
    - if $l_i\leq0$, this summand is $\geq 1$. Thus if $i > 1$, then the constraint doesn't hold.
    - thus we assume that $l_i > 0 \forall i$
    - But now we can take an $l_i$ and reduce it a tiny bit (enough so that $\sum_i1/B^{l_i}$ is still less than 1, which violates the minimality of $\sum_il_ip_i$.
      - this is a contradiction, so we have that $\sum_i 1/B^{l_i} = 1$ if $\sum_il_ip_i$ is minimized.
- New Problem Statement: minimize $\sum_i l_ip_i$ given that $\sum_i 1/B^{l_i} = 1$
  - assume $B >1$ (for the log in the next statement)
  - also assume $i > 1$ I think
- let $q_i = 1/B^{l_i}$, i.e. $l_i = \log_B1/q_i$. Then 
  - the constraint turns into $\sum_iq_i = 1$
  - minimization turns into $\min_{q_i > 0} \sum p_i\log_B1/q_i$

# 3.2: Entropy as a lower bound on expected length (part 2)

- let $f(q) = \sum_i p_i \log_B 1/q_i$
  - let's see if it's convex: take the Hessian
  - note that $\log_Bx = (\ln x)/(\ln B)$, so that $\frac{d}{dx}\log_B\frac{1}{x} = \frac{-1}{x\ln B}$
  - $\frac{d}{dq_i} f = \frac{-p_i}{q_i\ln B}$
  - $\frac{d^2}{dq_idq_j} f = \frac{p_i}{q^2\ln B}$ if $i = j$ else 0
  - Thus the Hessian is diagonal with $\frac{p_i}{q^2\ln B}$ in the diagonal
    - eigenvalues are the diagonal elements, so are all positive.
    - Thus $f$ is strictly convex as the Hessian is positive definite
- Let's think about the constraint $\sum q_i = 1$
  - The set of $q_i$ that satisfy this are also a convex set
    - $\forall \alpha \in [0,1]$, then if $\sum q_i = 1$ and $\sum q_i' = 1$, then $\sum \alpha q_i + (1-\alpha)q_i\ = 1$, so all points in between are also in the set.
- So we're minimizing a convex function over a convex set
  - this is great, because the restriction of the convex function over the convex set will also be strictly convex
  - a critical point of the restricted function will be a global minimum for the whole convex set.

# 3.3: Entropy as a lower bound on expected length (part 3)

- We use Lagrange Multipliers to get critical points
  - $\mathcal{L}(q,\lambda) = \sum_i p_i\log(1/q_i) + \lambda (\sum_i q_i - 1)$
    - We solve $\nabla_q\mathcal{L} = 0$, $\nabla_\lambda\mathcal{L} = 0$ for $q$ and $\lambda$
      - second restraint reduces to $\sum_iq_i = 1$
      - first restraint: we calculated the derivative before of the first part of $\mathcal{L}$
        - so reduces to $0 = \frac{-p_i}{q_i\ln B} + \lambda$
      - $\frac{p_i}{\ln B} = \lambda q_i$
      - $\frac{1}{\ln B}\sum_i p_i = \lambda \sum_i q_i$
        - we have $\sum_i p_i = 1$ by definition, $\sum_i q_i = 1$ by the constraing
        - $\frac{1}{\ln B} = \lambda$
      - substitute for $1/\ln B$, get $q_i = p_i$ for all $i$
  - so the critical points of the constrained problem are given by $q_i = p_i$
    - remember $l_i = \log_B(1/q_i)$, so $l_i = \log_B (1/p_i)$ are the lengths that minimize the expected length
    - Thus the minimimum expected length is simply the information entropy
      \[\sum_i l_i p_i = \sum_i p_i \log_B(1/p_i)\]
- let's look at the solution we found:
  - $p_i$ are all positive and less than 1, so the optimal lengths are non-negative
  - The optimal lengths are not necessarily integers
    - we've found a lower bound (i.e. integer solution is at best this minimum expected length)
- final notation remark: if $B$ is very often $2$, in which case the entropy is
  \[-\sum_i p_i \log_2 p_i\]

# 3.4: Remark - an alternate proof

- can be done using "relative entropy" to get a quick proof, but the proof isn't so intuitive

# 3.5: Bounds on the optimal expected length

- $H_B(p)$ is a lower bound on the expected code length $L$.
- In fact, we can achieve a code $L$ such that $H_B(p)\leq L\leq H_B(p) + 1$
  - remember that the optimal real valued $l_i$ were $\log_B(1/p_i)$ (we assume $p_i > 0$)
    - and for these optimal values $\sum_i 1/B^{l_i} = 1$
    - but these are not necessarily integers
    - suppose we round them - the Kraft-McMillan constraint might not necessarily hold again
    - let's try $l_i = ceil(\log_B(1/p_i) < log_B(1/p_i) + 1$
      - as all the $l_i$ are $\geq$ what they were before, the inequality still holds
      - so by Kraft-McMillan, there exists a uniquely decodable code with these lengths.
      - what's the expected code word length of this code?
        - $\log_B(1/p_i) \leq l_i < log_B(1/p_i) + 1$
        - $p_i\log_B(1/p_i) \leq p_il_i < p_ilog_B(1/p_i) + p_i$
        - $H_B(p) \leq \sum_i p_il_i < H_B(p) + 1$ as we sum over the $i$
- so just rounding $\log_B(1/p_i)$ up to the nearest integer is a code with expected length within one of the entropy $H_B(p)$
  - we call the prefix code thus obtained "Shannon coding"
- Consider an optimal code, then $H_B(p) \leq L_{Optimal} \leq L_{Shannon} < H_B(p) + 1$
  - Later, we'll see an optimal symbol code (Huffman)
- $+1$ is up to one extra bit per source symbol (if $B=2$)
  - so not that great in practice perhaps
  - we'll see better approaches later (Block Coding, Source Coding Theorem)

# 3.6: Example - entropy as a lower bound

- Look at (A) from before: symbols $\{a,b,c,d\}$
  - Definition
    - code: $0,10,110,111$
    - lengths: $1,2,3,3$
    - $p(x)$: $1/2,1/4,1/8,1/8$
  - Expected Length is $1.75$
  - Entropy is $H_2(p) = \sum_i p_i\log_2(1/p(x)) = \frac{1}{2}\log_2(2) + \frac{1}{4}\log_2(4)\dots = 1/2 + 1/2 + 3/8 + 3/8 = 1.75$
  - we've achieved the lower bound for the expected code word length
    - which makes sense as the ideal lengths are $\log_2(1/p_i)$, which corresponds to the code.

# 3.7: Block codes for compression

- Symbol code $x_1\dots x_n \rightarrow C(x_1)\dots C(x_n)$
  - code word has to have an integer value lenth
  - $H \leq L \leq H + 1$ as expected code word length
    - so can have as much as one extra bit per word, so $n$ bits for $x_1\dots x_n$
- Block codes
  - spread out the one bit of inefficiency among a number of the $x_i$
    - e.g. if spread out among 3 symbols, then spread out the one bit among the 3 symbols
  - until now, $C:\mathcal{X}\rightarrow \mathcal{A}_{0}$, with $\mathcal{X}$ distributed ot $p$. Discrete memoryless source: iid according to $p$. Calculated expected length $L$
  - Now, we encode in blocks: given $k$
    - $C:\mathcal{X}^k\rightarrow A_{0}$
    - $x_{1:k} = x_1\dots x_k$ distributed according to $p^k$: iid, with each generated by $p$
    - $L^{(k)} = \sum_{x_{1:k}}l^k(x_{1:k})p^k(x_{1:k})$
  - we can kind of "change of variables" with $\mathcal{X}' = \mathcal{X}^k$, $p' = p^k$, and have the same situation as before
  - But now we can be more efficient:
    - look at the expected encoding length per source symbol $L_k:= L^{(k)}/k$
    - $H_B(p^k) \leq L^{(k)} < H_B(p^k)+1$

# 3.8: Entropy of iid random variables

- What is $H_B(p^k)$?
- $p^k(x_{1:k}) = p(x_1)\dots p(x_k)$
- Theorem: $H_B(p^k) = kH_B(p)$
  - $H_B(p^k) = \sum_{x_{1:k}} p^k(x_{1:k})\log_B(1/p^k(x_{1:k}))$
  - $\log_B(1/p^k(x_{1:k})) = \log 1/(p(x_1)\dots p(x_n)) = \sum_1^k \log(1/p(x_i))$
  - so $H_B(p^k) = \sum_{i = 1}^k\sum_{x_{1:k}}p^k(x_{1:k})\log_B(1/p(x_i))$
  - note that $\sum_{x_1:k} = \sum_{x_1}\sum_{x_2}\dots\sum_{x_k}$, so we have
  - $H_B(p^k) = \sum_i \sum_{x_i}\log_B(1/p(x_i))\sum_{x_j, j \neq i}p^k(x_{1:k})$
  - Note that $\sum_{x_j, j\neq i}p^k(x_{1:k}) = p(x_i)$
  - $H_B(p^k) = \sum_{i=1}^k\sum_{x_i\in \mathcal{X}}p(x_i)\log_B(1/p(x_i)) = \sum_{i = 1}^k H_B(p) = kH_B(p)$

# 3.9: Source Coding Theorem

- Three big theorems of Information Theory
  - Source Coding theorem: Establishes the best possible compression that one can achieve
  - Channel Coding theorem: given a noisy channel, the best possible rate at which one can communicate and still achieve arbitrarily low error
  - Rate Distortion theorem: given a noisy channel and acceptable level of distortion, the best possible rate that one can communicate with distortion under that level.
- Notation:
  - $C:\mathcal{X}^k \rightarrow A_{0}$
  - $L^{(k)}$ is the expected length for encoding of $k$ symbols
  - $L_k = L^{(k)}/k$: the expected length per source symbol
- Note that if $|\mathcal{X}|=|A|$, then $L_k$ is the compression ratio of the code
- Theorem (Shannon): Let $X_1, X_2\dots$ be a discrete memoryless source given by a iid probability distribution $p$, $X_i$ is countable
  - (a): "you can't beat the entropy": For any uniquely decodable block code $C:\mathcal{X}^k\rightarrow A_{0}$, $H_B(p)\leq L_k$
  - (b): "you can get as close as you wish": $\forall \epsilon > 0$, $\exists N$ s.t. $\forall k > N$ there is a prefix block code $C:\mathcal{X}^k\rightarrow A_{0}$ s.t. $L_k < H(p) + \epsilon$
- Proof:
  - (a): Let $C$ be such a block code, we've shown that $L^{(k)} \geq H_B(p^k)$. Thus $L_k \geq H_B(p)$ as desired
  - (b): Let $\epsilon > 0$ and consider $k > 0$. Then from our earlier results, we can make a prefix code $C$ such that $L^{(k)} < H(p^k)+1$. Substituting, this is $L_k < H(p) + 1/k$. So as $k$ grows, we can get arbitrarily close (i.e. $N > 1/\epsilon$).
- This is mainly theoretical, we need to have a code word for every sequence $x_1\dots x_n$, which gets computationally impractical as $k$ grows

# 3.10: Relative Entropy as the mismatch inefficiency

- To do shannon coding, we need to know the true probabilities $p_i$ so that we can assign lengths $ceil(\log_B (1/p+i))$.
  - in practice, we don't know the true $p_i$
- Notation: $q_i$ as our estimate for $p_i$, then define lengths using these
  - $l_i = ceil(\log_B(1/q_i)) = \log_B(1/q_i) + r_i$ with $r_i\in [0,1)$
- How much does expected code length increase?
  - $L = \sum l_ip_i = \sum p_i \log (1/q_i) + \sum r_i p_i$
  - let $R = \sum r_i p_i \geq 0$
  - $\sum p_i \log (1/q_i)$ is minimized when $q_i = p_i$
    - $\sum p_i \log (1/q_i) = \sum p_i \log (p_i/q_i) + \sum p_i \log(1/p_i) = \sum p_i \log(p_i/q_i) + H(p)$
    - $\sum p_i \log_B(p_i/q_i)$ is known as the relative entropy $D(p||q)$ or the Kullback-Leibler divergence
  - Thus, the increase in code length is $KL(p||q) + R$
    - $R$ is the "rounding inefficiency"
    - $KL(p||q)$ is the mismatch inefficiency
- Note that $H(p)\leq \sum p_i \log (1/q_i) = H(p) + KL(p||q)$
  - thus $KL(p||q) \geq 0$
- Also note that $KL(p||q) \neq KL(q||p)$, but $KL$ is intuitively thought of as the "distance" between $p$ and $q$, although it isn't really a true distance.

# 4.1: Huffman coding - introduction and example

- Problem: Design an algorithm taking a pmf $p = (p_1,\dots,p_n)$ and producing an optimal code $C$, where $C$ is optimal w.r.t. minimum length.
- Shannon-Fano coding: proposed, but not optimal
- Huffman: student of Fano, invents Huffman coding as a class assignment
  - Huffman coding is quite simple to implement
- Example (a): $p(x) = (0.35, 0.2, 0.2, 0.15, 0.1)$
  - take the two smallest probabilities $(0.15, 0.1)$. The sum of probabilities is 0.25
  - Now take the two smallest probabilities, replacing 0.15 and 0.1 with 0.25
  - continue, building a tree until we get 1.0
  - now label each line in the tree with a symbol from the code alphabet.
    - e.g. binary, so either 0 or 1
  - assign the code word to a leaf in the tree by following the path from 1 to the leaf.
    - this is always a prefix code by construction
  - expected length is 2.25, which is equal to the entropy $H_2(p) = 2.2016$
    - this is the best possible $L$ due to rounding inefficiency

# 4.2: Huffman coding - more examples

- Example (b): $p(x) = (0.3, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1)$
  - do the same process as before, with $B=2$ and $A = \{0,1\}$
  - Expected length is 2.7 bits, $H_2(p) = 2.6464$ bits
  - again, pretty close
  - note that the code is non-unique
    - there isn't a uniquely optimal code: e.g. if $B=2$, as we can swap 0 with 1 in all code words and we get another optimal code.
    - There can also be more than one Huffman code, as we can make different choices in building our tree
      - build a different tree, get a different code
      - the set of lengths of this new code are also different
      - the expected length is still 2.7, as apparently all Huffman codes are optimal (!)

# 4.3: $B$-ary Huffman codes

- Take the same probabilities as (a): $(0.35, 0.2, 0.2, 0.15, 0.1)$
  - suppose $B=3$
  - take 3 minimal probabilities, do the same process
  - get an optimal 3-ary code
  - Expected Length is 1.45 "trits", Entropy is $H_3(p) = 1.3891$ trits
  - larger $B$, can get shorter length: trits vs bits
- Example (d): let $p(x) = (0.35, 0.2, 0.15, 0.1, 0.1, 0.1)$
  - do the same process
  - after 2 steps, we only have 2 symbols left, which is a problem for the algorithm
  - so we pad the probabilities with a 0, and now it works
- (for a $B$-ary code, we pad the length to be $k(B-1)+1$ for some $k$ for the algorithm to work)
      
# TODO
- 4.1 - 4.5: Huffman 1: 6 +12 = 56min
- 4.6 - 4.9: Optimality 1: 15 + 22 + 8 + 18 = 1h 3 min
- 4.10 - 4.13: Optimality 2: 22 + 14 + 24 + 9 = 1h 9min
- 5.1 - 5.3: Arithmetic intro: 7 + 29 + 27 = 1h 4min
- 5.4 - 5.5: misc: 25 + 23 = 48min
- 5.6 - 5.7: encoder/decoder: 16 + 20 = 36min
- 5.8: optimality: 32min
- 5.9 - 5.10: misc: 19 + 24 = 43min
- 5.11 - 5.13: Finite-precision: 32 + 8 + 18 = 58min
