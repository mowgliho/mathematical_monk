---
title: "Information Theory"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Outline of Topics

- efficient (i.e. compression/source coding) and reliable (i.e. error correction/channel coding) transfer of data from a source to a destination
- examples
  - compression of images, videos, text files, audio files
  - RAM, space stuff, bar codes, tv, dsl,
  - etc.
- axis 1: efficiency vs reliablity
  - efficiency: compression; source coding
  - reliability: error correction; channel coding
- axis 2: math vs algorithms
  - math: Information Theory
  - algorithms: Coding methods
- topics:
  - compression & information theory:
    - Lossless compression
      - source coding theory (Shannon)
      - Kraft-Mcmillan inequality
    - Lossy compression
      - Rate-Distortion theorem
  - error-correction & information theory
    - (noisy) channel coding theorem
    - Channel capacity
    - Typicality & Asymptotic Equipartition Property
  - compression & coding methods
    - Symbol codes
      - Huffman 
    - Stream codes
      - Arithmetic coding
      - Lempel-Zim
  - error-correction & coding methods
    - Hamming 
    - BCH 
    - Reed-Solomon
    - Turbo codes
    - Gallager codes (LDPC)
- closely related fields
  - cryptography/cryptoanalysis
  - algorithmic information theory
  - kolmogorov complexity
  - minimum description length
  - network information theory
- also related
  - statistics
  - machine learning
  - portfolio theory/gambling

# 1.2: Applications of Compression Codes/Source coding

- focus on lossless compression algorithms
- Applications
  - Huffman
    - in some sense, an optimal compression algorithm (among "symbol codes")
    - simple to understand, implement
    - efficient in smallish applications
    - png, jpg, mpeg, winzip, gzip, mp3, aac
  - Arithmetic coding
    - sometimes, a symbol code is not the best to use (so not Hoffman)
    - scales better than Huffman
    - had IP and computation issues, but these are better now
    - jpg, jbig, mpeg, skype, flash, ppm, paq, DjVu
  - Lempel-Ziv
    - was shipped was unix
    - is asymptotically optimal for "universal codes"
    - efficient and simple to implement, useful for files generally on computers
    - png, gif, pkzip, gzip, pdf

# 1.3: Applications of Error Correction Codes (ECC)/Channel Coding

- Hamming (1950)
  - first reasonably good error correction code
  - are better ones now
  - still used in
    - DRAM (dynamic memory): reading and writing from chips is not error-free: due to magnetic fluctuations etc.
    - Static memory: RAID 2
- Reed-Solomon (1960)
  - uses abstract algebra, RSV (V is viterbi, RS is Reed-Solomon)
  - robust to burst errors (lots of errors together) - good for scratches on CDs
  - Bar codes, CD, DVD, Blu-Ray, DSL, RAID6, DVB
  - Space Missions: Voyager 1/2 (1977), Galileo (1989), Cassini (1997), Mars Pathfinder (1996), MER (2003)
- Turbocodes (1993)
  - much closer to the "Shannon limit": State of the Art
  - 3G, 4G, LTE, MedioFLO, WiMax: what enables you to get data at such a high rate
  - Space Missions: Mars Reconnaisance Orbiter
- Gallager (Low Density Parity Check) 1960
  - Written in Gallager's PhD Thesis, but not good for the computers at the time
  - Rediscovered in 1995, also State of the Art
  - Ethernet, WiFi 802.11n, Internet over power lines (ITU-T Ghn), "Smart Grid", Satellite TV (DVB-S2), CMMB (China MultiMedia Broadcasting), DTMB

# 1.4: Source-channel separation

- Overview
  - Source: emits message from some set (usually have little control)
  - Encoder: transforms message and sends to channel: we can design the encoder
  - Channel: typically noisy
  - Decoder: takes output of channel and tries to transform back
  - Destination
- can be limited to sending messages at a certain rate
  - want to send 
    - as few bits as possible
    - lowest chance of having an error
- compression and error-correction are in the encoder and decoder
- Forward Error Correction: e.g. Decoder can't ask Encoder to resend something
  - unlike some internet protocols.
- Source-channel Separation Theorem (Shannon):
  - This process can be rewritten as
    - Source
    - Source Encoder (Compression)
    - Channel Encoder (Error Correction encoding)
    - Noisy Channel
    - Channel Decoder (error correction decoding)
    - Source Decoder (decompression)
    - Destination
  - I.e. you can design the source and channel encoder separately and do just as well
    - doesn't say anything about computational complexity involved - could design together in theory with equal performance and less computational complexity.
  - holds for all sources that satisfy the "Acquisition Equipartition Property"

# 1.5: Examples of source-encoder-channel pipelines

- Pale Blue Dot image from Voyager
  - Image of earth on vehicle (S) -> Difference Mapping (SE) -> RSV (CE) -> electromagnetic waves (C) -> RSV (CD) -> Difference Mapping (SD) -> image file on earth (D)
- Internet Connection
  - directed connected to internet/modem
  - no source encoder/decoder
  - Computer (S) -> Ethernet encoder (CE) -> Cat-5 cable (C) -> Ethernet decoder (CD) -> Router/modem (D)
- Rip a CD and Play it
  - source and destination can be same physical location
    - Hard disk is the channel
  - CD Reader (S) -> mp3 (SE) -> CRC & RSV (CE) -> Hard Disk (C) -> CRC & RSV (CD) -> mp3 (SD) -> human ears (D)
- Skype over 4G
  - can chain bits together
  - Webcam (S) -> mpeg:SILK (SE) -> 4G encoding: turbocodes (CE) -> electromagnetic waves (C) -> 4G (CD) -> internet encoder (CE) -> internet (C) -> internet decoder (CD) -> mpeg decoder (SD) -> your friend

# 1.6: A different notion of "information"

- The information-theoretic notion of "information"
  - mathematical formulation vs everyday notion:
    - everyday notion: wikipedia article has more information than image of static
    - if judge by number of bits to encode/entropy, then it's the other way around
    - static is much more random/unpredictable
    - no concept of "utility" in information theory
    - information is exhibited through randomnmess: i.e. how unpredictable it is
  - we're trying to faithfully transmit messages that are sent by humans/computeres/etc
    - so the source is useful from the start
    - or the real-world usefulness of a message is an assumption that we don't bother with
    - what if we're doing lossy compression though?
      - then we make judgements on what parts of a message are "unimportant"
      - measure "utility" via distortion theory/function
        - i.e. accept a particular amount of distortion, try to send a minimal amount of information
