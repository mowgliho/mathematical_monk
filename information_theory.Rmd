---
title: "Information Theory"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Outline of Topics

- efficient (i.e. compression/source coding) and reliable (i.e. error correction/channel coding) transfer of data from a source to a destination
- examples
  - compression of images, videos, text files, audio files
  - RAM, space stuff, bar codes, tv, dsl,
  - etc.
- axis 1: efficiency vs reliablity
  - efficiency: compression; source coding
  - reliability: error correction; channel coding
- axis 2: math vs algorithms
  - math: Information Theory
  - algorithms: Coding methods
- topics:
  - compression & information theory:
    - Lossless compression
      - source coding theory (Shannon)
      - Kraft-Mcmillan inequality
    - Lossy compression
      - Rate-Distortion theorem
  - error-correction & information theory
    - (noisy) channel coding theorem
    - Channel capacity
    - Typicality & Asymptotic Equipartition Property
  - compression & coding methods
    - Symbol codes
      - Huffman 
    - Stream codes
      - Arithmetic coding
      - Lempel-Zim
  - error-correction & coding methods
    - Hamming 
    - BCH 
    - Reed-Solomon
    - Turbo codes
    - Gallager codes (LDPC)
- closely related fields
  - cryptography/cryptoanalysis
  - algorithmic information theory
  - kolmogorov complexity
  - minimum description length
  - network information theory
- also related
  - statistics
  - machine learning
  - portfolio theory/gambling

# 1.2: Applications of Compression Codes/Source coding

- focus on lossless compression algorithms
- Applications
  - Huffman
    - in some sense, an optimal compression algorithm (among "symbol codes")
    - simple to understand, implement
    - efficient in smallish applications
    - png, jpg, mpeg, winzip, gzip, mp3, aac
  - Arithmetic coding
    - sometimes, a symbol code is not the best to use (so not Hoffman)
    - scales better than Huffman
    - had IP and computation issues, but these are better now
    - jpg, jbig, mpeg, skype, flash, ppm, paq, DjVu
  - Lempel-Ziv
    - was shipped was unix
    - is asymptotically optimal for "universal codes"
    - efficient and simple to implement, useful for files generally on computers
    - png, gif, pkzip, gzip, pdf

# 1.3: Applications of Error Correction Codes (ECC)/Channel Coding

- Hamming (1950)
  - first reasonably good error correction code
  - are better ones now
  - still used in
    - DRAM (dynamic memory): reading and writing from chips is not error-free: due to magnetic fluctuations etc.
    - Static memory: RAID 2
- Reed-Solomon (1960)
  - uses abstract algebra, RSV (V is viterbi, RS is Reed-Solomon)
  - robust to burst errors (lots of errors together) - good for scratches on CDs
  - Bar codes, CD, DVD, Blu-Ray, DSL, RAID6, DVB
  - Space Missions: Voyager 1/2 (1977), Galileo (1989), Cassini (1997), Mars Pathfinder (1996), MER (2003)
- Turbocodes (1993)
  - much closer to the "Shannon limit": State of the Art
  - 3G, 4G, LTE, MedioFLO, WiMax: what enables you to get data at such a high rate
  - Space Missions: Mars Reconnaisance Orbiter
- Gallager (Low Density Parity Check) 1960
  - Written in Gallager's PhD Thesis, but not good for the computers at the time
  - Rediscovered in 1995, also State of the Art
  - Ethernet, WiFi 802.11n, Internet over power lines (ITU-T Ghn), "Smart Grid", Satellite TV (DVB-S2), CMMB (China MultiMedia Broadcasting), DTMB

# 1.4: Source-channel separation

- Overview
  - Source: emits message from some set (usually have little control)
  - Encoder: transforms message and sends to channel: we can design the encoder
  - Channel: typically noisy
  - Decoder: takes output of channel and tries to transform back
  - Destination
- can be limited to sending messages at a certain rate
  - want to send 
    - as few bits as possible
    - lowest chance of having an error
- compression and error-correction are in the encoder and decoder
- Forward Error Correction: e.g. Decoder can't ask Encoder to resend something
  - unlike some internet protocols.
- Source-channel Separation Theorem (Shannon):
  - This process can be rewritten as
    - Source
    - Source Encoder (Compression)
    - Channel Encoder (Error Correction encoding)
    - Noisy Channel
    - Channel Decoder (error correction decoding)
    - Source Decoder (decompression)
    - Destination
  - I.e. you can design the source and channel encoder separately and do just as well
    - doesn't say anything about computational complexity involved - could design together in theory with equal performance and less computational complexity.
  - holds for all sources that satisfy the "Acquisition Equipartition Property"

# 1.5: Examples of source-encoder-channel pipelines

- Pale Blue Dot image from Voyager
  - Image of earth on vehicle (S) -> Difference Mapping (SE) -> RSV (CE) -> electromagnetic waves (C) -> RSV (CD) -> Difference Mapping (SD) -> image file on earth (D)
- Internet Connection
  - directed connected to internet/modem
  - no source encoder/decoder
  - Computer (S) -> Ethernet encoder (CE) -> Cat-5 cable (C) -> Ethernet decoder (CD) -> Router/modem (D)
- Rip a CD and Play it
  - source and destination can be same physical location
    - Hard disk is the channel
  - CD Reader (S) -> mp3 (SE) -> CRC & RSV (CE) -> Hard Disk (C) -> CRC & RSV (CD) -> mp3 (SD) -> human ears (D)
- Skype over 4G
  - can chain bits together
  - Webcam (S) -> mpeg:SILK (SE) -> 4G encoding: turbocodes (CE) -> electromagnetic waves (C) -> 4G (CD) -> internet encoder (CE) -> internet (C) -> internet decoder (CD) -> mpeg decoder (SD) -> your friend

# 1.6: A different notion of "information"

- The information-theoretic notion of "information"
  - mathematical formulation vs everyday notion:
    - everyday notion: wikipedia article has more information than image of static
    - if judge by number of bits to encode/entropy, then it's the other way around
    - static is much more random/unpredictable
    - no concept of "utility" in information theory
    - information is exhibited through randomnmess: i.e. how unpredictable it is
  - we're trying to faithfully transmit messages that are sent by humans/computeres/etc
    - so the source is useful from the start
    - or the real-world usefulness of a message is an assumption that we don't bother with
    - what if we're doing lossy compression though?
      - then we make judgements on what parts of a message are "unimportant"
      - measure "utility" via distortion theory/function
        - i.e. accept a particular amount of distortion, try to send a minimal amount of information

# 2.1: A puzzle on weighing coins

- 12 coins, either all the same weight, or one has a different weight
- have a scale
- 3 weighings to figure out if one is different, and if so which one and if heavier or lighter
- can use basic ideas from information theory to simplify problems of this sort

# 2.2: Symbol codes - terminology and notation

- 1830s, telegraph technology
- Samuel Morse and Alfed Veil invent Morse Code
  - text messages over the telegram
  - can be seen as sequence of 0s/1s
- symbol code/variable length code
  - each symbol has different lengths
  - morse code can be seen as each letter goes to a binary code word
    - more frequent letters have shorter codes
    - i.e. lossless message compression using a probabilistic model
      - where letters are assumed to follow a iid probability
- terms
  - source: sequence of random variables $x_1,\dots,x_n$
  - memoryless: iid
  - Discrete memoryless source: $x_1,\dots,x_n \in \mathcal{X}$, where $\mathcal{X}$ is countable.
  - alphabet: set of elements
    - generally a source alphabet $\mathcal{X}$ and a code alphabet $\mathcal{A}$
    - morse code: $\mathcal{X} = \{A,B,C...,0\}$, $\mathcal{A} = \{0,1\}$
  - $\mathcal{A}_0 = \{a_1,\dots,a_k: k >= 0, a_i\in \mathcal{A} \forall i\}$, i.e. all finite length sequences of letter in $\mathcal{A}$

# 2.3: Symbol codes - definition and examples

- Symbol Code: (Variable length code) is a function $C: \mathcal{X} \rightarrow \mathcal{A}_{0}$.
- The extension of $C$: the function $C_{0}: \mathcal{X}_{0} \rightarrow \mathcal{A}_{0}$ such that $\forall n\geq 0, \forall x_i \in \mathcal{X}, C_{0} (x_1\dots x_n) = C(x_1)\dots C(x_n)$
  - i.e. image of the concatenation is the concatenation of the images
    - sequence of letters -> sequence of code words
- Codewords of $C$: The sequences $a_1\dots a_k \in \mathcal{A}_0$ such that $C(x) = a_1\dots a_k$ for some $x \in \mathcal{X}$
- Examples
  - Morse Code
    - $\mathcal{X} = \{A,B,\dots,0\}$
    - $\mathcal{A} = \{0,1,\text{pause}\}$
    - $C$ is the mapping
  - Toy example (A)
    - $\mathcal{X} = \{a,b,c,d\}$, $\mathcal{A} = \{0,1\}$, $X$ is $a$, $b$, $c$, and $d$ with probability $1/2, 1/4, 1/8, 1/8$.
    - $C(a) = 0, C(b) = 10, C(c) = 110, C(d) = 111$
    - e.g. $C_{0}(aacb) = 0011010 \in \mathcal{A}_{0}, aacb \in \mathcal{X}_{0}$
  - Other toy examples
    - (B) same as before, but $C(a) = 101, C(b) = 00, C(c) = 0001, C(d) = 1$
    - (C) same as before, but $C(a) = 0, C(b) = 1, C(c) = 01, C(d) = 10$
      - is a valid code, but $C_{0}$ is non-injective (or not uniquely decodeable): $C_{0}(ab) = C_{0}(c)$

# 2.4 Decoding - prefix vs. non-prefix

- Definition: $C$ is "uniquely decodable" if $C_{0}$ is injective.
  - necessary for lossless compression
- Desirable properties for compression
  - efficiency
  - computational speed
  - simplicity
- Computational speed
  - encoding: if $\mathcal{X}$ is finite, then is easy (lookup table)
  - decoding: in some examples, (A) is easier to decode than (B) as you have to look farther ahead to decode.
- (A) is an example of a "prefix code" - we can decode upon finishing the letter: don't have to look farther forward

# 2.5 Prefix codes

- Terminology: $a_1\dots a_n\in \mathcal{A}_{0}$ is a "prefix" of $b_1\dots b_m$ if $n\leq m$ and $a_1\dots a_n = b_1\dots b_n$
- Definition: $C$ is a "prefix code", aka prefix-free aka instantaneous if no codeword is a prefix of another codeword.
- Corollary: any prefix code is uniquely decodable
- can also think of a code as a tree
  - start at a root node, then a child for each letter option, a leaf for options
  - a prefix code is such that the path to a code word node doesn't pass through another code word node.
- there's also a concept of right-prefix and left-prefix codes.
  - left-prefix and right-prefix codes are both uniquely decodable.

# 2.6 Prefix codes - remarks and what's next

- Prefix codes are decodable in linear time.
- As we will see, prefix codes are also efficient
- Kraft-McMillan inequality will allow us to focus on only prefix codes
  - tells us that for any uniquely decodable code, there's a prefix code that's "just as good"

# 2.7 Expected codeword length

- measure of compression
- we want to make the encoded message "short on average"
- let $X \in \mathcal{X}$ be a discrete random variable with probability density function $p$.
  - $\mathbb{P}(X = x) = p(x)$
  - words in $X$ are iid
- Notation: if $\alpha = \alpha_1\dots\alpha_k\in\mathcal{A}_{0}$ with $\alpha_i\in\mathcal{A}$, then $|\alpha| = k$. Also denote $l(x) = |C(x)|$.
- Definition: The expected codeword length for $C$ for $\mathcal{X}$ is
  \[L = \sum_{x\in \mathcal{X}}l(x)p(x)\]

# TODO
- 2.8 - 2.11: Kraft-Mc-Millan: 14 + 18 + 21 + 12 = 1h 5min
- 3.1 - 3.4: Entropy as a lower bound: 15 + 13 + 18 + 2 = 48min
- 3.5 - 3.8: Misc: 14 + 4 + 13 + 8 = 40min
- 3.9 - 3.10: misc2: 16 + 16: 32min
- 4.1 - 4.5: Huffman 1: 12 + 14 + 12 + 6 +12 = 56min
- 4.6 - 4.9: Optimality 1: 15 + 22 + 8 + 18 = 1h 3 min
- 4.10 - 4.13: Optimality 2: 22 + 14 + 24 + 9 = 1h 9min
- 5.1 - 5.3: Arithmetic intro: 7 + 29 + 27 = 1h 4min
- 5.4 - 5.5: misc: 25 + 23 = 48min
- 5.6 - 5.7: encoder/decoder: 16 + 20 = 36min
- 5.8: optimality: 32min
- 5.9 - 5.10: misc: 19 + 24 = 43min
- 5.11 - 5.13: Finite-precision: 32 + 8 + 18 = 58min
