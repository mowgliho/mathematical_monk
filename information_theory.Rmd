---
title: "Information Theory"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Outline of Topics

- efficient (i.e. compression/source coding) and reliable (i.e. error correction/channel coding) transfer of data from a source to a destination
- examples
  - compression of images, videos, text files, audio files
  - RAM, space stuff, bar codes, tv, dsl,
  - etc.
- axis 1: efficiency vs reliablity
  - efficiency: compression; source coding
  - reliability: error correction; channel coding
- axis 2: math vs algorithms
  - math: Information Theory
  - algorithms: Coding methods
- topics:
  - compression & information theory:
    - Lossless compression
      - source coding theory (Shannon)
      - Kraft-Mcmillan inequality
    - Lossy compression
      - Rate-Distortion theorem
  - error-correction & information theory
    - (noisy) channel coding theorem
    - Channel capacity
    - Typicality & Asymptotic Equipartition Property
  - compression & coding methods
    - Symbol codes
      - Huffman 
    - Stream codes
      - Arithmetic coding
      - Lempel-Zim
  - error-correction & coding methods
    - Hamming 
    - BCH 
    - Reed-Solomon
    - Turbo codes
    - Gallager codes (LDPC)
- closely related fields
  - cryptography/cryptoanalysis
  - algorithmic information theory
  - kolmogorov complexity
  - minimum description length
  - network information theory
- also related
  - statistics
  - machine learning
  - portfolio theory/gambling

# 1.2: Applications of Compression Codes/Source coding

- focus on lossless compression algorithms
- Applications
  - Huffman
    - in some sense, an optimal compression algorithm (among "symbol codes")
    - simple to understand, implement
    - efficient in smallish applications
    - png, jpg, mpeg, winzip, gzip, mp3, aac
  - Arithmetic coding
    - sometimes, a symbol code is not the best to use (so not Hoffman)
    - scales better than Huffman
    - had IP and computation issues, but these are better now
    - jpg, jbig, mpeg, skype, flash, ppm, paq, DjVu
  - Lempel-Ziv
    - was shipped was unix
    - is asymptotically optimal for "universal codes"
    - efficient and simple to implement, useful for files generally on computers
    - png, gif, pkzip, gzip, pdf

# 1.3: Applications of Error Correction Codes (ECC)/Channel Coding

- Hamming (1950)
  - first reasonably good error correction code
  - are better ones now
  - still used in
    - DRAM (dynamic memory): reading and writing from chips is not error-free: due to magnetic fluctuations etc.
    - Static memory: RAID 2
- Reed-Solomon (1960)
  - uses abstract algebra, RSV (V is viterbi, RS is Reed-Solomon)
  - robust to burst errors (lots of errors together) - good for scratches on CDs
  - Bar codes, CD, DVD, Blu-Ray, DSL, RAID6, DVB
  - Space Missions: Voyager 1/2 (1977), Galileo (1989), Cassini (1997), Mars Pathfinder (1996), MER (2003)
- Turbocodes (1993)
  - much closer to the "Shannon limit": State of the Art
  - 3G, 4G, LTE, MedioFLO, WiMax: what enables you to get data at such a high rate
  - Space Missions: Mars Reconnaisance Orbiter
- Gallager (Low Density Parity Check) 1960
  - Written in Gallager's PhD Thesis, but not good for the computers at the time
  - Rediscovered in 1995, also State of the Art
  - Ethernet, WiFi 802.11n, Internet over power lines (ITU-T Ghn), "Smart Grid", Satellite TV (DVB-S2), CMMB (China MultiMedia Broadcasting), DTMB

# 1.4: Source-channel separation

- Overview
  - Source: emits message from some set (usually have little control)
  - Encoder: transforms message and sends to channel: we can design the encoder
  - Channel: typically noisy
  - Decoder: takes output of channel and tries to transform back
  - Destination
- can be limited to sending messages at a certain rate
  - want to send 
    - as few bits as possible
    - lowest chance of having an error
- compression and error-correction are in the encoder and decoder
- Forward Error Correction: e.g. Decoder can't ask Encoder to resend something
  - unlike some internet protocols.
- Source-channel Separation Theorem (Shannon):
  - This process can be rewritten as
    - Source
    - Source Encoder (Compression)
    - Channel Encoder (Error Correction encoding)
    - Noisy Channel
    - Channel Decoder (error correction decoding)
    - Source Decoder (decompression)
    - Destination
  - I.e. you can design the source and channel encoder separately and do just as well
    - doesn't say anything about computational complexity involved - could design together in theory with equal performance and less computational complexity.
  - holds for all sources that satisfy the "Acquisition Equipartition Property"

# 1.5: Examples of source-encoder-channel pipelines

- Pale Blue Dot image from Voyager
  - Image of earth on vehicle (S) -> Difference Mapping (SE) -> RSV (CE) -> electromagnetic waves (C) -> RSV (CD) -> Difference Mapping (SD) -> image file on earth (D)
- Internet Connection
  - directed connected to internet/modem
  - no source encoder/decoder
  - Computer (S) -> Ethernet encoder (CE) -> Cat-5 cable (C) -> Ethernet decoder (CD) -> Router/modem (D)
- Rip a CD and Play it
  - source and destination can be same physical location
    - Hard disk is the channel
  - CD Reader (S) -> mp3 (SE) -> CRC & RSV (CE) -> Hard Disk (C) -> CRC & RSV (CD) -> mp3 (SD) -> human ears (D)
- Skype over 4G
  - can chain bits together
  - Webcam (S) -> mpeg:SILK (SE) -> 4G encoding: turbocodes (CE) -> electromagnetic waves (C) -> 4G (CD) -> internet encoder (CE) -> internet (C) -> internet decoder (CD) -> mpeg decoder (SD) -> your friend

# 1.6: A different notion of "information"

- The information-theoretic notion of "information"
  - mathematical formulation vs everyday notion:
    - everyday notion: wikipedia article has more information than image of static
    - if judge by number of bits to encode/entropy, then it's the other way around
    - static is much more random/unpredictable
    - no concept of "utility" in information theory
    - information is exhibited through randomnmess: i.e. how unpredictable it is
  - we're trying to faithfully transmit messages that are sent by humans/computeres/etc
    - so the source is useful from the start
    - or the real-world usefulness of a message is an assumption that we don't bother with
    - what if we're doing lossy compression though?
      - then we make judgements on what parts of a message are "unimportant"
      - measure "utility" via distortion theory/function
        - i.e. accept a particular amount of distortion, try to send a minimal amount of information

# 2.1: A puzzle on weighing coins

- 12 coins, either all the same weight, or one has a different weight
- have a scale
- 3 weighings to figure out if one is different, and if so which one and if heavier or lighter
- can use basic ideas from information theory to simplify problems of this sort

# 2.2: Symbol codes - terminology and notation

- 1830s, telegraph technology
- Samuel Morse and Alfed Veil invent Morse Code
  - text messages over the telegram
  - can be seen as sequence of 0s/1s
- symbol code/variable length code
  - each symbol has different lengths
  - morse code can be seen as each letter goes to a binary code word
    - more frequent letters have shorter codes
    - i.e. lossless message compression using a probabilistic model
      - where letters are assumed to follow a iid probability
- terms
  - source: sequence of random variables $x_1,\dots,x_n$
  - memoryless: iid
  - Discrete memoryless source: $x_1,\dots,x_n \in \mathcal{X}$, where $\mathcal{X}$ is countable.
  - alphabet: set of elements
    - generally a source alphabet $\mathcal{X}$ and a code alphabet $\mathcal{A}$
    - morse code: $\mathcal{X} = \{A,B,C...,0\}$, $\mathcal{A} = \{0,1\}$
  - $\mathcal{A}_0 = \{a_1,\dots,a_k: k >= 0, a_i\in \mathcal{A} \forall i\}$, i.e. all finite length sequences of letter in $\mathcal{A}$

# 2.3: Symbol codes - definition and examples

- Symbol Code: (Variable length code) is a function $C: \mathcal{X} \rightarrow \mathcal{A}_{0}$.
- The extension of $C$: the function $C_{0}: \mathcal{X}_{0} \rightarrow \mathcal{A}_{0}$ such that $\forall n\geq 0, \forall x_i \in \mathcal{X}, C_{0} (x_1\dots x_n) = C(x_1)\dots C(x_n)$
  - i.e. image of the concatenation is the concatenation of the images
    - sequence of letters -> sequence of code words
- Codewords of $C$: The sequences $a_1\dots a_k \in \mathcal{A}_0$ such that $C(x) = a_1\dots a_k$ for some $x \in \mathcal{X}$
- Examples
  - Morse Code
    - $\mathcal{X} = \{A,B,\dots,0\}$
    - $\mathcal{A} = \{0,1,\text{pause}\}$
    - $C$ is the mapping
  - Toy example (A)
    - $\mathcal{X} = \{a,b,c,d\}$, $\mathcal{A} = \{0,1\}$, $X$ is $a$, $b$, $c$, and $d$ with probability $1/2, 1/4, 1/8, 1/8$.
    - $C(a) = 0, C(b) = 10, C(c) = 110, C(d) = 111$
    - e.g. $C_{0}(aacb) = 0011010 \in \mathcal{A}_{0}, aacb \in \mathcal{X}_{0}$
  - Other toy examples
    - (B) same as before, but $C(a) = 101, C(b) = 00, C(c) = 0001, C(d) = 1$
    - (C) same as before, but $C(a) = 0, C(b) = 1, C(c) = 01, C(d) = 10$
      - is a valid code, but $C_{0}$ is non-injective (or not uniquely decodeable): $C_{0}(ab) = C_{0}(c)$
