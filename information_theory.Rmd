---
title: "Information Theory"
output:
  html_document:
    toc: true
    toc_depth: 2
---

# 1.1: Outline of Topics

- efficient (i.e. compression/source coding) and reliable (i.e. error correction/channel coding) transfer of data from a source to a destination
- examples
  - compression of images, videos, text files, audio files
  - RAM, space stuff, bar codes, tv, dsl,
  - etc.
- axis 1: efficiency vs reliablity
  - efficiency: compression; source coding
  - reliability: error correction; channel coding
- axis 2: math vs algorithms
  - math: Information Theory
  - algorithms: Coding methods
- topics:
  - compression & information theory:
    - Lossless compression
      - source coding theory (Shannon)
      - Kraft-Mcmillan inequality
    - Lossy compression
      - Rate-Distortion theorem
  - error-correction & information theory
    - (noisy) channel coding theorem
    - Channel capacity
    - Typicality & Asymptotic Equipartition Property
  - compression & coding methods
    - Symbol codes
      - Huffman 
    - Stream codes
      - Arithmetic coding
      - Lempel-Zim
  - error-correction & coding methods
    - Hamming 
    - BCH 
    - Reed-Solomon
    - Turbo codes
    - Gallager codes (LDPC)
- closely related fields
  - cryptography/cryptoanalysis
  - algorithmic information theory
  - kolmogorov complexity
  - minimum description length
  - network information theory
- also related
  - statistics
  - machine learning
  - portfolio theory/gambling

# 1.2: Applications of Compression Codes/Source coding

- focus on lossless compression algorithms
- Applications
  - Huffman
    - in some sense, an optimal compression algorithm (among "symbol codes")
    - simple to understand, implement
    - efficient in smallish applications
    - png, jpg, mpeg, winzip, gzip, mp3, aac
  - Arithmetic coding
    - sometimes, a symbol code is not the best to use (so not Hoffman)
    - scales better than Huffman
    - had IP and computation issues, but these are better now
    - jpg, jbig, mpeg, skype, flash, ppm, paq, DjVu
  - Lempel-Ziv
    - was shipped was unix
    - is asymptotically optimal for "universal codes"
    - efficient and simple to implement, useful for files generally on computers
    - png, gif, pkzip, gzip, pdf

# 1.3: Applications of Error Correction Codes (ECC)/Channel Coding

- Hamming (1950)
  - first reasonably good error correction code
  - are better ones now
  - still used in
    - DRAM (dynamic memory): reading and writing from chips is not error-free: due to magnetic fluctuations etc.
    - Static memory: RAID 2
- Reed-Solomon (1960)
  - uses abstract algebra, RSV (V is viterbi, RS is Reed-Solomon)
  - robust to burst errors (lots of errors together) - good for scratches on CDs
  - Bar codes, CD, DVD, Blu-Ray, DSL, RAID6, DVB
  - Space Missions: Voyager 1/2 (1977), Galileo (1989), Cassini (1997), Mars Pathfinder (1996), MER (2003)
- Turbocodes (1993)
  - much closer to the "Shannon limit": State of the Art
  - 3G, 4G, LTE, MedioFLO, WiMax: what enables you to get data at such a high rate
  - Space Missions: Mars Reconnaisance Orbiter
- Gallager (Low Density Parity Check) 1960
  - Written in Gallager's PhD Thesis, but not good for the computers at the time
  - Rediscovered in 1995, also State of the Art
  - Ethernet, WiFi 802.11n, Internet over power lines (ITU-T Ghn), "Smart Grid", Satellite TV (DVB-S2), CMMB (China MultiMedia Broadcasting), DTMB

# 1.4: Source-channel separation

- Overview
  - Source: emits message from some set (usually have little control)
  - Encoder: transforms message and sends to channel: we can design the encoder
  - Channel: typically noisy
  - Decoder: takes output of channel and tries to transform back
  - Destination
- can be limited to sending messages at a certain rate
  - want to send 
    - as few bits as possible
    - lowest chance of having an error
- compression and error-correction are in the encoder and decoder
- Forward Error Correction: e.g. Decoder can't ask Encoder to resend something
  - unlike some internet protocols.
- Source-channel Separation Theorem (Shannon):
  - This process can be rewritten as
    - Source
    - Source Encoder (Compression)
    - Channel Encoder (Error Correction encoding)
    - Noisy Channel
    - Channel Decoder (error correction decoding)
    - Source Decoder (decompression)
    - Destination
  - I.e. you can design the source and channel encoder separately and do just as well
    - doesn't say anything about computational complexity involved - could design together in theory with equal performance and less computational complexity.
  - holds for all sources that satisfy the "Acquisition Equipartition Property"

# 1.5: Examples of source-encoder-channel pipelines

- Pale Blue Dot image from Voyager
  - Image of earth on vehicle (S) -> Difference Mapping (SE) -> RSV (CE) -> electromagnetic waves (C) -> RSV (CD) -> Difference Mapping (SD) -> image file on earth (D)
- Internet Connection
  - directed connected to internet/modem
  - no source encoder/decoder
  - Computer (S) -> Ethernet encoder (CE) -> Cat-5 cable (C) -> Ethernet decoder (CD) -> Router/modem (D)
- Rip a CD and Play it
  - source and destination can be same physical location
    - Hard disk is the channel
  - CD Reader (S) -> mp3 (SE) -> CRC & RSV (CE) -> Hard Disk (C) -> CRC & RSV (CD) -> mp3 (SD) -> human ears (D)
- Skype over 4G
  - can chain bits together
  - Webcam (S) -> mpeg:SILK (SE) -> 4G encoding: turbocodes (CE) -> electromagnetic waves (C) -> 4G (CD) -> internet encoder (CE) -> internet (C) -> internet decoder (CD) -> mpeg decoder (SD) -> your friend

# 1.6: A different notion of "information"

- The information-theoretic notion of "information"
  - mathematical formulation vs everyday notion:
    - everyday notion: wikipedia article has more information than image of static
    - if judge by number of bits to encode/entropy, then it's the other way around
    - static is much more random/unpredictable
    - no concept of "utility" in information theory
    - information is exhibited through randomnmess: i.e. how unpredictable it is
  - we're trying to faithfully transmit messages that are sent by humans/computeres/etc
    - so the source is useful from the start
    - or the real-world usefulness of a message is an assumption that we don't bother with
    - what if we're doing lossy compression though?
      - then we make judgements on what parts of a message are "unimportant"
      - measure "utility" via distortion theory/function
        - i.e. accept a particular amount of distortion, try to send a minimal amount of information

# 2.1: A puzzle on weighing coins

- 12 coins, either all the same weight, or one has a different weight
- have a scale
- 3 weighings to figure out if one is different, and if so which one and if heavier or lighter
- can use basic ideas from information theory to simplify problems of this sort

# 2.2: Symbol codes - terminology and notation

- 1830s, telegraph technology
- Samuel Morse and Alfed Veil invent Morse Code
  - text messages over the telegram
  - can be seen as sequence of 0s/1s
- symbol code/variable length code
  - each symbol has different lengths
  - morse code can be seen as each letter goes to a binary code word
    - more frequent letters have shorter codes
    - i.e. lossless message compression using a probabilistic model
      - where letters are assumed to follow a iid probability
- terms
  - source: sequence of random variables $x_1,\dots,x_n$
  - memoryless: iid
  - Discrete memoryless source: $x_1,\dots,x_n \in \mathcal{X}$, where $\mathcal{X}$ is countable.
  - alphabet: set of elements
    - generally a source alphabet $\mathcal{X}$ and a code alphabet $\mathcal{A}$
    - morse code: $\mathcal{X} = \{A,B,C...,0\}$, $\mathcal{A} = \{0,1\}$
  - $\mathcal{A}_0 = \{a_1,\dots,a_k: k >= 0, a_i\in \mathcal{A} \forall i\}$, i.e. all finite length sequences of letter in $\mathcal{A}$

# 2.3: Symbol codes - definition and examples

- Symbol Code: (Variable length code) is a function $C: \mathcal{X} \rightarrow \mathcal{A}_{0}$.
- The extension of $C$: the function $C_{0}: \mathcal{X}_{0} \rightarrow \mathcal{A}_{0}$ such that $\forall n\geq 0, \forall x_i \in \mathcal{X}, C_{0} (x_1\dots x_n) = C(x_1)\dots C(x_n)$
  - i.e. image of the concatenation is the concatenation of the images
    - sequence of letters -> sequence of code words
- Codewords of $C$: The sequences $a_1\dots a_k \in \mathcal{A}_0$ such that $C(x) = a_1\dots a_k$ for some $x \in \mathcal{X}$
- Examples
  - Morse Code
    - $\mathcal{X} = \{A,B,\dots,0\}$
    - $\mathcal{A} = \{0,1,\text{pause}\}$
    - $C$ is the mapping
  - Toy example (A)
    - $\mathcal{X} = \{a,b,c,d\}$, $\mathcal{A} = \{0,1\}$, $X$ is $a$, $b$, $c$, and $d$ with probability $1/2, 1/4, 1/8, 1/8$.
    - $C(a) = 0, C(b) = 10, C(c) = 110, C(d) = 111$
    - e.g. $C_{0}(aacb) = 0011010 \in \mathcal{A}_{0}, aacb \in \mathcal{X}_{0}$
  - Other toy examples
    - (B) same as before, but $C(a) = 101, C(b) = 00, C(c) = 0001, C(d) = 1$
    - (C) same as before, but $C(a) = 0, C(b) = 1, C(c) = 01, C(d) = 10$
      - is a valid code, but $C_{0}$ is non-injective (or not uniquely decodeable): $C_{0}(ab) = C_{0}(c)$

# 2.4 Decoding - prefix vs. non-prefix

- Definition: $C$ is "uniquely decodable" if $C_{0}$ is injective.
  - necessary for lossless compression
- Desirable properties for compression
  - efficiency
  - computational speed
  - simplicity
- Computational speed
  - encoding: if $\mathcal{X}$ is finite, then is easy (lookup table)
  - decoding: in some examples, (A) is easier to decode than (B) as you have to look farther ahead to decode.
- (A) is an example of a "prefix code" - we can decode upon finishing the letter: don't have to look farther forward

# 2.5 Prefix codes

- Terminology: $a_1\dots a_n\in \mathcal{A}_{0}$ is a "prefix" of $b_1\dots b_m$ if $n\leq m$ and $a_1\dots a_n = b_1\dots b_n$
- Definition: $C$ is a "prefix code", aka prefix-free aka instantaneous if no codeword is a prefix of another codeword.
- Corollary: any prefix code is uniquely decodable
- can also think of a code as a tree
  - start at a root node, then a child for each letter option, a leaf for options
  - a prefix code is such that the path to a code word node doesn't pass through another code word node.
- there's also a concept of right-prefix and left-prefix codes.
  - left-prefix and right-prefix codes are both uniquely decodable.

# 2.6 Prefix codes - remarks and what's next

- Prefix codes are decodable in linear time.
- As we will see, prefix codes are also efficient
- Kraft-McMillan inequality will allow us to focus on only prefix codes
  - tells us that for any uniquely decodable code, there's a prefix code that's "just as good"

# 2.7 Expected codeword length

- measure of compression
- we want to make the encoded message "short on average"
- let $X \in \mathcal{X}$ be a discrete random variable with probability density function $p$.
  - $\mathbb{P}(X = x) = p(x)$
  - words in $X$ are iid
- Notation: if $\alpha = \alpha_1\dots\alpha_k\in\mathcal{A}_{0}$ with $\alpha_i\in\mathcal{A}$, then $|\alpha| = k$. Also denote $l(x) = |C(x)|$.
- Definition: The expected codeword length for $C$ for $\mathcal{X}$ is
  \[L = \sum_{x\in \mathcal{X}}l(x)p(x)\]

# 2.8 Kraft-McMillan Inequality

- Terminology: A $B$-ary code is a code $C:\mathcal{X} \rightarrow \mathcal{A}_{0}$ s.t. $|A| = B \geq 1$
  - e.g. the codes (A), (B), and (C) are 2-ary, e.g. Binary
- Theorem:
  - McMillan (a): For any uniquely decodable $B$-ary code $C$,
\[ \sum_{x\in\mathcal{X}} \frac{1}{B^{l(x)}} \leq 1 \text{ where }l(x) = |C(x)|\]
  - Kraft (b): if $l: \mathcal{X} \rightarrow \{0,1,2,\dots\}$ satisfies
\[ \sum_{x\in\mathcal{X}} \frac{1}{B^{l(x)}} \leq 1\]
    then there exists a $B$-ary prefix code $C$ s.t. $|C(x)| = l(x) \forall x\in \mathcal{X}$
- worked out examples with (A), (B), and (C)
- i.e. not all words can have short code words if want to be uniquely decodable
  - have to strike a balance between long and short code words

# 2.9: Proof of (a)

- Terminology: $x_{i:k} = x_1\dots x_k = (x_1,\dots,x_k)\in \mathcal{X}^k$
- Proof:
  - Case 1: $\mathcal{X}$ is finite.
    - Let $k,s \in \mathbb{Z}$ with $k > 0$, $s\geq 0$.
    - Consider the set $S = \{ x_{1:k}\in \mathcal{X}^k\text{ s.t. }\sum_{i=1}^kl(x_i)=s\}$
      - Claim that  $|S| \leq B^s$
        - $|C_{0}(x_{1:k})| = |C(x_1)\dots C(x_k)| = \sum l(x_i) = s$, $C_{0}(x_{1:k})\in \mathcal{A}^s$
        - $|\mathcal{A}^s| = B^s$
        - Thus the image of $S$ in $\mathcal{A}^s$ has size less than or equal to $B^s$.
        - $C_{0}$ is a 1-1 function, as $C$ is uniquely decodable
        - done
    - Consider $(\sum_{x\in\mathcal{X}} B^{-l(x)})^k = \sum_{x_1}\dots\sum_{x_k} B^{-l(x_1)}\dots B^{-l(x_k)} = \sum_{x_{1:k}\in \mathcal{X}^k} B^{-\sum l(x_i)}$
    - as $\mathcal{X}$ is finite, $\sum_{i=1}^k l(x_i) \leq kl_{max}$, where $l_{max} = max_{x \in \mathcal{X}} l(x)$
    - group by like terms by the sums of their lengths: 
\[\sum_{x_{1:k}\in \mathcal{X}^k} B^{-\sum l(x_i)} = \sum_{s = 0}^{kl_{max}} \sum_{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s} B^{-\sum l(x_i)} = \sum_{s = 0}^{kl_{max}} \sum_{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s} B^{-s} = \sum_{s = 0}^{kl_{max}} B^{-s} \sum_{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s} 1\]
\[= \sum_{s = 0}^{kl_{max}} B^{-s} |\{x_{1:k}\in \mathcal{X}^k \text{ s.t.}\sum_{l(x_i)} = s\}| \leq \sum_{s = 0}^{kl_{max}} B^{-s} B^s = kl_{max} + 1\]
    - let $r = sum_{x \in \mathcal{X}}B^{-l(x)}$
    - we've shown that $r^k \leq kl_{max}+1$
    - suppose that $r > 1$. then as we increase the value of $k$, $r^k$ goes to $\infty$ exponentially, while $kl_{max}+1$ goes only linearly. So at some point the inequality fails.
    - thus $r <= 1$
  - Case 2: $\mathcal{X}$ is countably infinite
    - let $\mathcal{X} = \{1,2,\dots\}$
    - $\sum_{x\in\mathcal{X}}B^{-l(x)} = lim_{n\rightarrow\infty}\sum_{x = 1}^n B^{-l(x)}$
    - let $\mathcal{X}' = \{1,\dots,n\}$. Then we can create a code $C'$ on the subset $\mathcal{X}'$.
    - Then the $r\leq 1$ for $C'$.
    - limit of things $\leq 1$ is itself $\leq 1$

# 2.10: Examples for (b)

- formal proof is a bit confusing, so easier to work out some examples
- Setup
  - Let $\mathcal{X} = \{x_1,x_2,\dots\}$, $l_i = l(x_i)$, $l = (l_1, l_2,\dots)$
  - Suppose $l_1 \leq l_2 \leq l_3 \dots$
  - $B$-ary expansions
    - e.g. $B=2$, then $0.1 = 1/2, 0.01 = 1/4, \dots$
    - normal decimals are $B=10$
- General idea: divide up the interval 0-1 into lengths given by $1/2^{l(x)}$, read off the $B$-ary expansions of the intervals -> take the partial sum of the expansion as the encoding, expanding to $i$ places for $l_i$
- Examples
  - (A) from before: $l = (1,2,3,3)$, $x_1 = a, x_2 = b, x_3 = c, x_4 = d$
    - we try to create a $2$-ary prefix code
    - divide $[0,1]$ into $1/2, 1/4, 1/8, 1/8$, i.e. $0.1, 0.01, 0.001, 0.001$
    - partial sums are $0, 1/2, 3/4, 7/8$, i.e. $0, 0.1, 0.11, 0.111$
    - take to $l_i$ places, get $0.0, 0.10, 0.110, 0.111$
    - the fractional part is the code: 0, 10, 110, 111

# 2.11: Proof sketch for (b)

- why does the procedure above always give us a prefix code?
  - Fact $\alpha_1\dots\alpha_j$ is a prefix of $\beta_1\dots\beta_m$ if and only if the $B$-ary number $0.\alpha_1\dots\alpha_j \leq 0.\beta_1\dots\beta_m < 0.\alpha_1\dots\alpha_j + 1/2^j$
    - i.e. $0.1\beta_1\dots\beta_m\in[0.\alpha_1\dots\alpha_j, 0.\alpha_1\dots\alpha_j + 1/2^j]$
  - So by doing this interval process and adding the $0$s for the padding, we make impossible for one to be a prefix of the other.

# 3.1: Entropy as a lower bound on expected length (part 1)

- $L = \sum_{x\in \mathcal{X}}l(x)p(x)$, $p$ is pmf for Discrete Memoryless Source, i.i.d.
- want to find a uniquely decodable code to minimize this expected length
- note that we can classify codes by their lengths for the purposes here.
  - Kraft-McMillan gives us which lengths are possible
  - So we want to minimize $L$ given the condition from Kraft-McMillan
- Notation
  - $p_i = p(x_i)$, $l_i = l(x_i)$, for $\mathcal{X} = \{x_1,\dots,\}$
  - assume $p_i > 0\forall i$
- Problem Statement: minimize $\sum_i l_ip_i$ given that $\sum_i 1/B^{l_i} \leq 1$
  - to simplify, assume that we allow $l_i\in\mathbb{R}\forall i$, which allows us to use calculus
  - constrained minimization problem: we'll use Lagrange Multipliers
- Suppose $l$ is such that $\sum_il_ip_i$ is minimal and $\sum_i1/B^{l_i} < 1$
  - take an $l_i$:
    - $1/B^{l_i} \geq 0$
    - if $l_i\leq0$, this summand is $\geq 1$. Thus if $i > 1$, then the constraint doesn't hold.
    - thus we assume that $l_i > 0 \forall i$
    - But now we can take an $l_i$ and reduce it a tiny bit (enough so that $\sum_i1/B^{l_i}$ is still less than 1, which violates the minimality of $\sum_il_ip_i$.
      - this is a contradiction, so we have that $\sum_i 1/B^{l_i} = 1$ if $\sum_il_ip_i$ is minimized.
- New Problem Statement: minimize $\sum_i l_ip_i$ given that $\sum_i 1/B^{l_i} = 1$
  - assume $B >1$ (for the log in the next statement)
  - also assume $i > 1$ I think
- let $q_i = 1/B^{l_i}$, i.e. $l_i = \log_B1/q_i$. Then 
  - the constraint turns into $\sum_iq_i = 1$
  - minimization turns into $\min_{q_i > 0} \sum p_i\log_B1/q_i$

# 3.2: Entropy as a lower bound on expected length (part 2)

- let $f(q) = \sum_i p_i \log_B 1/q_i$
  - let's see if it's convex: take the Hessian
  - note that $\log_Bx = (\ln x)/(\ln B)$, so that $\frac{d}{dx}\log_B\frac{1}{x} = \frac{-1}{x\ln B}$
  - $\frac{d}{dq_i} f = \frac{-p_i}{q_i\ln B}$
  - $\frac{d^2}{dq_idq_j} f = \frac{p_i}{q^2\ln B}$ if $i = j$ else 0
  - Thus the Hessian is diagonal with $\frac{p_i}{q^2\ln B}$ in the diagonal
    - eigenvalues are the diagonal elements, so are all positive.
    - Thus $f$ is strictly convex as the Hessian is positive definite
- Let's think about the constraint $\sum q_i = 1$
  - The set of $q_i$ that satisfy this are also a convex set
    - $\forall \alpha \in [0,1]$, then if $\sum q_i = 1$ and $\sum q_i' = 1$, then $\sum \alpha q_i + (1-\alpha)q_i\ = 1$, so all points in between are also in the set.
- So we're minimizing a convex function over a convex set
  - this is great, because the restriction of the convex function over the convex set will also be strictly convex
  - a critical point of the restricted function will be a global minimum for the whole convex set.

# 3.3: Entropy as a lower bound on expected length (part 3)

- We use Lagrange Multipliers to get critical points
  - $\mathcal{L}(q,\lambda) = \sum_i p_i\log(1/q_i) + \lambda (\sum_i q_i - 1)$
    - We solve $\nabla_q\mathcal{L} = 0$, $\nabla_\lambda\mathcal{L} = 0$ for $q$ and $\lambda$
      - second restraint reduces to $\sum_iq_i = 1$
      - first restraint: we calculated the derivative before of the first part of $\mathcal{L}$
        - so reduces to $0 = \frac{-p_i}{q_i\ln B} + \lambda$
      - $\frac{p_i}{\ln B} = \lambda q_i$
      - $\frac{1}{\ln B}\sum_i p_i = \lambda \sum_i q_i$
        - we have $\sum_i p_i = 1$ by definition, $\sum_i q_i = 1$ by the constraing
        - $\frac{1}{\ln B} = \lambda$
      - substitute for $1/\ln B$, get $q_i = p_i$ for all $i$
  - so the critical points of the constrained problem are given by $q_i = p_i$
    - remember $l_i = \log_B(1/q_i)$, so $l_i = \log_B (1/p_i)$ are the lengths that minimize the expected length
    - Thus the minimimum expected length is simply the information entropy
      \[\sum_i l_i p_i = \sum_i p_i \log_B(1/p_i)\]
- let's look at the solution we found:
  - $p_i$ are all positive and less than 1, so the optimal lengths are non-negative
  - The optimal lengths are not necessarily integers
    - we've found a lower bound (i.e. integer solution is at best this minimum expected length)
- final notation remark: if $B$ is very often $2$, in which case the entropy is
  \[-\sum_i p_i \log_2 p_i\]

# 3.4: Remark - an alternate proof

- can be done using "relative entropy" to get a quick proof, but the proof isn't so intuitive

# 3.5: Bounds on the optimal expected length

- $H_B(p)$ is a lower bound on the expected code length $L$.
- In fact, we can achieve a code $L$ such that $H_B(p)\leq L\leq H_B(p) + 1$
  - remember that the optimal real valued $l_i$ were $\log_B(1/p_i)$ (we assume $p_i > 0$)
    - and for these optimal values $\sum_i 1/B^{l_i} = 1$
    - but these are not necessarily integers
    - suppose we round them - the Kraft-McMillan constraint might not necessarily hold again
    - let's try $l_i = ceil(\log_B(1/p_i) < log_B(1/p_i) + 1$
      - as all the $l_i$ are $\geq$ what they were before, the inequality still holds
      - so by Kraft-McMillan, there exists a uniquely decodable code with these lengths.
      - what's the expected code word length of this code?
        - $\log_B(1/p_i) \leq l_i < log_B(1/p_i) + 1$
        - $p_i\log_B(1/p_i) \leq p_il_i < p_ilog_B(1/p_i) + p_i$
        - $H_B(p) \leq \sum_i p_il_i < H_B(p) + 1$ as we sum over the $i$
- so just rounding $\log_B(1/p_i)$ up to the nearest integer is a code with expected length within one of the entropy $H_B(p)$
  - we call the prefix code thus obtained "Shannon coding"
- Consider an optimal code, then $H_B(p) \leq L_{Optimal} \leq L_{Shannon} < H_B(p) + 1$
  - Later, we'll see an optimal symbol code (Huffman)
- $+1$ is up to one extra bit per source symbol (if $B=2$)
  - so not that great in practice perhaps
  - we'll see better approaches later (Block Coding, Source Coding Theorem)

# 3.6: Example - entropy as a lower bound

- Look at (A) from before: symbols $\{a,b,c,d\}$
  - Definition
    - code: $0,10,110,111$
    - lengths: $1,2,3,3$
    - $p(x)$: $1/2,1/4,1/8,1/8$
  - Expected Length is $1.75$
  - Entropy is $H_2(p) = \sum_i p_i\log_2(1/p(x)) = \frac{1}{2}\log_2(2) + \frac{1}{4}\log_2(4)\dots = 1/2 + 1/2 + 3/8 + 3/8 = 1.75$
  - we've achieved the lower bound for the expected code word length
    - which makes sense as the ideal lengths are $\log_2(1/p_i)$, which corresponds to the code.

# 3.7: Block codes for compression

- Symbol code $x_1\dots x_n \rightarrow C(x_1)\dots C(x_n)$
  - code word has to have an integer value lenth
  - $H \leq L \leq H + 1$ as expected code word length
    - so can have as much as one extra bit per word, so $n$ bits for $x_1\dots x_n$
- Block codes
  - spread out the one bit of inefficiency among a number of the $x_i$
    - e.g. if spread out among 3 symbols, then spread out the one bit among the 3 symbols
  - until now, $C:\mathcal{X}\rightarrow \mathcal{A}_{0}$, with $\mathcal{X}$ distributed ot $p$. Discrete memoryless source: iid according to $p$. Calculated expected length $L$
  - Now, we encode in blocks: given $k$
    - $C:\mathcal{X}^k\rightarrow A_{0}$
    - $x_{1:k} = x_1\dots x_k$ distributed according to $p^k$: iid, with each generated by $p$
    - $L^{(k)} = \sum_{x_{1:k}}l^k(x_{1:k})p^k(x_{1:k})$
  - we can kind of "change of variables" with $\mathcal{X}' = \mathcal{X}^k$, $p' = p^k$, and have the same situation as before
  - But now we can be more efficient:
    - look at the expected encoding length per source symbol $L_k:= L^{(k)}/k$
    - $H_B(p^k) \leq L^{(k)} < H_B(p^k)+1$

# 3.8: Entropy of iid random variables

- What is $H_B(p^k)$?
- $p^k(x_{1:k}) = p(x_1)\dots p(x_k)$
- Theorem: $H_B(p^k) = kH_B(p)$
  - $H_B(p^k) = \sum_{x_{1:k}} p^k(x_{1:k})\log_B(1/p^k(x_{1:k}))$
  - $\log_B(1/p^k(x_{1:k})) = \log 1/(p(x_1)\dots p(x_n)) = \sum_1^k \log(1/p(x_i))$
  - so $H_B(p^k) = \sum_{i = 1}^k\sum_{x_{1:k}}p^k(x_{1:k})\log_B(1/p(x_i))$
  - note that $\sum_{x_1:k} = \sum_{x_1}\sum_{x_2}\dots\sum_{x_k}$, so we have
  - $H_B(p^k) = \sum_i \sum_{x_i}\log_B(1/p(x_i))\sum_{x_j, j \neq i}p^k(x_{1:k})$
  - Note that $\sum_{x_j, j\neq i}p^k(x_{1:k}) = p(x_i)$
  - $H_B(p^k) = \sum_{i=1}^k\sum_{x_i\in \mathcal{X}}p(x_i)\log_B(1/p(x_i)) = \sum_{i = 1}^k H_B(p) = kH_B(p)$

# 3.9: Source Coding Theorem

- Three big theorems of Information Theory
  - Source Coding theorem: Establishes the best possible compression that one can achieve
  - Channel Coding theorem: given a noisy channel, the best possible rate at which one can communicate and still achieve arbitrarily low error
  - Rate Distortion theorem: given a noisy channel and acceptable level of distortion, the best possible rate that one can communicate with distortion under that level.
- Notation:
  - $C:\mathcal{X}^k \rightarrow A_{0}$
  - $L^{(k)}$ is the expected length for encoding of $k$ symbols
  - $L_k = L^{(k)}/k$: the expected length per source symbol
- Note that if $|\mathcal{X}|=|A|$, then $L_k$ is the compression ratio of the code
- Theorem (Shannon): Let $X_1, X_2\dots$ be a discrete memoryless source given by a iid probability distribution $p$, $X_i$ is countable
  - (a): "you can't beat the entropy": For any uniquely decodable block code $C:\mathcal{X}^k\rightarrow A_{0}$, $H_B(p)\leq L_k$
  - (b): "you can get as close as you wish": $\forall \epsilon > 0$, $\exists N$ s.t. $\forall k > N$ there is a prefix block code $C:\mathcal{X}^k\rightarrow A_{0}$ s.t. $L_k < H(p) + \epsilon$
- Proof:
  - (a): Let $C$ be such a block code, we've shown that $L^{(k)} \geq H_B(p^k)$. Thus $L_k \geq H_B(p)$ as desired
  - (b): Let $\epsilon > 0$ and consider $k > 0$. Then from our earlier results, we can make a prefix code $C$ such that $L^{(k)} < H(p^k)+1$. Substituting, this is $L_k < H(p) + 1/k$. So as $k$ grows, we can get arbitrarily close (i.e. $N > 1/\epsilon$).
- This is mainly theoretical, we need to have a code word for every sequence $x_1\dots x_n$, which gets computationally impractical as $k$ grows

# 3.10: Relative Entropy as the mismatch inefficiency

- To do shannon coding, we need to know the true probabilities $p_i$ so that we can assign lengths $ceil(\log_B (1/p+i))$.
  - in practice, we don't know the true $p_i$
- Notation: $q_i$ as our estimate for $p_i$, then define lengths using these
  - $l_i = ceil(\log_B(1/q_i)) = \log_B(1/q_i) + r_i$ with $r_i\in [0,1)$
- How much does expected code length increase?
  - $L = \sum l_ip_i = \sum p_i \log (1/q_i) + \sum r_i p_i$
  - let $R = \sum r_i p_i \geq 0$
  - $\sum p_i \log (1/q_i)$ is minimized when $q_i = p_i$
    - $\sum p_i \log (1/q_i) = \sum p_i \log (p_i/q_i) + \sum p_i \log(1/p_i) = \sum p_i \log(p_i/q_i) + H(p)$
    - $\sum p_i \log_B(p_i/q_i)$ is known as the relative entropy $D(p||q)$ or the Kullback-Leibler divergence
  - Thus, the increase in code length is $KL(p||q) + R$
    - $R$ is the "rounding inefficiency"
    - $KL(p||q)$ is the mismatch inefficiency
- Note that $H(p)\leq \sum p_i \log (1/q_i) = H(p) + KL(p||q)$
  - thus $KL(p||q) \geq 0$
- Also note that $KL(p||q) \neq KL(q||p)$, but $KL$ is intuitively thought of as the "distance" between $p$ and $q$, although it isn't really a true distance.

# 4.1: Huffman coding - introduction and example

- Problem: Design an algorithm taking a pmf $p = (p_1,\dots,p_n)$ and producing an optimal code $C$, where $C$ is optimal w.r.t. minimum length.
- Shannon-Fano coding: proposed, but not optimal
- Huffman: student of Fano, invents Huffman coding as a class assignment
  - Huffman coding is quite simple to implement
- Example (a): $p(x) = (0.35, 0.2, 0.2, 0.15, 0.1)$
  - take the two smallest probabilities $(0.15, 0.1)$. The sum of probabilities is 0.25
  - Now take the two smallest probabilities, replacing 0.15 and 0.1 with 0.25
  - continue, building a tree until we get 1.0
  - now label each line in the tree with a symbol from the code alphabet.
    - e.g. binary, so either 0 or 1
  - assign the code word to a leaf in the tree by following the path from 1 to the leaf.
    - this is always a prefix code by construction
  - expected length is 2.25, which is equal to the entropy $H_2(p) = 2.2016$
    - this is the best possible $L$ due to rounding inefficiency

# 4.2: Huffman coding - more examples

- Example (b): $p(x) = (0.3, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1)$
  - do the same process as before, with $B=2$ and $A = \{0,1\}$
  - Expected length is 2.7 bits, $H_2(p) = 2.6464$ bits
  - again, pretty close
  - note that the code is non-unique
    - there isn't a uniquely optimal code: e.g. if $B=2$, as we can swap 0 with 1 in all code words and we get another optimal code.
    - There can also be more than one Huffman code, as we can make different choices in building our tree
      - build a different tree, get a different code
      - the set of lengths of this new code are also different
      - the expected length is still 2.7, as apparently all Huffman codes are optimal (!)

# 4.3: $B$-ary Huffman codes

- Take the same probabilities as (a): $(0.35, 0.2, 0.2, 0.15, 0.1)$
  - suppose $B=3$
  - take 3 minimal probabilities, do the same process
  - get an optimal 3-ary code
  - Expected Length is 1.45 "trits", Entropy is $H_3(p) = 1.3891$ trits
  - larger $B$, can get shorter length: trits vs bits
- Example (d): let $p(x) = (0.35, 0.2, 0.15, 0.1, 0.1, 0.1)$
  - do the same process
  - after 2 steps, we only have 2 symbols left, which is a problem for the algorithm
  - so we pad the probabilities with a 0, and now it works
- (for a $B$-ary code, we pad the length to be $k(B-1)+1$ for some $k$ for the algorithm to work)

# 4.4: Weighted minimization with Huffman coding

- We've been trying to minimize $\sum l_i p_i$.
- Now consider a different function $f(l) = \sum l_i w_i$ with arbitrary weights instead
- It turns out that solving this minimization over UD codes has the same solution
  - replace the $p_i$ with $w_i$ and do Huffman coding again
  - will also get an optimal UD code
- i.e. the fact that $p$ is a pdf is not important.
- For example, suppose that some messages are more important: Can give them more weight so that they get shorter code words

# 4.5: An issue with Huffman coding

- Huffman coding is only optimal as a symbol code
- sometimes symbol codes perform poorly
  - e.g. when the source has low entropy
    - Huffman always has $H\leq L< H+1$, but we can be arbitrarily close to this upper limit $H+1$. And $(H+1)/H$ can be large
- Suppose $\mathcal{X} = \{1,2\}$, with $p(1) = 0.001$, p(2) = 0.999$.
  - Huffman coding: $C(1) = 0$, $C(2) = 1$. Then $l = (1,1)$ with $L = 1$.
  - Shannon coding: $p = (0.001, 0.999)$, so $\log_2(1/p_x) \approx (10, 0.0014)$. If we round them up, then we have lengths $l = (10, 1)$. Thus $L \approx 1.009$.
    - a prefix code could be $(1111111110, 0)$
    - not that the bulk of $1.009$ came from the second length $1$, and not really from the first length $10$
  - What's the entropy: $H_2(p) \approx 0.01$, which is much less than either $L$, with $H_2(p)+1 = 1.01$
    - if we make $p$ more asymmetric, we can get even closer to $H+1$.
    - to get closer, we could use block coding and encode blocks and use Huffman
      - but then the size of the source alphabet is $|\mathcal{X}|^k$, which grows exponentially
- it turns out that there's an elegant way to remedy this: arithmetic codes
  - the issue is that we need integer length code lengths for symbol codes

# 4.6: Optimality of Huffman Codes (1)

- We'll prove for $B=2$, $\mathcal{A}=\{0, 1\}$
  - it's pretty easy to generalize to the more general case.
- We have:
  - $\mathcal{X} = \{x_1,\dots,x_n\}$ is finite, $|\mathcal{X}| = n$ (for Huffman, $\mathcal{X}$ is finite.
  - $p = (p_1,\dots,p_n)$ as the pmf
  - $l = (l_1,\dots,l_n)$ as the lengths of the code words $x_i$
  - we want to minimize $L = \sum l_i p_i$  
  - we consider all UD codes.
  - we can focus on prefix codes because of Kraft-McMillan (for all UD codes, there's a prefix code with the same lengths)
- Claim: The Huffman coding procedure always gives an optimal solution for the lengths.
- Lemma (Existence): For any pmf $p$, there exists an optimal prefix code.
- Proof: To be done later
- Lemma (Inverse ordering): For any optimal prefix code, $\forall j,k$ if $p_j > p_k$ then $l_j\leq l_k$. In other words, more common symbols have smaller (or same) lengths.
- Proof: Let $C$ be an optimal prefix code with lengths $l$ with code words (images of $x_i$ in $\mathcal{A}_{0}$) $\{w_1,\dots w_n\}$
  - Suppose that $p_j > p_k$
  - Let $C'$ be such that $w'i = w_i\forall i$ s.t. $i\neq j$ and $i\neq k$. Let $w'_j = w_k$, $w'_k = w_j$.
  - As $C$ is optimal, then $L_C\leq L_{C'}$, i.e.
    - $0\leq L_{C'} - L_C = \sum l_i'p_i - \sum l_ip_i = l_kp_j + l_jp_k - l_jp_j - l_kp_k$
    - $= l_k(p_j - p_k) - l_j(p_j-p_k) = (l_k-l_j)(p_j - p_k)$
    - as $p_j - p_k > 0$, we have
    - $0 \leq l_k - l_j$, or $l_j \leq l_k$, as desired.


# 4.7: Optimality of Huffman Codes (2)

- Lemma (Siblings): For any pmf $p = (p_1,\dots,p_n)$, there exists an optimal binary prefix code such that two of the longest codewords satisfy the sibling property:
  - (a): they have the same length
  - (b): they differ only in the last bit.
- Proof:
  - From the set of optimal prefix codes, take the prefix code $C$ that has the smallest $\sum_i l_i$.
  - We claim that htis code satisfies the lemma.
    - (a): Suppose that (a) is false. Then one code word is longer than all the others: WLOG, $l_n > l_i\forall i\neq n$.
      - let $C'$ be a new code with $w_i' = w_i\forall i < n$ and $w_n' = w_n$ without the last bit. I.e. $w_n = w_n'\alpha$ with $\alpha\in\mathcal{A}$
        - $C'$ is a prefix code: If $w_n'$ is a prefix of another code word $w_i$, it has to be equal because $w_n'$ is at least as long as $w_i$. But that would imply that $w_i$ is a prefix of $w_n$, which is a contradiction. Similarly, none of the $w_i$ can be a prefix of $w_n'$.
        - Lengths: $C'$ has length at least as good as $C$. If $p_n\neq 0$, then we have a contradiction, but if $p_n = 0$ then $\sum l_i' = \sum l_i - 1$, another contradiction. Thus (a) is true
    - (b): Suppose that (b) is false. Let $k$ be the length of the longest codewords.
      - (b) says that there exists two codewords that only differ in the last bit, so we proceed by assuming that any two code words of length $k$ differ somewhere in the first $k-1$ bits.
      - Let's define $C'$ as being the same as $C$, except for one of the codewords with length $k$ - we remove the last bit from this one.
      - as before $\sum l_i' = \sum l_i - 1$, so to prove the contradiction we only need to show that it's a prefix code.
      - To check that it's a prefix, we only have to check that
        - the new code word is not a prefix of any other word: can't be a prefix of anything of length $k$, as then it'd be the same in the first $k-1$ elements, which breaks (b). Also can't be a prefix of anything of length $k-1$, as then they'd be the same and then that one would be a prefix in $C$. And of course can't be a prefix of stuff with shorter length, as it has length $k-1$
        - nor that any word is a prefix of the new code word: impossible, as it'd also be a prefix in $C$
      - so contradiction, and done.
- note: we had to do the silliness with $\sum l_i$, as some of the $p$ could be $0$ in general.

# 4.8: Optimality of Huffman Codes (3)

- Key Lemma (Sibling Code): Let$ $p$ be a pmf s.t. $p_1\geq\dots\geq p_n$. There exists an optimal binary prefix code with code words $w_1,\dots,w_n$ with $w_i = C(x_i)$ s.t.
  - (a) $l_1\leq\dots\leq l_{n-1} = l_n$ 
  - (b) $w_{n-1}$ and $w_n$ differ only in the last bit
- Note that this sibling property is slightly stronger because we are guaranteed that it folds for $x_{n-1}$ and $x_n$
- Proof Sketch:
  - use sibling lemma from before to get an optimal prefix code with properties. Then rearrange (using the inverse ordering lemma) to get (a). To get (b), swap code words to get them at the end.

# 4.9: Optimality of Huffman Codes (4)

- The process of creating a Huffman code is a recursive procedure. So proof by induction is probably a good idea
- Let
  - $p$ be a pmf $(p_1,\dots,p_n)$ with $p_1\geq\dots\geq p_n$.
  - $p'$ be a pmf $(p_1,\dots,p_{n-2}, p_{n+1} + p_n)$, i.e. After one Huffman step.
- Definition: Given a sibling code $C^s$ for $p$, the $H$-contraction (Huffman contraction) $C^c$ to $p'$ is the same as $C^s$ for all except the sibling. On the sibling it removes the last bit.
  - i.e. $w^c_i = w^s_i\forall i < n-1$, $w^c_{n-1}\alpha = w^s_n$, where $\alpha is the last bit$ of $w^s_n$ and $w^s_{n-1}$.
- Definition: Given an optimal code $C^o$ for $p'$, the $H$-extension $C^e$ to $p$ of $C^o$ has code words $w^e_i = w^o_i\forall i < n-1$, $w^e_{n-1} = w^o_{n-1}0$, $w^e_n = w^o_{n-1}1$.
- Note that Any Huffman code for $p$ is the $H$-extension of a Huffman code for $p'$

# 4.10: Optimality of Huffman Codes (5)

- Lemma (Extension): Let $p$ be a pmf with $p_1\geq\dots\geq p_n$. Let $p' = (p_1,\dots,p_{n-2},p_{n-1} + p_n)$. The $H$-extension to $p$ of any optimal code for $p'$ is optimal for $p$.
- This will be the induction step of the final proof
- Proof: Let $C^o$ be optimal for $p'$. Let $C^e$ be the $H$-extension to $p$. Then
  - $L^e = l^e_1 p_1 + \dots + l^e_{n-1}p_{n-1} + l^e_np_n = l^o_1p_1 + \dots (l^{o}+1)(p_{n-1} + p_n) = L^o + p_{n-1} + p_n$
  - Let $C^s$ be a optimal prefix sibling code (from Key Lemma) for $p$. Let $C^c$ be the $H$-contraction to $p'$.
  - Then $L^c = l_1^cp_1+\dots+l^c_{n-1}(p_{n-1} + p_n) = L^s - p_{n-1} - p_n$
  - If we add the two expressions, we have $L^e_n + L^c_{n-1} = L^o_{n-1} + L^s_n$, where we add subscripts to keep track of which the code is about.
    - Rearrange: $(L^e_n - L^s_n) + (L^c_{n-1}- L^o_{n-1}) = 0$
    - Both $C^s$ and $C^o$ are optimal by assumption: Thus both terms are $\geq 0$. Thus both are 0, so that $L^e_n = L^s_n$ and $L^e$ is optimal.
  - bonus result that $L^c_{n-1} = L^o_{n-1}$, i.e. that the contraction is also optimal.

# 4.11: Optimality of Huffman Codes (6)

- We assumed that the Huffman code was presorted $p_1\geq\dots p_{n-1}\geq p_n$.
  - Let's define that if we combine $p_{n-1}$ and $p_n$ first then we've made a "Standard Huffman Code"
  - All Huffman codes are a Standard Huffman Code of some ordering.
- Theorem: Any Huffman code is optimal.
- Proof: $S_k$ = "Any Huffman code on a pmf with $k$ elements is optimal" - the induction statement.
  - The base step $S_2$ is true, as nothing is better than having one bit per code word.
  - Induction step: Suppose that $S_{n-1}$ is true.
    - Let $q$ be a pmf on $n$ elements, $C$ be a Huffman code for $q$.
    - Then as we remarked above, $C$ is a standard Huffman code for an ordering $p$ of $q$.
      - (permute the code words along with the $p_i$, of course)
    - This implies that $C$ is the $H$-extension of a Huffman code for $p'$.
    - As $S_{k-1}$ holds, then the Huffman code in $p'$ is optimal.
    - Then by the extension lemma, the Huffman code $C$ on $p$ is optimal.
- Remark: Remember that symbol codes can be inefficient, as all things have to have integer length - and block codes get computationally difficult.
- Remark: Not every optimal code is Huffman. (later)

# 4.12: Optimality of Huffman Codes (7)

- Existence of optimal codes: Most people skip this, so we'll prove it
- Lemma (Existence): if $\mathcal{X}$ is finite and $p$ is a pmf on $\mathcal{X}$, then there exists an optimal (UD with minimal expected code length $L$) code for $p$
- Note: we can even have $p_i=0$ for some $i$
- Proof:
  - Let $|\mathcal{X}| = n$, $p = (p_1,\dots,p_n)$ with $p(x_i) = p_i$ and $p_1\geq \dots\geq p_k > p_{k+1} = \dots= p_n = 0$.
  - Define $C_n$ by $C_n(x_1) = 0$, $C_n(x_2) = 10$, $C_n(x_3) = 110$ etc.
    - this is a uniquely decodable prefix code with $L \leq n$.
  - For $i = 1,\dots,k$, let $m_i = n/p_i$. Then if $l_i\geq m_i$, then $l_ip_i\geq n$.
    - if a code word has a length for $x_i$ that is more than $m_i$, then it is worse than $C_n$ for that part
  - let $\mathcal{C} = \{C: C$ is UD with lengths $l_1,\dots,l_n$ where $l_i\leq m_i$ for all $i\}$
    - note that $C_n\in \mathcal{C}$ as $l_i^{C_n}\leq n = m_ip_i\leq m_i$
  - let $\mathcal{L} = \{L_C\in \mathbb{R}: C\in\mathcal{C}\}$
  - Note that $|\mathcal{C}|$ is potentially infinite, as we could have some $p_i = 0$
  - However, $\mathcal{L}$ is finite with size $\leq m_1m_2\dots m_k$
  - Thus there is a minimum $L$ (we know that $\mathcal{L}$ is nonempty as $C_n\in\mathcal{L}$)
  - Let $C_*\in\mathcal{C}$ s.t. $L_{C_*} = \min \mathcal{L}$.
  - let $C$ be UD
    - if $C\in\mathcal{C}$, then $L_C\geq L_{C_*}$
    - if $C\notin\mathcal{C}$, then for some $j$, $l_j > m_j = n/p_j$, so that $l_jp_j > n$, which means that $L_C > L_{C_n}$ and it's length not better.

# 4.13: Not every optimal prefix code is Huffman

- Trivially, we can take a prefix code and swap 0s and 1s => get a "suffix" code and get a UD optimal code.
  - Thus, not every optimal code is a prefix code
- Stronger claim: Not every optimal prefix code is a Huffman Code
  - Example: look at $p = (0.3, 0.3, 0.2, 0.2)$. All Huffman codes have the first digits of $C(x_1)$ and $C(x_2)$ the same, and these are different than the first digit of $C(x_3) and $C(x_4)$.
    - $L = 2$
    - we can make another code that doesn't satisfy that first property by simply mixing around the code words -> and get an optimal code that's not Huffman.

# 5.1: Arithmetic coding - introduction

- Elegant and powerful technique, current state of the art in lossless compression.
- associate a symbol with a subinterval in [0,1], and then choose a number in that subinterval that has a short binary expansion as the code word, e.g. $0.01101_2\rightarrow 01101$
- Symbol codes
  - Huffman codes are only optimal as symbol codes
  - each symbol has an integer value length
  - can have up to one bit of inefficiency per input word, so if a message has $n$ inputs, can have $n$ bits of inefficiency
  - we can use block codes to reduce this, but this is computationally inefficient
- Arithmetic coding: can be within a few bits of the ideal entropy of the whole message
  - near optimal
  - efficient
  - can accomodate more complex (e.g. non-iid) sources in a pretty simple/adaptive manner, separating modelling from coding

# 5.2: Arithmetic coding - Example 1

- Let
  - $\mathcal{X} = \{0,1,2\}$.
  - $p = (p_0, p_1, p_2) = (0.2, 0.4, 0.4)$
- take a sequence $x_1x_2x_3 = 210$
  - encode it in two parts
    - associate the sequence with a subinterval
      - draw interval [0,1)
      - subdivide according to the pmf $p$: i.e. [0,0.2), [0.2,0.6), [0.6,1)
      - first symbol is a $2$, so focus on [0.6, 1).
      - then subdivide this by the pmf again: [0.6,0.68), [0.68, 0.84), [0.84,1)
      - second symbol is $1$, so focus on [0.68,0.84)
      - subdivide again, third symbol is 0, so focus on [0.68,0.712)
    - now find the shortest binary expansion in this interval
      - i.e. split the interval in half until one of the intervals lies within [0.68,0.712)
        - [0,1) => [0.5,1) => [0.5,0.75) => [0.625,0.75] => [0.6875,0.75) => [0.6875,0.71875) => [0.6875,0.703125)
      - We end up with the interval $[0.101100_2,0.101101_2)$
      - encode 210 as the bottom part of the interval: 101100
        - even though the bottom was in since 1011, we also added the 0s.
  - how do we decode: not entirely trivial
    - one way: encode the length of the original message as well (e.g. 3)
    - better way: use an "end of file (EOF) symbol"
      - use a symbol from the source alphabet to always occur at the end of the sequence encoded.
      - for example, $0\in\mathcal{X}$ is at the end of sequences
        - so when we find a 0, we know that it's the end of the sequence, and that the next digits are part of the next sequence.
  - what if we have a long sequence before the EOF occurs?
    - then the distance on the interval gets really small, which clashes with machine precision
    - can modify to use only finite precision
      - periodically rescale back up to 1, have subtleties in rounding
      - we'll look at this later

# 5.3: Arithmetic coding - Example 2

- This example is also iid
  - but it doesn't have to be: we just have to split the intervals differently on each layer
- we're looking for the shortest possible sequence whose interval lies within
  - it has to be the shortest to disambiguate between one longer sequence and multiple sequence s put together
    - i.e. to be uniquely decodable
    - pick the side that is closer
- Let
  - $\mathcal{X} = \{0,1,2,3\}$
  - EOF = 0
  - $p = (p_0,p_1,p_2,p_3) = (0.05, 0.05, 0.5, 0.4)$
    - note that doesn't have to be decreasing
- take the sequence $x_1x_2x_3x_4 = 2320$
  - let $c = (c_0,c_1,c_2,c_3) = (0, 0.05, 0.1, 0.6)$, i.e. partial sums of $p$
  - interval $[a,b)$ is given by 
    - $a = c_2 + p_2c_3 + p_2p_3c_2 = 0.42$
    - $b = a + p_2p_3p_2p_0 = 0.425$
  - note that $b = a + f$, with $f = \prod_i p_i$
  - Now we find the binary sequence
    - interval in this corresponds to $[0.011011000_2,0.011011001_2)$
  - thus we take $2320$ to $011011000$
- In general, the length of the interval that you end up with after the first step is $\prod_i p_i$
  - by construction

# 5.4: Why the interval needs to be completely contained

- Why not stop when we get one number in the interval?
  - It wouldn't be uniquely decodable
- $C:\mathcal{X} \rightarrow \mathcal{A}_0$
  - prefix if $C(x_1)$ is not a prefix of $C(x_2)$ if $x_1\neq x_2$
  - uniquely decodable if $C_0$ is 1-1 where $C_0(x_1\dots x_k) = C(x_1)\dots C(x_k)$
- Define $\mathcal{X}^0 = \{x_1\dots x_k0: x_i\in \mathcal{X}\setminus \{0\}, k \geq 0\}$
  - then $C:\mathcal{X}^0 \rightarrow \mathcal{A}_0$ is a restricted code on this subset
- Define the "number version" of the algorithm which stops when one binary expansion number is within, i.e. not the interval. The "interval version" is the normal one.
  - The number version is not prefix and not uniquely decodable
  - The interval verison is prefix and uniquely decodable
- Look at Example 1, where we mapped $210$ to $10110$. Under the number version, it goes to $1011$
  - Suppose the source was $210121011230$ or something. Under the number version, we have $1011$ + stuff. And we don't know when to stop.
    - unless we encoded the lengths as well.
    - in the number version, pretty much any sequence is a valid encoding.
      - e.g. 101 is a prefix of 1011, and both are valid encoded messages
    - in the interval version, this isn't the case.
      - if $C(x^1)$ is a prefix of $C(x^2)$, then $x^1$ is a prefix of $x^2$ as members of $\mathcal{X}^0$
        - prove by thinking about the intervals, which interval has to be within which, etc.
        - which is impossible as $x^i$ can only have one 0, which is at the end.

# 5.5: Rescaling operations for arithemetic coding

- we've been assuming infinite precision, as it's easier to understand
- let's look at an implementation of infinite precision that extends naturally to finite precision.
- rescaling
  - keep first part the same for now, look at second part: find binary interval within the other interval
  - e.g. if we look at list time $[0.68,0.712)$
  - first split: $[0,0.5), [0.5,1)$
  - note that the the first part of the code word is 1
  - then look at $[0.5,1)$
    - rescale up to $[0,1)$
    - note that $[a,b)$ goes to $[0.36, 0.424)$
    - note that we will select the first half, so second part of the code word is 0
    - repeat
  - at some point, 0.5 could split $[a,b)$
    - look at $0.5-a$ and $b-0.5$
    - we split the bigger one, as we will be able to get inside $[a,b)$ faster.
    - or could keep track at this split point and go on.
  - three options: $[a,b)$
    - in lower half or upper half: focus
    - straddle the middle point: keep track and continue expanding.
      - the shortest code will be around the middle, so either 011..(LHS) or 100 (RHS) for this portion
- more precisely, given $[a,b)$:
  - $b < 0.5$: "emit" 0, consider the new interval $[2a, 2b)$
  - $a > 0.5$: emit 1, consider the new interval $[2(a-0.5), 2(b-0.5))$
  - $a \in (0.25,0.5]$, $b\in [0.5,0.75)$: look at $[2(a-1/4), 2(b-0.1/4))$, i.e. the middle two, keep track $s:= s+1$ ($s$ starts out at $0$)
  - $a\leq 1/2$, $b\geq 3/4$: $s:= s+1$, emit $10\dots 0$, with $s$ zeros.
  - $a\leq 1/4$, $b\geq 1/2$: $s:= s+1$, emit $01\dots 1$, with $s$ ones.
  - note that if $a\leq 1/4$ and $b\geq 3/4$ we can pick either source.

# 5.6: Encoder for arithmetic coding (infinite-precision)

- practially impossible, of course (due to machine precision etc), but still nice to know
- Let 
  - $\mathcal{X}=\{0,1,\dots,n\}$
  - $p = (p_0,p_1,\dots,p_n)$ be a pmf on $\mathcal{X}$
  - $c_0 = 0$, $c_j = p_0 + \dots + p_{j-1}$ for $j = 1,\dots,n$
  - $d_j = c_j + p_j = c_{j+1}$
- Encoder: input $x_1\dots x_{k+1}$
  - (A): $a=0$, $b=1$. For $i = 1,\dots,k+1$
    - $w = b-a$
    - $b := a + wd_{x_i}$, $a := a + wc_{x_i}$
    - in other words, it's the part of the $[0,1)$ interval that we're "focusing" on previously.
    - at the end we obtain the interval in that we want to fit the other interval within
  - (B): $s:= 0$ 
    - while $b< 1/2$ or $a>1/2$:
      - if $b<1/2$ emit 0, $a:=2a$, $b:=2b$
      - if $a>1/2$ emit 1, $a:=2(a-1/2)$, $b:=2(b-1/2)$
      - i.e. the algorithm from before (rescaling)
    - while $a > 1/4$ and $b < 3/4$ (straddle midpoint)
      - $s:= s+1$, $a = 2(a-1/4)$, $b=2(b-1/4)$
    - if:
      - $a \leq 1/4$, then emit $01\dots 1$ with $s$ 1's
      - else emit $10\dots 0$, with $s$ 0's

# 5.7: Decoder for arithmetic coding (infinite precision)

- tricky in finite precision version, but not so bad for infinite precision
- in the example, we had $0.101100_2$. We found the number for it
  - saw that it was in the 2 chunk, so the first number is 2
  - saw that it was in the 21 chunk, so the second number is 1
  - saw that it was in the 210 chunk, so the third number is 0
  - 0 is EOF, so done
- Decoder:
  - Input is $\beta_1\dots \beta_m\beta_{m+1}\dots\beta_M$, where $\beta_i\in\{0,1\}$, $\beta_{m+1}\dots \beta_M$ is like the next part of data, perhaps the next set of symbols, etc.
  - we'll assume for now that we have $\beta_1\dots \beta_m$
  - initialize $a=1$, $b=1$, $z = 0.\beta_1\dots \beta_m$ in base 2 (i.e. the binary expansion)
  - while True:
    - for $j = 0,1,\dots,n$: the possible next source symbols
      - $w = b-a$
      - $b_0 := a + wd_j$, $a_0 := a + wc_j$
      - if $a_0\leq z\leq b_0$
        - then emit $j$, set $a:= a_0$, $b:= b_0$, break for loop
        - if $j=0$, then quit the whole thing (EOL)
  - output is $x_1\dots x_k0$, where the $x_i$ are the emitted stuff
- what if we have a longer sequence (e.g. $\beta_1\dots \beta_m\beta_{m+1}\dots\beta_M$)
  - note that the encoding is a prefix code (because the interval is enclosed)
  - consider $z = 0.\beta_1\dots \beta_M$, i.e. as if the whole sequence was one set
    - then this $z$ is in the same interval as the $z$ from before
    - so as we decode the new $z$, we'll get the same sequence
      - until we get the EOL, at which point we start again with $\beta_{m+1}\dots\beta_M$
        - or rescale the remainder
  - in practice, $M$ can be quite large - but if we assume the EOL happens not too rarely, we don't have to do the whole thing.

# 5.8: Near optimality of arithmetic coding

- Huffman coding is optimal but not scalable, because to we'd have to make arbitrarily large blocks - which is impractical
- infinite precision arithmetic coding does scale up though (essentially has arbitrarily large blocks)
  - you can also get very close to the entropy: within 2 bits of the ideal for the entire sequence
  - finite precision coding isn't too shabby either
- Let
  - $\mathcal{X}=\{0,1,\dots,n\}$, EOF = 0, $p=(p_0,p_1,\dots,p_n)$
  - define $\mathcal{X}^1=\{x_1\dots x_k0: x_i\in \mathcal{X}\setminus\{0\}, k\geq 0\}$, i.e. sequences to encode with the EOF
  - we want to encode $x_1x_2\dots x_k0\in \mathcal{X}^1$
  - $p^1(x_1\dots x_k0) = p_{x_1}p_{x_2}\dots p_{0}$
    - note that this is a pmf on $\mathcal{X}^1$: easily shown
  - What is the expected encoded length $L^1 = \sum l(x)p^1(x)$ for $x \in \mathcal{X}^1$
    - $l(x) = $number of bits to encode for $[a,b)$ for $x_1\dots x_k0$, i.e. $\min\{m: \exists \beta_1,\dots,\beta_m\in\{0,1\}$ s.t. $[0.\beta_1\dots\beta_m,0.\beta_1\dots\beta_m + 1/2^m)\subset [a,b)\}$
      - the length of the $\beta$ interval $1/2^m$ and at smallest $(b-a)/2$.
      - i.e. $1/2^m\leq (b-a)/2$ implies that $l(x)\leq m$
        - $1/2^{m-1}\leq b-a = \prod_{i\in x} p_i = p^1(x)$
          - $\log_2(1/p^1(x))\leq m-1$
          - choose $m = ceil(\log_2(1/p^1(x))) + 1$.
            - then the condition holds, and $l(x)\leq m = ceil(\log_2(1/p^1(x))) + 1$, giving us an upper bound for $l(x)$
            - note that this also the length of a shannon code on $p^1$ and $\mathcal{X}^1$
              - thus $H_2(p^1) \leq L^1(x) < H_2(p^1)+1$
              - $l(x)\leq ceil(\log_2(1/p^1(x)))+1 = l_{sh}(x)+1$
              - $L^1 = \sum_{\mathcal{X}^1}l(x)p^1(x)\leq L_{sh}+1 < H_2(p^1) + 2$
      - thus, $L^1< H_2(p^1)+2$ for elements in $\mathcal{X}^1$.
  - in practice, of course we don't know $p$ so we can do better perhaps
  - note that using the shannon code is also computationally impractical.
    - it's just useful that the arithmetic coding is bounded by the shannon code

# 5.9: Computational Complexity of Arithmetic Coding

- Infinite precision case
  - assume that we can do infinite precision addition and multiplication in constant time
    - this is unreasonable, but is somewhat the case for finite precision
  - Let 
    - $\mathcal{X}=\{0,1,\dots,n\}$, $p=(p_0,p_1,\dots,p_n)$
    - $x = x_1\dots x_k$, $|x| = k$
    - $l(x) = m$, i.e. the length of the encoded message
  - e.g. in our example we took $210$ to $101100$, i.e. $|x|=3$ and $l(x) = 6$
  - we can precompute $c$, $d$. We want to see complexity as $|x|$ changes.
    - encoder: 
      - part 1 (the for loop to find $[a,b)$): all the stuff inside is constant time, so $O(|x|)$, i.e. the number of loops
      - part 2 (find the interval inside):
        - number of iterations of loop is $O(l(x))$: each part of the loops emits one
      - thus encoder complexity is $O(|x| + l(x))$
    - decoder:
      - assume that $M\approx m$, which we'll see in the finite case
        - computing $z$ is about $O(M)=O(m)=O(l(x))$
        - the while loop: stuff inside is constant (note that $n$ is fixed), so it depend on the number of iterations of the while loop:
          - i.e. number that we emit, which is $|x|$
      - thus decoder complexity is $O(|x| + l(x))$
  - So this is good if $l(x)$ is not too enormous
    - but as we saw before, arithmetic coding is pretty near optimal. So the expected $l(x)$ is also near optimal. $\mathbb{E}(l(x)) = L^1 < H(p^1)+2$
- Finite precision case:
  - encoder and decoder complexity are still linear in $l(x)$ and $|x|$
  - what does degrade is the compression performance
    - this is dependent on being able to approximate $p$
    - which is pretty easy with 32 bit precision/for most cases

# 5.10: Generalizing arithmetic coding to non-iid models

# TODO
- 5.10: misc: 24min
- 5.11 - 5.14: Finite-precision: 32 + 8 + 18 + 24 = 1h 22min
